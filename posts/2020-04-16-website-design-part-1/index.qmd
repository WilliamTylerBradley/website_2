---
title: 'Website Design: Part 1'
description: "This blog post and the next one cover the code for making the site images. The first post pulls the data, and the second one creates the images. The setup first pulls the contents from the [Data Science](https://en.wikipedia.org/wiki/Data_science) page of Wikipedia to get the letter frequencies. The code then pulls the overall letter frequencies for the English language. Finally, it saves them together to graph later."
date: "2020-04-16"
categories: [R, Blog]
image: "ds_letter_thumbnail.png"
execute: 
  eval: false
---

<b>NOTE: This post is about the old blog design. The images are not used on this site.</b>

This blog post and the next one cover the code for making the site images. The first post pulls the data, and the second one creates the images. The setup first pulls the contents from the [Data Science](https://en.wikipedia.org/wiki/Data_science) page of Wikipedia to get the letter frequencies. The code then pulls the overall letter frequencies for the English language. Finally, it saves them together to graph later.

This first section just lists out the R libraries. The `here` and `ggplot2` libraries are standard for most of my projects. The main additional library is `rvest`, which pulls the content from web pages.

```{r libraries}
#----------
# Libraries
#----------

# Set up
library(here)
library(ggplot2)

# Library for getting html
library(rvest)
help(html_text)
```

The second section pulls the Data Science Wikipedia page, cleans up the result a little, and graphs the data to check it.

```{r data_science_data}
#--------------------
# 'Data Science' data
#--------------------

# Scrape the Data Science data
data_science <- read_html("https://en.wikipedia.org/wiki/Data_science")

# Pull out main title
heading1 <- html_text(html_nodes(data_science, "h1"))
heading1

# Pull out body text
body <- html_text(html_nodes(data_science, "p"))
head(body)

# Add main title
body[1] <- paste(heading1, body[1])
body[1]

# Crush together
body <- paste(body, collapse = " ")

# barplot of counts
ds_letter <- data.frame(table(
  strsplit(tolower(gsub("[^[:alpha:]]", "", body)), "")))
names(ds_letter) <- c("Letter", "Frequency")
ds_letter$Frequency <- ds_letter$Frequency / sum(ds_letter$Frequency) * 100
ds_letter

ggplot(data = ds_letter, aes(Letter, Frequency)) +
  geom_bar(stat = "identity") +
  ggtitle("Data Science Wikipedia Page")
ggsave(filename = here::here("images", "ds_letter.png"))
```

![](ds_letter.png)

The next section is like the previous one. Instead of pulling the text from a Wikipedia page, the code pulls data out of a table for letter frequencies.

```{r overall_data}
#---------------
# Overall data
#---------------

# Get overall letter counts page
letter_frequency <-
  xml2::read_html("https://en.wikipedia.org/wiki/Letter_frequency")

# Find the right table
# Thanks Robert Lewand and Pavel Micka
tables <- html_nodes(letter_frequency, "table")
overall_letter <- html_table(tables[1], fill = TRUE)[[1]]
overall_letter <- overall_letter[c(1, 3)]
names(overall_letter) <- c("Letter", "Frequency")
overall_letter

ggplot(data = overall_letter, aes(Letter, Frequency)) +
  geom_bar(stat = "identity") +
  ggtitle("Overall")
ggsave(filename = here::here("images", "overall_letter.png"))
```

![](overall_letter.png)

The last two sections combine the data and save it for future use.

```{r combine_data}
#-------------
# Combine data
#-------------
overall_letter$Source <- "Overall"
ds_letter$Source <- "Data Science"
freqs <- rbind(overall_letter, ds_letter)
freqs
```

```{r output_table, eval = TRUE, echo = FALSE}
library(kableExtra)
data <- read.csv(file = "https://raw.githubusercontent.com/WilliamTylerBradley/site_design/master/freqs.csv")
data %>%
  kable(format = "html",
        caption = "The final data set",
        digits = 2) %>%
  kable_styling(position = "center",
                full_width = FALSE)

```

```{r save_data}
# Save the file in case it's needed later after Wikipedia updates
write.csv(freqs,
          file = here::here("freqs.csv"),
          row.names = FALSE)
```

The source code file is found [here](https://github.com/WilliamTylerBradley/site_design/blob/master/scrape_wiki.R). This data will be read back into R to create the images in the [next post](https://www.willtybrad.com/2020-04-16-website-design-part-2/).
