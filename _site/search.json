[
  {
    "objectID": "posts/2021-01-31-genuary-2021/index.html",
    "href": "posts/2021-01-31-genuary-2021/index.html",
    "title": "GENUARY 2021",
    "section": "",
    "text": "The three main takeaways from my experience are:\n\nHaving a ‘times-up-step-away-from-the-keyboard’ attitude was vital for getting me through this project. Unfortunately, several works ended up unfinished for what I wanted, and there are probably bugs left in the code. Normally I hate that, but time-based stopping surprisingly set up a good attitude the next day. Having a plan to avoid burn-out is a necessity (like GENUARY warns).\nGoing wide instead of deep was great for the first GENUARY. I tried as many new packages, programming techniques, and art styles as I could. I’ll mark the packages on at least the first day throughout this post. The R community’s numerous packages and blogposts are incredibly helpful.\nggplot2 is amazing to use. This package saves an incredible amount of time. I have an advantage from previous experience, but it is so easy to wrap up a lot of data prep into something nice.\n\nYou can check out the code here.\n\nJAN.1 // TRIPLE NESTED LOOP\n\n\nI started out avoiding for loops (spoiler alert: used on day three). So for this one, I created a double nested tibble to get three layers. The first layer lists coefficients and standard deviations to generate data for a regression. The number of observations and then the raw data are the final two layers. The code runs through linear models for each data set. The output graphs the number of observations on the y axis, the estimated coefficient on x, and the summary statistic for sigma as the size. Color also maps to x. The effect isn’t a complicated picture, but I do like the bubbling up action. I used dplyr, purrr, tidyr, and ggplot2.\n\n\n\n\n\nJAN_01\n\n\n\n\n\n\nJAN.2 Rule 30 (elementary cellular automaton)\n\n\nI used data.table for data manipulation. The rules increase the dataset by adding new columns and using shift (lead/lag) to get the input subsets. The outcome was supposed to emphasize that each cell depends on three other cells, but I ran out of time before I got anywhere.\n\n\n\n\n\nJAN_02\n\n\n\n\n\n\nJAN.3 Make something human.\n\n\nThe gganatogram package produces the human diagrams. viridis helps with color and gridExtra for layout. The output is accidentally oddly shaped.\n\n\n\n\n\nJAN_03\n\n\n\n\n\n\nJAN.4 Small areas of symmetry.\n\n\nBlackwork cross stitch serves as the basis here. The small patterns are hardcoded, and I’ll probably try generating them randomly one day.\n\n\n\n\n\nJAN_04\n\n\n\n\n\n\nJAN.5 Do some code golf! How little code can you write to make something interesting? Share the sketch and its code together if you can.\n\n\nFor today’s #genuary #genuary2021 prompt, I tried to use #rstats to recreate Georg Nees' Schotter. My goal was to get something close to the original, have the code fit in a tweet, and give base R a good shot before using other packages. Here’s what I got:\n\n— Tyler (@w_tyler_bradley) January 6, 2021\n\n\n\n\n\nf=function(i){l=ceiling(i/12);u=2*(l-1);\nt=(c(1,3,5,7)*45+runif(1,-u,u))*.0174;\nm=rnorm(2,0,l/90);s=.707;\nx=c(s*cos(t)+m[1]+i%%12,NA);\ny=c(s*sin(t)+m[2]-l,NA);\ndata.frame(x,y)}\ndf=do.call(rbind,lapply(1:288,f));\nplot(-5:25,-30:0,\"n\",ax=F,an=F,as=1);\npolygon(df$x,df$y)\n\n\n\n\n\n\nJAN_05\n\n\n\n\n\n\nJAN.6 Triangle subdivision.\n\n\nA function subdivides the triangles but keeps the same pattern of either lightening or darkening the base color. The colorspace package provides the color manipulation capabilities.\n\n\n\n\n\nJAN_06\n\n\n\n\n\n\nJAN.7 Generate some rules, then follow them by hand on paper.\n\n\nI didn’t do this one. I took time off because of my birthday and to keep up with the (unrelated) coup attempt.\n\n\n\nNA\n\n\n\n\n\nJAN.8 Curve only.\n\n\nPursuit curves set up this work. Several trails of points chase after one point moving around in a circle. The deSolve package handles most of the math.\n\n\n\n\n\nJAN_08\n\n\n\n\n\n\nJAN.9 Interference patterns.\n\n\nggforce provides the geom for circles. I’m not sure I really get inference patterns and I feel like this was a swing and a miss from me.\n\n\n\n\n\nJAN_09\n\n\n\n\n\n\nJAN.10 // TREE\n\n\nCode using ape, phytools, and stringr create and draw a random phylogenetic tree. Where parenthesizes and dashes fall in a string determine the tree’s structure.\n\n\n\n\n\nJAN_10\n\n\n\n\n\n\nJAN.11 Use something other than a computer as an autonomous process (or use a non-computer random source).\n\n\nCloudflare’s use of lava lamps for randomness inspired this work. I picked a gif of a lava lamp and convert it to binary using the magick package.\n\n\n\n\n\nJAN_11\n\n\n\n\n\n\nJAN.12 Use an API (e.g. the weather). Here’s a huge list of free public APIs.\n\n\nI use tidycensus, tidyverse, sf, and ggridges to pull population by county, group by state/territory, average over longitude to create ridges, and center them. I tried this out in Joy Division’s Unknown Pleasures style, but it was too busy for me.\n\n\n\n\n\nJAN_12\n\n\n\n\n\n\nJAN.13 Do not repeat.\n\n\nI avoided repeating by basing the work on prime numbers using the primes package. I also used the built-in as.roman() function to convert numbers to Roman numerals.\n\n\n\n\n\nJAN_13\n\n\n\n\n\n\nJAN.14 // SUBDIVISION\n\n\nI really struggled on this one. There are supposed to be four images colored by tvthemes, one for each of the Avatar: The Last Airbender nations. I couldn’t get it to work with any shape that wasn’t a square. I think the real issue is that sf just wasn’t built for what I was trying to do.\n\n\n\n\n\nJAN_14\n\n\n\n\n\n\nJAN.15 Let someone else decide the general rules of your piece.\n\n\nThe rules are from the snakes and ladders board game. The code generates a random board, simulates through several players, and graphs the results. I got to use a while loop since the game doesn’t end until falling precisely on the 100th square.\n\n\n\n\n\nJAN_15\n\n\n\n\n\n\nJAN.16 Circles only\n\n\nA phyllotaxis arrangement places the circles for this work. This is one of the most simple outputs but still one of my favorites. The 100 small circles repeat in the 16 sets, but sizes change based on their order modulo 16. Within each set, the locations are all the same.\n\n\n\n\n\nJAN_16\n\n\n\n\n\n\nJAN.17 Draw a line, pick a new color, move a bit.\n\n\nThe endpoints of the lines are points on two bounded random walks. I used grid to draw and move separately. The colorspace mixes colors from the endpoint locations. The ragg package helps with drawing over a million lines.\n\n\n\n\n\nJAN_17\n\n\n\n\n\n\nJAN.18 One process grows, another process prunes.\n\n\nThis work samplings points from several normal distributions using MASS for growth. Then samples from smaller normal distributions, find the closest point from the first set, and removes them. The anti_join() function made the pruning very easy. hexbin graphs the final dataset.\n\n\n\n\n\nJAN_18\n\n\n\n\n\n\nJAN.19 Increase the randomness along the Y-axis.\n\n\nI had a lot of problems with this one too. It was supposed to be based on multi-level modeling, but I couldn’t get it working and ran out of time. This one ended up as samples from t-distributions with decreasing degrees of freedom. The top one is a Cauchy distribution.\n\n\n\n\n\nJAN_19\n\n\n\n\n\n\nJAN.20 No loops.\n\n\nI wanted something that looked like the code would have loops but then not actually use any. That’s not too difficult with R. The patterns are various hypotrochoids on a grid background.\n\n\n\n\n\nJAN_20\n\n\n\n\n\n\nJAN.21 function f(x) { DRAW(x); f(1 * x / 4); f(2 * x / 4); f(3 * x / 4); }\n\n\nThe recursive code starts with veins growing out of the middle towards the edges of a triangle. They increase in number based on one parameter and decrease in length based on the other. If you look really closely, some lines are drawn over points instead of having all the lines drawn then points. That is done using grid.\n\n\n\n\n\nJAN_21\n\n\n\n\n\n\nJAN.22 Draw a line. Wrong answers only.\n\n\nThis isn’t a line. It’s a circle, a great circle. The additional rnaturalearth, rnaturalearthdata, and geosphere packages provide the data and functions. I had a lot of issues getting randomly drawn maps to look good. So there is only one. I like that it fell on New Zealand, where R originated.\n\n\n\n\n\nJAN_22\n\n\n\n\n\n\nJAN.23 #264653 #2a9d8f #e9c46a #f4a261 #e76f51, no gradients. Optionally, you can use a black or white background.\n\n\nThe Copperajah Pokemon is the foundation here. The colors match the prompt, and the Pokemon’s design provides the shapes. They repeat on grids based on primes numbers, but I don’t think that showed nicely.\n\n\n\n\n\nJAN_23\n\n\n\n\n\n\nJAN.24 500 lines.\n\n\nThis is supposed to be a drawing of my family’s dog. It’s just the head. The eyes, nose, and ears are the darker sections. I use the png package to read in the image and determine the path using TSP to solve the traveling salesman problem. The sampling of points could be a lot better, especially on the edges. Although, I’m not sure 500 lines will cut it here.\n\n\n\n\n\nJAN_24\n\n\n\n\n\n\nJAN.25 Make a grid of permutations of something.\n\n\nPermutations of quadratic bézier curves provide the grid. The endpoints of each curve are separated one unit horizontally. The interior control points permute around pentagons centered on the endpoints. The change along the x-axis adjusts the first control point while the y-axis has the second. Then, additional bézier curves are created between sets on the same line to connect them nicely.\n\n\n\n\n\nJAN_25\n\n\n\n\n\n\nJAN.26 2D Perspective.\n\n\nA Hilbert curve is given a few glitches and bent to give a 2D perspective illusion. The Hilbert curve code uses look-up tables for the rules with data.table.\n\n\n\n\n\nJAN_26\n\n\n\n\n\n\nJAN.27 Monochrome gradients without lines.\n\n\nThe output is not monochrome because the prompt didn’t specify that at the beginning of the month. I had the tab open and never hit refresh. So I didn’t know about the update until after I finished. I like the colors from katiejolly/nationalparkcolors and decided to keep them. The shapes are Poisson disk sampled using coolbutuseless/poissoned and outlined using Voronoi diagrams from ggvoronoi. There are multiple layers with decreasing alpha values.\n\n\n\n\n\nJAN_27\n\n\n\n\n\n\nJAN.28 Use sound.\n\n\nI used Spotify data with the charlie86/spotifyr package for the sound requirement. Each of the lines is a different top 50 song with parameters from their data. The moldach/vapoRwave package provides the colors.\n\n\n\n\n\nJAN_28\n\n\n\n\n\n\nJAN.29 Any shape, none can touch.\n\n\nggrepel made this a pretty easy prompt. I copied the text from the Introduction to R section on the R website for the shapes. Their locations are sampled from the R logo. The R logo image is read in and manipulated using imager.\n\n\n\n\n\nJAN_29\n\n\n\n\n\n\nJAN.30 Replicate a natural concept (e.g. gravity, flocking, path following).\n\n\nI finally got to use rayshader. This gif is supposed to replicate tides. The terrain comes from sampling font with extrafont combined with noise from ambient.\n\n\n\nToo big to fit. So click here.\n\n\n\n\n\nJAN.31 10 SEARCH FOR “ENO’S OBLIQUE STRATEGIES” 20 OBTAIN ONE 30 THAT IS YOUR PROMPT FOR TODAY\n\n\nI got ‘Emphasize repetitions’. This work is motivated by the repetitive act of knitting. The shading is normal error on top of an arima model that zig-zags up the y-axis.\n\n\n\n\n\nJAN_31"
  },
  {
    "objectID": "posts/2021-08-12-bertrand-paradox/index.html",
    "href": "posts/2021-08-12-bertrand-paradox/index.html",
    "title": "Bertrand Paradox",
    "section": "",
    "text": "The code can be found here. We’ll walk through a subset of it in this post. For each method, we generate some features at random, then get lines and points from them.\n\n\n\nRandomization Methods\n\n\n\nFor these methods, we’ll build a function called line_points that takes in the center and radius of the circle along with the features we randomly generated for each method. The function outputs the two endpoints for the chord and the midpoint. After creating line_points, we’ll generate the random features then run them through pmap_dfr from the purrr package to apply the line_points function.\nFor the following code snippets, circle_h, circle_k, and circle_radius are the circle’s center x coordinate, center y coordinates, and radius.\nFor method 1, we generate the two points on the perimeter as uniform distances from 0 to 2 * pi. Then we take each of those and convert them to coordinates based on the circle’s center and radius. Finally, get the midpoint and return.\n\n### Transformation\n## Parameters\n# circle_x is x value of center\n# circle_y is y value of center\n# circle_radius is radius\n# point_1 is one point from 0 to 2*pi\n# point_2 is one point from 0 to 2*pi\n## Returns\n# dataframe with\n# point_1_x, point_1_y, point_2_x, point_2_y, midpoint_x, midpoint_y\nline_points <- function(circle_x, circle_y, circle_radius, point_1, point_2) {\n  # Get x,y values\n  point_1_x = circle_x + circle_r * cos(point_1)\n  point_1_y = circle_y + circle_r * sin(point_1)\n  point_2_x = circle_x + circle_r * cos(point_2)\n  point_2_y = circle_y + circle_r * sin(point_2)\n  \n  # Get midpoints\n  midpoint_x = (point_1_x + point_2_x) / 2\n  midpoint_y = (point_1_y + point_2_y) / 2\n  \n  return(data.frame(point_1_x = point_1_x,\n                    point_1_y = point_1_y,\n                    point_2_x = point_2_x, \n                    point_2_y = point_2_y,\n                    midpoint_x = midpoint_x,\n                    midpoint_y = midpoint_y))\n}\n\n##-------------\n# Dataset setup\n##-------------\nlines <- data.frame(line = seq(1, n_lines),\n                    point_1 = runif(n_lines, 0, 2 * pi),\n                    point_2 = runif(n_lines, 0, 2 * pi))\n\nlines <- lines %>%\n  bind_cols(pmap_dfr(list(circle_x = circle_h,\n                          circle_y = circle_k,\n                          circle_radius = circle_r, \n                          point_1 = .$point_1,\n                          point_2 = .$point_2), line_points))\n\nFor method 2, our line_points function needs a radius (random uniform from 0 to 2 * pi) and the proportionate distance from the center to the perimeter (random uniform from 0 to 1). The line_points function takes those values, converts the proportion to the actual distance, finds the ends of a chord crossing that point if the angle was 0, rotates everything to the appropriate angle, then moves to the circle’s location.\n\n### Transformation\n## Parameters\n# circle_x is x value of center\n# circle_y is y value of center\n# circle_radius is radius\n# radius is radian\n# chord is fraction along the radius \n## Returns\n# dataframe with\n# point_1_x, point_1_y, point_2_x, point_2_y, midpoint_x, midpoint_y\nline_points <- function(circle_x, circle_y, circle_radius, radius, chord) {\n  # move out to radius from origin\n  x_1 = circle_radius * cos(radius)\n  y_1 = circle_radius * sin(radius)\n  x_2 = circle_radius * cos(radius)\n  y_2 = circle_radius * sin(radius)\n  \n  # https://www.mathsisfun.com/algebra/trig-solving-ssa-triangles.html\n  # we know one angle is 90*\n  # one side is circle_radius\n  # one side is chord\n  new_angle = pi - pi/2 - asin(((chord * circle_radius) * \n                                  sin(pi/2)) / circle_radius)\n  \n  # now rotate these points around to the radius\n  x_1_moved = cos(new_angle) * x_1 - sin(new_angle) * y_1\n  y_1_moved = sin(new_angle) * x_1 + cos(new_angle) * y_1\n  x_2_moved = cos(-new_angle) * x_2 - sin(-new_angle) * y_2\n  y_2_moved = sin(-new_angle) * x_2 + cos(-new_angle) * y_2\n  \n  # move back to where the circle is\n  x_1_moved = x_1_moved + circle_x\n  y_1_moved = y_1_moved + circle_y\n  x_2_moved = x_2_moved + circle_x\n  y_2_moved = y_2_moved + circle_y\n  \n  return(data.frame(point_1_x = x_1_moved,\n                    point_1_y = y_1_moved,\n                    point_2_x = x_2_moved, \n                    point_2_y = y_2_moved,\n                    midpoint_x = (x_1_moved + x_2_moved) / 2,\n                    midpoint_y = (y_1_moved + y_2_moved) / 2))\n}\n\n##-------------\n# Dataset setup\n##-------------\nlines <- data.frame(line = seq(1, n_lines),\n                    radius = runif(n_lines, 0, 2*pi),\n                    chord = runif(n_lines, 0, 1))\nlines <- lines %>%\n  bind_cols(pmap_dfr(list(circle_x = circle_h,\n                          circle_y = circle_k,\n                          circle_radius = circle_r, \n                          radius = .$radius,\n                          chord = .$chord), line_points))\n\nFor method 3, line_points now needs a point’s x and y coordinates in the unit circle. We then find the chord by rotating the point to angle 0 then finding the vertical line through that point. The chord’s endpoints are where the vertical line crosses the circle. We then rotate those points back, so their midpoint matches the original point.\n\n### Transformation\n## Parameters\n# circle_x is x value of center\n# circle_y is y value of center\n# circle_radius is radius\n# point_x is the x value of the random point\n# point_y is the y value of the random point\n## Returns\n# dataframe with\n# point_1_x, point_1_y, point_2_x, point_2_y, midpoint_x, midpoint_y\nline_points <- function(circle_x, circle_y, circle_radius, point_x, point_y) {\n  # move out to radius from origin\n  point_x = circle_radius * point_x\n  point_y = circle_radius * point_y\n  \n  # rotate so just dealing with y\n  flatten_by = atan2(point_y, point_x)\n  \n  # now rotate these points around to the radius\n  x_rotated = cos(-flatten_by) * point_x - sin(-flatten_by) * point_y\n  y_rotated = sin(-flatten_by) * point_x + cos(-flatten_by) * point_y # Should be zero\n  \n  # find y values \n  # x^2 + y^2 = r^2\n  # y^2 = r^2 - x^2\n  # y = +/- sqrt(r^2 - x^2)\n  y1 = sqrt(circle_radius^2 - x_rotated^2)\n  y2 = -sqrt(circle_radius^2 - x_rotated^2)\n  \n  # rotate back\n  point_1_x = cos(flatten_by) * x_rotated - sin(flatten_by) * y1\n  point_1_y = sin(flatten_by) * x_rotated + cos(flatten_by) * y1\n  point_2_x = cos(flatten_by) * x_rotated - sin(flatten_by) * y2\n  point_2_y = sin(flatten_by) * x_rotated + cos(flatten_by) * y2\n\n  # move back to where the circle is\n  point_1_x = point_1_x + circle_x\n  point_1_y = point_1_y + circle_y\n  point_2_x = point_2_x + circle_x\n  point_2_y = point_2_y + circle_y\n  \n  return(data.frame(point_1_x = point_1_x,\n                    point_1_y = point_1_y,\n                    point_2_x = point_2_x, \n                    point_2_y = point_2_y,\n                    midpoint_x = (point_1_x + point_2_x) / 2,\n                    midpoint_y = (point_1_y + point_2_y) / 2))\n}\n\n##-------------\n# Dataset setup\n##-------------\nlines <- data.frame(line = seq(1, n_lines),\n                    r = sqrt(runif(n_lines, 0, 1)),\n                    theta = runif(n_lines, 0, 2*pi))\nlines <- lines %>%\n  mutate(point_x = r * cos(theta),\n         point_y = r * sin(theta)) %>%\n  select(line, point_x, point_y)\nlines <- lines %>%\n  bind_cols(pmap_dfr(list(circle_x = circle_h,\n                          circle_y = circle_k,\n                          circle_radius = circle_r, \n                          point_x = .$point_x,\n                          point_y = .$point_y), line_points))\n\nNow that we have the basic attributes for the three methods, we can talk about coloring them.\n\n\n\nColor Selection\n\n\n\nFor the methods, I wanted each one to be colored based on different hues. So, I selected them from a yellow-orange/blue-green/purple-red triad. The colors are sampled from ellipses in the HCL color space. The following images show samples from the same ellipses. The line colors are on top and points on the bottom.\n\n\n\n\n\n\nMethod 1: Colors\n\n\n\n\n\n\n\nMethod 2: Colors\n\n\n\n\n\n\n\nMethod 3: Colors\n\n\n\n\n\nIn addition to the lines having a lower base Luminance value, the lines’ ellipses are tilted down towards the center while the points’ slope upwards on the Chroma-Luminance plane for their respective Hue values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod 1: Chroma-Luminance Plane\n\n\n\n\n\n\n\nMethod 2: Chroma-Luminance Plane\n\n\n\n\n\n\n\nMethod 3: Chroma-Luminance Plane\n\n\n\n\n\nEach of the lines is closer to the secondary colors in Hue than the points. For the order, the revolve around the color wheel counter-clockwise. We can see this when looking at the Hue-Chroma plane at their respective Luminance values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod 1: Hue-Chroma Plane\n\n\n\n\n\n\n\nMethod 2: Hue-Chroma Plane\n\n\n\n\n\n\n\nMethod 3: Hue-Chroma Plane\n\n\n\n\n\nI tried picking the colors by algorithm, but I couldn’t get that to work as well as I wanted. I often ended up with too much brown or blue. So, the base colors are hand-picked.\n\n\n\nFinal Output\n\n\n\nNow that we can generate the lines/point and colors let’s stick it together. We’ll add diagrams towards the bottom of the images to explain the methods along with borders. The Github repo contains the complete code if you want to see it. The diagrams are affected by the random seed, so any example lines/points can be chosen. I used grid for this instead of ggplot2 because I wanted the points and lines to overlap back and forth. That was easier to do with grid. Later I changed and put all the points on top of all the lines, but by then, I had it all done in grid. I also found this setup easier to control where to place everything on the page instead of worrying about margins or other artifacts getting in the way.\nFrom here to the end are the final products. Better 18” x 24” jpeg files can be found here."
  },
  {
    "objectID": "posts/2021-12-09-deep-blue-versus-gary-kasparov/index.html",
    "href": "posts/2021-12-09-deep-blue-versus-gary-kasparov/index.html",
    "title": "Deep Blue versus Garry Kasparov",
    "section": "",
    "text": "The idea came out of watching The Queen’s Gambit last year. There’s a terrific scene where the main character says, “Chess isn’t always competitive … chess can also be beautiful.” I thought that was interesting because the series happened in 1950’s/1960’s before computers were winning chess. So today, the most competitive setup involves computers, but that still leaves room for the beautiful part from humans. There’s a neat comparison between chess and generative art. This project uses moves from Deep Blue for the generative aspect, connecting to when computers started taking over competitively. Also, I think using chess data for generative art is neat for a found materials project.\nEach of the games has a grid layout. Horizontally, the pieces lay across the columns. Vertically, the games’ actions move down the grid, with pieces dropping out when captured. Horizontal lines connect the pieces when captured. The square’s color tells the location on the board. Rank follows luminance (white is lighter, black is darker) while files move along hue/chroma in the HCL color space. As pieces move on the board, their color changes from one row to the next.\nThe code uses the rchess package with game files from chessgames.com. data.table handles the data manipulation and image setup. The code can be found here.\nHere is the piece order. This also shows the first row in each grid (notices the notches separating the types). The order starts in the middle of the king and queen, works outward, moves to the pawns, works inwards, then reverses to the other side. The two kings end up on the outside edges. The winning side’s king will drop down into the final row. If there is no winner, the kings end on the same row.\n\n\n\nPiece Order\n\n\nA different grid of colors exists for each board of the 14 games (seven for 1996 and seven for 1997). The colors sets are eight by eight grids that maximize the distance between points in the HCL color space as they move around the hue values.\n\n\n\nColor Grids\n\n\nThe following graph shows all the colors with their hue/chroma values. I found moving chroma out as hue moves give some distinction between the sides of the board without creating too much of a striping effect. There are different luminance values for each grid to maximize the space they can take.\n\n\n\nColors’ Hue and Chroma Values\n\n\nFinally, here are the two works. The background colors are from the middle of the grid then rotated around the space.\n\n\n\n1996 Games\n\n\n\n\n\n1997 Games\n\n\nWe can see how the number of turns affects the length of the grids, how some pieces (some pawns) get taken more often than others, and how there’s a lot of exchanges in pieces."
  },
  {
    "objectID": "posts/2022-02-28-constellations/index.html",
    "href": "posts/2022-02-28-constellations/index.html",
    "title": "Constellations",
    "section": "",
    "text": "The code can be found here. Unfortunately, the code and the outputs are not as polished as I would like. I got tired of the project and decided to wrap it up where it was, but I hope this can be useful.\nSetting up the projection function first is crucial because I continuously use it to check the code. I’m using the stereographic projection. This projection will graph both hemispheres as big circles. I tried Nicolosi globular projection but found the lines curved too much. I also tried gnomonic projection since all great circles are straight lines, but it doesn’t graph the whole hemisphere nicely. Stereographic is an excellent compromise, but I wish all the lines connecting the stars were straight.\n\nlibrary(tidyverse)\n\n# df needs:\n# x, y, z on unit sphere\nprojection <- function(df) {\n  \n  df <- df %>%\n    mutate(lat = asin(z),\n           long = atan2(y, x)) %>%\n    mutate(new_x = if_else(z < 0, x / (1 - z), x / (1 + z) + 2),\n           new_y = if_else(z < 0, y / (1 - z), y / (1 + z)))\n  \n  return(df)\n}\n\nTo set the stars and constellation centers, I used Mitchell’s best candidate algorithm for the disk sampling method. This algorithm will space out points without putting any too close together. The code starts with one random point on the unit sphere, creates candidates_n new points, finds the one farthest away from the previously drawn point, and accepts it if it’s past the distance_limit from the already selected point. Then, for the following points_n times, random points are drawn, compared, and possibly selected. The function can return fewer than points_n points if either the points_n or distance_limit is too large.\n\n# points_n, approx. number of final points\n# candidates_n, number of attempts per point\n# distance_limit, minimum distance between points\ndisk_sampling <- function(points_n, candidates_n, distance_limit) {\n  \n  df <- tibble(\n    x = c(rnorm(1), rep(0, points_n - 1)),\n    y = c(rnorm(1), rep(0, points_n - 1)),\n    z = c(rnorm(1), rep(0, points_n - 1))\n  ) %>%\n    mutate(normalize = sqrt(x^2 + y^2 + z^2)) %>%\n    mutate(normalize = if_else(normalize == 0, 1, normalize)) %>%\n    mutate(x = x / normalize,\n           y = y / normalize,\n           z = z / normalize) %>%\n    select(-normalize) %>%\n    rowid_to_column(\"id\") \n  \n  for(i in seq(2, points_n)) {\n    \n    candidates <- tibble(\n      can_x = rnorm(candidates_n),\n      can_y = rnorm(candidates_n),\n      can_z = rnorm(candidates_n)\n    ) %>%\n      mutate(normalize = sqrt(can_x^2 + can_y^2 + can_z^2)) %>%\n      mutate(can_x = can_x / normalize,\n             can_y = can_y / normalize,\n             can_z = can_z / normalize) %>%\n      select(-normalize) %>%\n      rowid_to_column(\"can_id\") \n    \n    best_candidate <- df %>%\n      filter(x != 0 | y != 0 | z != 0) %>%\n      expand_grid(candidates) %>%\n      mutate(distance = acos(x * can_x + \n                               y * can_y + \n                               z * can_z)) %>%\n      group_by(can_id) %>%\n      summarise(distance = min(distance)) %>%\n      filter(distance == max(distance)) %>%\n      filter(distance > distance_limit) %>%\n      slice(1) %>%\n      pull(can_id)\n    \n    if(length(best_candidate) > 0) {\n      best_candidate <- candidates %>%\n        filter(can_id == best_candidate) %>%\n        mutate(can_id = i) %>%\n        rename_with(~ gsub(\"can_\", \"\", .x))\n      \n      df <- df %>%\n        rows_update(best_candidate, by = \"id\")\n    }\n    \n  }\n  \n  df <- df %>%\n    filter(x != 0 | y != 0 | z != 0)\n  \n  return(df)\n}\n\nThe following function chooses the lines that connect the stars for a single constellation. All of the lines are great circles on the unit sphere. That changes the math from typical line intersections. I don’t want any of the lines to cross. So we’ll first list out all the intersections. Then we’ll select one star at random and pull one of the lines connected to that star. I want higher probabilities on shorter lines. So the selection is weighted by the inverse of the line length. We’ll use the intersection list we previously found to avoid line crossing by removing any crossing lines from future selection. Then, we select the next star and repeat until all stars are connected.\n\n# df needs x, y, z\nconnections <- function(df) {\n  \n  lines <- data.frame(\n    line_id_1_2 = integer(),\n    id_2 = integer(),\n    id_1 = integer(),\n    x_2 = numeric(),\n    x_1 = numeric(),\n    y_2 = numeric(),\n    y_1 = numeric(),\n    z_2 = numeric(),\n    z_1 = numeric()\n  )\n  \n  if(nrow(df) <= 1) {\n    return(lines)\n  }\n  \n  connections <- df %>%\n    select(starts_with(\"star\")) %>%\n    rename_with(~ paste0(gsub(\"star_\", \"\", .x), \"_1\")) %>%\n    select(id_1, x_1, y_1, z_1)\n  \n  connections <- connections %>%\n    rename_with(~ paste0(gsub(\"1\", \"\", .x), \"2\")) %>%\n    expand_grid(connections) %>%\n    filter(id_1 < id_2) %>%\n    mutate(a_1_2 = y_1 * z_2 - z_1 * y_2,\n           b_1_2 = z_1 * x_2 - x_1 * z_2,\n           c_1_2 = x_1 * y_2 - y_1 * x_2) %>%\n    rowid_to_column(\"line_id_1_2\")\n  \n  # https://blog.mbedded.ninja/mathematics/geometry/spherical-geometry/finding-the-intersection-of-two-arcs-that-lie-on-a-sphere/\n  # https://www.dirkbertels.net/computing/greatCircles_files/great_circles_070206.pdf\n  intersections <- connections %>%\n    rename_with(~ gsub(\"1\", \"3\", .x)) %>%\n    rename_with(~ gsub(\"2\", \"4\", .x)) %>% \n    expand_grid(connections) %>%\n    filter(line_id_1_2 < line_id_3_4) %>%\n    filter((id_1 != id_3 & id_1 != id_4) &\n             (id_2 != id_3 & id_2 != id_4)) %>% # remove ones that share a star\n    mutate(l_1 = b_1_2 * c_3_4 - c_1_2 * b_3_4,\n           m_1 = c_1_2 * a_3_4 - a_1_2 * c_3_4,\n           n_1 = a_1_2 * b_3_4 - b_1_2 * a_3_4) %>%\n    mutate(normal = sqrt(l_1^2 + m_1^2 + n_1^2)) %>%\n    mutate(l_1 = l_1 / normal,\n           m_1 = m_1 / normal,\n           n_1 = n_1 / normal) %>%\n    mutate(l_2 = -l_1,\n           m_2 = -m_1,\n           n_2 = -n_1) %>%\n    mutate(normal_1 = sqrt(x_1^2 + y_1^2 + z_1^2),\n           normal_2 = sqrt(x_2^2 + y_2^2 + z_2^2),\n           normal_3 = sqrt(x_3^2 + y_3^2 + z_3^2),\n           normal_4 = sqrt(x_4^2 + y_4^2 + z_4^2),\n           normal_l_1 = sqrt(l_1^2 + m_1^2 + n_1^2),\n           normal_l_2 = sqrt(l_2^2 + m_2^2 + n_2^2)) %>%  # technically, all 1's\n    mutate(angle_1_l_1 = acos(round((x_1 * l_1 + y_1 * m_1 + z_1 * n_1) / \n                                      (normal_1 * normal_l_1), 7)) * 180/pi,\n           angle_l_1_2 = acos(round((x_2 * l_1 + y_2 * m_1 + z_2 * n_1) / \n                                      (normal_2 * normal_l_1), 7)) * 180/pi,\n           angle_1_l_2 = acos(round((x_1 * l_2 + y_1 * m_2 + z_1 * n_2) / \n                                      (normal_1 * normal_l_2), 7)) * 180/pi,\n           angle_l_2_2 = acos(round((x_2 * l_2 + y_2 * m_2 + z_2 * n_2) / \n                                      (normal_2 * normal_l_2), 7)) * 180/pi,\n           angle_1_2 = acos(round((x_1 * x_2 + y_1 * y_2 + z_1 * z_2) / \n                                    (normal_1 * normal_2), 7)) * 180/pi,\n           angle_3_l_1 = acos(round((x_3 * l_1 + y_3 * m_1 + z_3 * n_1) / \n                                      (normal_3 * normal_l_1), 7)) * 180/pi,\n           angle_l_1_4 = acos(round((x_4 * l_1 + y_4 * m_1 + z_4 * n_1) / \n                                      (normal_4 * normal_l_1), 7)) * 180/pi,\n           angle_3_l_2 = acos(round((x_3 * l_2 + y_3 * m_2 + z_3 * n_2) / \n                                      (normal_3 * normal_l_2), 7)) * 180/pi,\n           angle_l_2_4 = acos(round((x_4 * l_2 + y_4 * m_2 + z_4 * n_2) / \n                                      (normal_4 * normal_l_2), 7)) * 180/pi,\n           angle_3_4 = acos(round((x_3 * x_4 + y_3 * y_4 + z_3 * z_4) / \n                                    (normal_3 * normal_4), 7)) * 180/pi) %>%\n    mutate(sum_angle_1_l_1_2 = angle_1_l_1 + angle_l_1_2,\n           sum_angle_1_l_2_2 = angle_1_l_2 + angle_l_2_2,\n           sum_angle_3_l_1_4 = angle_3_l_1 + angle_l_1_4,\n           sum_angle_3_l_2_4 = angle_3_l_2 + angle_l_2_4) %>%\n    mutate(on_segment_1_l_1_2 = \n             if_else(abs(sum_angle_1_l_1_2 - angle_1_2) < .001, 1, 0),\n           on_segment_1_l_2_2 = \n             if_else(abs(sum_angle_1_l_2_2 - angle_1_2) < .001, 1, 0),\n           on_segment_3_l_1_4 = \n             if_else(abs(sum_angle_3_l_1_4 - angle_3_4) < .001, 1, 0),\n           on_segment_3_l_2_4 = \n             if_else(abs(sum_angle_3_l_2_4 - angle_3_4) < .001, 1, 0)) %>%\n    mutate(intersects = if_else((on_segment_1_l_1_2 == 1 & on_segment_3_l_1_4) |\n                                  (on_segment_1_l_2_2 == 1 & on_segment_3_l_2_4),\n                                1, 0)) %>%\n    filter(intersects == 1)\n  \n  ## Loop through adding one at a time\n  star_list <- df %>%\n    arrange(sample(1:n(), n())) %>%\n    pull(star_id)\n  \n  for(i in seq(1, length(star_list))) {\n    current_star <- star_list[i]\n    potential_lines <- connections %>%\n      filter(id_1 == current_star | id_2 == current_star) %>%\n      anti_join(lines, by = \"line_id_1_2\")\n    \n    # remove intersections\n    limit_lines <- intersections %>%\n      select(line_id_1_2, line_id_3_4)\n    limit_lines <- intersections %>%\n      select(line_id_1_2, line_id_3_4) %>%\n      rename(line_id_1_2 = line_id_3_4,\n             line_id_3_4 = line_id_1_2) %>%\n      rbind(limit_lines) %>%\n      inner_join(lines, by = \"line_id_1_2\")\n    limit_lines <- limit_lines %>%\n      select(line_id_1_2, line_id_3_4)\n    limit_lines <- limit_lines %>%\n      select(line_id_1_2, line_id_3_4) %>%\n      rename(line_id_1_2 = line_id_3_4,\n             line_id_3_4 = line_id_1_2) %>%\n      rbind(limit_lines)\n    \n    potential_lines <- potential_lines %>%\n      anti_join(limit_lines, by = c(\"line_id_1_2\"))\n    \n    if(nrow(potential_lines) > 0) {\n      lines <- potential_lines %>%\n        mutate(weight = acos(x_1*x_2 + \n                               y_1 * y_2 + \n                               z_1 * z_2)) %>%\n        sample_n(1, weight = 1/weight) %>% # add weights here\n        select(-weight) %>%\n        rbind(lines)\n    }\n  }\n  \n  lines <- lines %>%\n    select(-a_1_2, -b_1_2, -c_1_2)\n  \n  return(lines)\n}\n\nFor the sky, we’ll start with 88 constellation centers using the disk sampling method. For the stars, we’ll set up eight sets of 33 stars. Each set of 33 stars is spaced nicely, while having eight groups adds a little noise.\n\nconstellations <- disk_sampling(88, 25, .01) %>%\n  select(-id) %>%\n  rowid_to_column(\"id\") %>%\n  rename_with(~ paste0(\"constellation_\", .x))\n\n# 88 * 3\n# 264 / 8 = 33\nstars <- rbind(disk_sampling(33, 25, .01),\n               disk_sampling(33, 25, .01),\n               disk_sampling(33, 25, .01),\n               disk_sampling(33, 25, .01),\n               disk_sampling(33, 25, .01),\n               disk_sampling(33, 25, .01),\n               disk_sampling(33, 25, .01),\n               disk_sampling(33, 25, .01)) %>%\n  select(-id) %>%\n  rowid_to_column(\"id\") %>%\n  rename_with(~ paste0(\"star_\", .x))\n\nWe’ll find the closest star to each constellation center and combine them. Checking that they don’t cross over from one hemisphere to the other keeps the constellation in only one of the big circles after projecting. We’ll move each star a little closer to the constellation center to create a bit more distance between the constellations.\n\nsky <- constellations %>%\n  expand_grid(stars) %>%\n  mutate(distance = acos(constellation_x * star_x + \n                           constellation_y * star_y + \n                           constellation_z * star_z)) %>%\n  filter(sign(star_z) == sign(constellation_z)) %>% # make sure they cross over line\n  group_by(star_id) %>%\n  filter(distance == min(distance))\n\n# https://math.stackexchange.com/questions/2375102/parametric-equation-of-a-circle-in-3d-given-center-and-two-points-on-the-circle\nmove_percentage <- .25\nstar_movement <- sky %>%\n  mutate(normal_x = star_y * constellation_z - star_z * constellation_y,\n         normal_y = star_z * constellation_x - star_x * constellation_z,\n         normal_z = star_x * constellation_y - star_y * constellation_x,\n         interior = star_x * constellation_x + \n           star_y * constellation_y + \n           star_z * constellation_z) %>%\n  mutate(normal_norm = sqrt(normal_x^2 + normal_y^2 + normal_z^2)) %>%\n  mutate(normal_x = normal_x / normal_norm,\n         normal_y = normal_y / normal_norm,\n         normal_z = normal_z / normal_norm) %>%\n  mutate(angle = acos(interior) * move_percentage,\n         perp_x = normal_y * star_z - normal_z * star_y,\n         perp_y = normal_z * star_x - normal_x * star_z,\n         perp_z = normal_x * star_y - normal_y * star_x) %>%\n  mutate(perp_norm = sqrt(perp_x^2 + perp_y^2 + perp_z^2)) %>%\n  mutate(perp_x = perp_x / perp_norm,\n         perp_y = perp_y / perp_norm,\n         perp_z = perp_z / perp_norm) %>%\n  mutate(star_x = cos(angle) * star_x + sin(angle) * perp_x,\n         star_y = cos(angle) * star_y + sin(angle) * perp_y,\n         star_z = cos(angle) * star_z + sin(angle) * perp_z)\n         \nstar_movement <- star_movement %>%\n  select(star_id, star_x, star_y, star_z) %>%\n  rename_with(~ gsub(\"star_\", \"\", .x)) %>%\n  rename_with(~ paste0(\"star_\", .x))\n\nsky <- sky %>%\n  rows_update(star_movement, by = \"star_id\")\n\nsky <- sky %>%\n  select(-distance) %>%\n  rownames_to_column(\"row\") %>%\n  pivot_longer(cols = -row,\n               names_to = c(\"object\", \".value\"),\n               names_sep =  \"_\") %>%\n  projection() %>%\n  pivot_wider(id_cols = row,\n              names_from = object,\n              names_glue = \"{object}_{.value}\",\n              values_from = c(id, x, y, z, lat, long,\n                              new_x, new_y)) %>%\n  select(-row)\n\nNow that we have the stars in their final positions and set to a constellation, we can add the lines.\n\nlines <- sky %>%\n  select(constellation_id, star_id,\n         star_x, star_y, star_z) %>%\n  nest(data = c(star_id, star_x, star_y, star_z)) %>%\n  mutate(data = map(data, connections)) %>%\n  unnest(data)\n\nlines <- lines %>%\n  pivot_longer(-c(constellation_id, line_id_1_2),\n               names_to = c(\".value\", \"star\"),\n               names_sep =  \"_\") %>%\n  projection() %>%\n  pivot_wider(id_cols = c(constellation_id, line_id_1_2),\n              names_from = star,\n              values_from = c(id, x, y, z, lat, long,\n                              new_x, new_y))\n\n# https://math.stackexchange.com/questions/383711/parametric-equation-for-great-circle\ncurves <- lines %>%\n  expand_grid(f = seq(0, 1, length.out = 100)) %>%\n  mutate(d = acos(sin(lat_1) * sin(lat_2) + cos(lat_1) * cos(lat_2) * cos(long_1 - long_2))) %>%\n  mutate(A = sin((1 - f) * d) / sin(d),\n         B = sin(f * d) / sin(d)) %>%\n  mutate(x = A * cos(lat_1) * cos(long_1) + B * cos(lat_2) * cos(long_2),\n         y = A * cos(lat_1) * sin(long_1) + B * cos(lat_2) * sin(long_2),\n         z = A * sin(lat_1)               + B * sin(lat_2)) %>%\n  select(constellation_id, line_id_1_2, x, y, z) %>%\n  projection() %>%\n  group_by(constellation_id, line_id_1_2) %>%\n  mutate(next_new_x = lead(new_x),\n         next_new_y = lead(new_y)) %>%\n  filter(!is.na(next_new_x))\n\nThe last part adds some background stars, the outline circles, and randomizes the features a little bit.\n\nbase_hue <- 260 + rnorm(1, 0, sd = 5)\nbase_chroma <- 25 + rnorm(1, 0, sd = 3)\ndark_luminance <- 25 + rnorm(1, 0, sd = 3)\nmed_luminance <- 50 + rnorm(1, 0, sd = 3)\nlight_luminance <- 75 + rnorm(1, 0, sd = 3)\nbright_luminance <- 90 + rnorm(1, 0, sd = 3)\n\nbackground_stars_n <- 1000\nbackground_stars <- tibble(\n  x = rnorm(background_stars_n),\n  y = rnorm(background_stars_n),\n  z = rnorm(background_stars_n)\n) %>%\n  mutate(normalize = sqrt(x^2 + y^2 + z^2)) %>%\n  mutate(normalize = if_else(normalize == 0, 1, normalize)) %>%\n  mutate(x = x / normalize,\n         y = y / normalize,\n         z = z / normalize) %>%\n  select(-normalize) %>%\n  rowid_to_column(\"id\") %>%\n  projection() %>%\n  mutate(hue = base_hue + rnorm(n(), 0, sd = 7),\n         chroma = base_chroma + rnorm(n(), 0, sd = 7),\n         luminance = med_luminance + rnorm(n(), 0, sd = 7)) %>%\n  mutate(chroma = if_else(chroma < 0, base_chroma, chroma),\n         luminance = if_else(luminance < 0 | luminance > 100, \n                             med_luminance, luminance)) %>%\n  mutate(color_value = hcl(hue, chroma, luminance),\n         size_value = runif(n(), .25, .5)) %>%\n  mutate(color_value = if_else(is.na(color_value), \n                               hcl(base_hue, base_chroma, med_luminance),\n                               color_value))\n\noutline_circles <- tibble(\n  circle_id = rep(c(1, 2), each = 360*2),\n  x = rep(seq(1, 100, length.out = 360*2), 2),\n  y = rep(seq(1, 100, length.out = 360*2), 2)\n) %>%\n  mutate(x = cos(x) + 2*(circle_id-1),\n         y = sin(y)) %>%\n  group_by(circle_id) %>%\n  mutate(xend = lead(x),\n         yend = lead(y)) %>%\n  filter(!is.na(xend)) %>%\n  mutate(hue = base_hue,\n         chroma = base_chroma,\n         luminance = med_luminance) %>%\n  mutate(color_value = hcl(hue, chroma, luminance))\n\nsky <- sky %>%\n  mutate(hue = base_hue + rnorm(n(), 0, sd = 7),\n         chroma = (base_chroma / 2) + rnorm(n(), 0, sd = 2),\n         luminance = bright_luminance + rnorm(n(), 0, sd = 2)) %>%\n  mutate(color_value = hcl(hue, chroma, luminance),\n         size_value = runif(n(), .5, 2)) %>%\n  mutate(color_value = if_else(is.na(color_value), \n                               hcl(base_hue, base_chroma, bright_luminance),\n                               color_value))\n\nFinally, we graph everything.\n\nggplot() +\n  geom_segment(data = outline_circles, aes(x = x, y = y, \n                                           xend = xend, yend = yend,\n                                           group = circle_id,\n                                           color = color_value),\n               size = .25) +\n  geom_point(data = background_stars, aes(x = new_x, y = new_y, color = color_value,\n                                          size = size_value),\n             shape = 16) +\n  geom_segment(data = curves, aes(x = new_x, y = new_y,\n                               xend = next_new_x, yend = next_new_y),\n               color = hcl(base_hue, base_chroma, light_luminance),\n               size = .15) +\n  geom_point(data = sky, aes(x = star_new_x, y = star_new_y, \n             color = hcl(base_hue, base_chroma, dark_luminance),\n             size = size_value + 1)) +\n  geom_point(data = sky, aes(x = star_new_x, y = star_new_y, color = color_value,\n                             size = size_value)) +\n  scale_color_identity() +\n  scale_radius(range = c(.01, .25)) +\n  scale_x_continuous(limits = c(-1, 3)) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  coord_equal() +\n  theme_void() +\n  theme(plot.background = element_rect(fill = hcl(base_hue, base_chroma, dark_luminance)),\n        legend.position = \"none\")\n\nHere are the first five images using set.seed(x).\n\n\n\nImage 1\n\n\n\n\n\nImage 2\n\n\n\n\n\nImage 3\n\n\n\n\n\nImage 4\n\n\n\n\n\nImage 5"
  },
  {
    "objectID": "posts/2021-12-20-az-900-exam-flashcards/index.html",
    "href": "posts/2021-12-20-az-900-exam-flashcards/index.html",
    "title": "AZ-900 Exam Flashcards",
    "section": "",
    "text": "The R code is pretty simple. To set up the presentation, you’ll need the latest version of the revealjs package (Any version of reveal.js past version 4 should work for the r-stack option.). The following front-matter setup should work fine. You can turn on the shuffle option to shuffle the cards every time the document is re-knitted.\n\n---\noutput: \n  revealjs::revealjs_presentation:\n    incremental: false\n    transition: 'none'\n    background_transition: 'none'\n    slide_level: 1\n    css: custom.css\n    reveal_options:\n      controls: false\n      slideNumber: false\n      progress: false\n      # shuffle: true # uncomment to randomize cards\n    self_contained: false\n---\n\nThe following code is the only markdown needed. It’ll read in the Excel file and loop through the rows to create the presentation. Each flashcard is a different slide. The term is the first fragment with the definition second because the r-stack places them on top of each other. The data-fragment-index is the same for both, so the term will fade out while the definition fades in on the same click. The line of breaks moves the term up a little. I just think it looks a little better.\n```{r results='asis', echo=FALSE}\nlibrary(readxl)\nterms <- read_excel(here::here(\"az 900 Skills.xlsx\"))\n\nfor(i in 1:nrow(terms)) {\n  term <- terms$Term[i]\n  definition <- terms$Definition[i]\n  \n  cat(paste(\"\n    <section class=\\\"center\\\">\\n\n    <div class=\\\"r-stack\\\">\\n\n    <div class=\\\"fragment fade-out\\\" data-fragment-index=\\\"1\\\"><b>\",\n    term,\n    \"</b><br> \\n <br> \\n <br>\n    </div>\\n\"))\n  \n  cat(paste(\n    \"<div class=\\\"fragment fade-in\\\" data-fragment-index=\\\"1\\\">\",\n    definition,\n    \"</div>\\n\n    </div>\\n\n    </section>\"))\n}\n```\nThere are opportunities to color the slides based on the subject area, add additional information in the notes, or filter the terms."
  },
  {
    "objectID": "posts/2020-05-15-calathea-theme/index.html",
    "href": "posts/2020-05-15-calathea-theme/index.html",
    "title": "Calathea Theme",
    "section": "",
    "text": "The base of the color scheme comes from the Calathea genus, which is a popular houseplant. I picked the colors by pulling them from searching Google or pictures I had. These were images downloaded from the web, and then the colors were picked using the Paint app on Windows 10. Any color picker in a browser or app will probably work. I saved all of the base colors as hex codes in a csv. I played around with the colors by using color-hex.com. Inputting a hex color here will give you several variations on shades, tints, and monochromatic colors. These options are useful when I wanted something a little brighter or darker.\n\n\n\n\n\nMy Calathea Medallion\n\n\n\n\nI tested out the colors by making an icon image. The icon was mostly for fun but did produce a suitable code file to test out the colors in RStudio. There are three different versions, one for each background. The code to create them can be found here.\n\n\n\n\n\n\nbase background\n\n\n\n\n\n\n\nselection/highlight\n\n\n\n\n\n\n\nR Markdown chunk\n\n\n\n\nThe foreground/background contrasts were determined to see if they passed WCAG AA standards for normal text. All the foreground colors have at least a 4.5:1 contrast with all the background colors.\n\n\n\n\n \n  \n    Feature \n    Color \n    Base Background \n    Selection/Highlight \n    R markdown chunk \n  \n \n\n  \n    Code \n    White \n    13.50 \n    11.05 \n    15.32 \n  \n  \n    Comment \n    Purple \n    5.66 \n    4.63 \n    6.42 \n  \n  \n    Keyword Contants \n    Pink \n    10.97 \n    8.99 \n    12.45 \n  \n  \n    Keyword Functions \n    Green \n    5.51 \n    4.51 \n    6.25 \n  \n  \n    Number \n    Dull \n    8.81 \n    7.22 \n    10.00 \n  \n  \n    String \n    Yellow \n    11.81 \n    9.67 \n    13.40 \n  \n\n\n\n\n\nI originally shot for AAA, but I couldn’t get that working while having the foreground colors stand apart from each other. The code to determine the contrast values is here.\nI also attempted to check color-blindness compatibilities using this simulator from color-blindness.com. These colors look ok but might need improvement for monochrome.\n\n\n\n\n\nbase colors\n\n\n\n\n\n\n\ndeuteranopia\n\n\n\n\n\n\n\nmonochromacy\n\n\n\n\nAt the end of this process, I ended up with this set of colors.\n\n\n\n\n \n  \n    Feature \n    Color Name \n    HEXCOL \n  \n \n\n  \n    R Markdown Chunk \n    Darker Dark Green \n    #1E271A \n  \n  \n    Background \n    Dark Green \n    #263121 \n  \n  \n    Selection \n    Lighter Dark Green \n    #353E31 \n  \n  \n    Highlight \n    Lighter Dark Green \n    #353E31 \n  \n  \n    Keyword Functions \n    Green \n    #85B258 \n  \n  \n    Comment \n    Purple \n    #C29CB7 \n  \n  \n    Number \n    Dull \n    #CAD3C1 \n  \n  \n    String \n    Yellow \n    #E9F5B9 \n  \n  \n    Code \n    White \n    #FCFFFB \n  \n  \n    Keyword Operator \n    White \n    #FCFFFB \n  \n  \n    Keyword Contants \n    Pink \n    #FFDFE5 \n  \n  \n    Python Meta \n    Pink \n    #FFDFE5 \n  \n\n\n\n\n\nI set up the RStudio theme using the Making and Sharing Themes instructions. The only change from the tmTheme to the rstheme is a manual edit to the R markdown chunk. The default was too bright, so the edit makes it a lot darker. There are still some areas of the IDE that can’t be edited without more advance CSS. I will deal with that another time.\n\n\n\nRStudio\n\n\n\n\n\n\nRStudio code\n\n\n\n\n\n\nRStudio R markdown\n\n\n NOTE: This is for the old website design \nThe color scheme was converted to CSS files for Hexo and highlight.js here using this code. To get these colors working syntax highlighting for blogdown, the custom CSS parameter needed to be set after moving the calathea_hexo.css to the CSS folder in static. The config file with these options:[[params.customCSS]] href = \"css/calathea_hexo.css\" will set up everything.\nFinally, to get the color scheme working with the correct R code syntax on the website, the files for R code syntax for highlight.js were downloaded from here and moved to the js folder in the static folder. To have this blogdown theme use this JavaScript file, the config option syntaxHighlighter = \"highlight.js\" needed to be set. Then to get the r.min.js file working, the custom JavaScript parameter needed to be set, similar to the custom CSS option. The [[params.customJS]] src = \"js/r.min.js\" was put in the config file. SQL and Python are pre-built into the theme. However, a python.min.js file was adapted from highlight.js 9.8.0 file with None True False moved to literal in order to get None to color successfully. The 9.8.0 version is needed to work with the current theme. This version number was found in the theme documentation.\n\n################################\n## R code syntax highlighting ##\n################################\n\nlibrary(ggplot2)\n\ncentre <- function(x, type, ...) {\n  switch(type,\n         mean = mean(x),\n         median = median(x),\n         trimmed = mean(x, trim = .1))\n}\n\nmyVar1\nmyVar.2\ndata$x\nfoo \"bar\" baz\n# test \"test\"\n\"test # test\"\n\n(123) (1) (10) (0.1) (.2) (1e-7)\n(1.2e+7) (2e) (3e+10) (0x0) (0xa)\n(0xabcdef1234567890) (123L) (1L)\n(0x10L) (10000000L) (1e6L) (1.1L)\n(1e-3L) (4123.381E-10i)\n(3.) (3.E10) # BUG: .E10 should be part of number\n\n# Numbers in some different contexts\n1L\n0x40\n.234\n3.\n1L + 30\nplot(cars, xlim=20)\nplot(cars, xlim=0x20)\nfoo<-30\nmy.data.3 <- read() # not a number\nc(1,2,3)\n1%%2\n\n\"this is a quote that spans\nmultiple lines\n\\\"\n\nis this still a quote? it should be.\n# even still!\n\n\" # now we're done.\n\n'same for\nsingle quotes #'\n\n# keywords\nNULL, NA, TRUE, FALSE, Inf, NaN, NA_integer_,\nNA_real_, NA_character_, NA_complex_, function,\nwhile, repeat, for, if, in, else, next, break,\n..., ..1, ..2\n\n# not keywords\nthe quick brown fox jumped over the lazy dogs\nnull na true false inf nan na_integer_ na_real_\nna_character_ na_complex_ Function While Repeat\nFor If In Else Next Break .. .... \"NULL\" `NULL` 'NULL'\n\n# operators\n+, -, *, /, %%, ^, >, >=, <, <=, ==, !=, !, &, |, ~,\n->, <-, <<-, $, :, ::\n\n# infix operator\nfoo %union% bar\n%\"test\"%\n`\"test\"`\n\n\n################################\n## Python syntax highlighting ##\n################################\n\n@requires_authorization\ndef somefunc(param1='', param2=0):\n    r'''A docstring'''\n    if param1 > param2: # interesting\n        print 'Gre\\'ater'\n    return (param2 - param1 + 1 + 0b10l) or None\n\nclass SomeClass:\n    pass\n\n>>> message = '''interpreter\n... prompt'''\n\n\n/***************************/\n/* SQL syntax highlighting */\n/***************************/\n\nCREATE TABLE \"topic\" (\n    \"id\" serial NOT NULL PRIMARY KEY,\n    \"forum_id\" integer NOT NULL,\n    \"subject\" varchar(255) NOT NULL\n);\nALTER TABLE \"topic\"\nADD CONSTRAINT forum_id FOREIGN KEY (\"forum_id\")\nREFERENCES \"forum\" (\"id\");\n\n-- Initials\ninsert into \"topic\" (\"forum_id\", \"subject\")\nvalues (2, 'D''artagnian');"
  },
  {
    "objectID": "posts/2022-05-20-cross-validation-a-toy-example/index.html",
    "href": "posts/2022-05-20-cross-validation-a-toy-example/index.html",
    "title": "Cross-validation: A Toy Example",
    "section": "",
    "text": "Background\n\n\n\nThis blog post comes from a presentation I gave a few years ago. It will walk through a demonstration of cross-validation for comparing predictive models. I assume you already know about cross-validation and won’t spend much time explaining it. Working through how this presentation functions is the main focus. The exercise works by having your co-workers (or other people) stand on a large grid marked by string. Each person has a toy animal representing their class, either African mammal or dinosaur, with information attached.\nThe toy also has a tag with:\n\nID number\nx-position of X1\ny-position of X2\nfold number.\n\nI used X1 and X2 instead of x and y since y is often reserved for the class, but x and y can be used. People will move on and off the grid as the toys move in and out of hold-out sets for different folds. Predictive models are built and tested to determine the best one. There are many variations on this idea, so feel free to change this up to suit your situation.\nI’d recommend starting with a brief explanation of cross-validation and when/why it is used. I usually have slides with a curve of data (like y = x^2 + noise). Then show three models: a line, a curve, and one that goes through all data points but is very squiggly. Then add in some data to estimate and show that the curve fits best. Or something similar to get the general picture.\n(As a side note, this presentation was inspired by the Dance Your Ph.D. contest. I’m not good with music, but if you are, I’m pretty confident this could be a dance instead.)\n\n\n\nSetup\n\n\n\nThe following picture gives a general idea of the setup. I’m using a table for the photography, but it would be much larger on the floor. I had small cones for the axis labels, but there are small pieces of paper here. String or tape can mark the grid lines. The axes are generic X1 and X2, but they can represent variables like height/weight ratio or transformations like PCA.\n\n\n\nSetup Grid\n\n\nThe exercise aims to create models using ribbons, to separate the mammals from dinosaurs. The ribbons are different lengths to represent the different flexibility of models. Then, we’ll test each of the models using cross-validation to estimate how well each one would perform on new data.\nI created test data sets to determine the toys’ positions and folds until I got some with the desired attributes. I used twelve animals, but your number will depend on the people you have. There are three folds and three models. Each fold has four animals, two mammals and two dinosaurs. The three models are a straight line, a curve, and a squiggle. I wanted some variety with accuracy, none of the models to be perfect, and have Model 2 to be the best. I created a decision boundary of a curve to get started. All animals are positioned on integers, with African mammals on the left/below the curve and dinosaurs on the right/above. Then I sampled data sets until I got some with the desired properties, compared them, and chose one with nice images. You can see the code in this repo.\n\n\n\nFull Data Set\n\n\n\nThe following images show a graphic of the data and a picture of the toys in place. In the graphic, mammals are yellow while dinosaurs are green. The toys are in place based on their tag’s information. The tea towel holds the assessment data. People can just walk off the grid to any place in real life. For this blogpost, graphics on the left would be displayed during the presentation to guide the exercise, while the pictures on the right show the physical experience.\n\n\n\n\n\n\nFull Data Set - Graphic\n\n\n\n\n\n\n\nFull Data Set - Picture\n\n\n\n\n\n\n\n\nFolds and Models\n\n\n\nThe next image shows which animals are in each fold. This image shows when people should move off the grid. The fold IDs are also on the tag attached to each toy.\n\n\n\nFolds Identification\n\n\n\nFold 1\nSo to set up the first fold, people with a fold ID of 1 move off the grid. These people are in the assessment set while everyone else is in the analysis set. (Sometimes called validation and training set.)\n\n\n\n\n\n\nFold 1 - Graphic\n\n\n\n\n\n\n\nFold 1 - Picture\n\n\n\n\n\n\n\nFold 1 Model 1\nFor Model 1, we have a piece of ribbon that stretches to a line across the grid. For this model, the ribbon is always a straight line. The analysis set people attempt to set the line down to separate the mammals from the dinosaurs. In this case, a straight line cannot separate the animals. So, the elephant in the bottom right corner ends up on the wrong side. The graphic displays the error with a green on the outside (labeled as a dinosaur) and yellow on the inside (actually a mammal) dot. For the graphics, dots with two colors are mismatches.\n\n\n\n\n\n\nFold 1 - Analysis Set - Model 1 - Graphic\n\n\n\n\n\n\n\nFold 1 - Analysis Set - Model 1 - Picture\n\n\n\n\n\nIt’s important to emphasize the analysis set doesn’t know about the assessment set. This way, people don’t try to remember where they were to improve accuracy. Driving home this point really helped explain the importance of cross-validation and how it estimates model performance on unseen data.\nWe bring the people in the hold-out area back onto the grid for the assessment set. The ribbon marking the model stays in place. For my presentation, I kept everyone on the grid. But for the pictures, it was easier to see the results by moving the analysis set out of the way. In the end, the model mislabels one of the dinosaurs because it’s on the wrong side. Its dot is two colors, and the toy is knocked over.\n\n\n\n\n\n\nFold 1 - Assessment Set - Model 1 - Graphic\n\n\n\n\n\n\n\nFold 1 - Assessment Set - Model 1 - Picture\n\n\n\n\n\n\n\nFold 2\nThe people in fold 2 move off the grid for the next step. Then a straight line model is built.\n\n\n\n\n\n\nFold 2 - Graphic\n\n\n\n\n\n\n\nFold 2 - Picture\n\n\n\n\n\n\n\nFold 2 Model 1\nIn this example, the model built by code mismarks the dinosaur in the model. I choose to move the model and correct it for the picture. In an actual presentation, people will move these however they want. So they are unlikely to match the computer result every time. You can either keep track of both results or only focus on the computer output.\n\n\n\n\n\n\nFold 2 - Assessment Set - Model 1 - Graphic\n\n\n\n\n\n\n\nFold 2 - Assessment Set - Model 1 - Picture\n\n\n\n\n\nWhen we add back in the assessment set, two errors occur: the giraffe in the top left and a dinosaur towards the bottom right.\n\n\n\n\n\n\nFold 2 - Analysis Set - Model 1 - Graphic\n\n\n\n\n\n\n\nFold 2 - Analysis Set - Model 1 - Picture\n\n\n\n\n\n\n\nFold 3\nFold 3 repeats the same action with the last set of people.\n\n\n\n\n\n\nFold 3 - Graphic\n\n\n\n\n\n\n\nFold 3 - Picture\n\n\n\n\n\n\n\nFold 3 Model 1\nWe end up building a model again. This time everyone in the analysis set can be labeled correctly, but we’ll miss the elephant in the bottom right of the assessment set.\n\n\n\n\n\n\nFold 3 - Assessment Set - Model 1 - Graphic\n\n\n\n\n\n\n\nFold 3 - Assessment Set - Model 1 - Picture\n\n\n\n\n\n\n\n\n\n\n\nFold 3 - Analysis Set - Model 1 - Graphic\n\n\n\n\n\n\n\nFold 3 - Analysis Set - Model 1 - Picture\n\n\n\n\n\n\n\nFold 1 Model 2\nThis set will mirror the first part of the exercise but use a different model. The new model is a longer piece of ribbon that can have a slight curve. Apart from that, everything is the same.\n\n\n\n\n\n\nFold 1 - Assessment Set - Model 2 - Graphic\n\n\n\n\n\n\n\nFold 1 - Assessment Set - Model 2 - Picture\n\n\n\n\n\n\n\n\n\n\n\nFold 1 - Analysis Set - Model 2 - Graphic\n\n\n\n\n\n\n\nFold 1 - Analysis Set - Model 2 - Picture\n\n\n\n\n\nFor fold 1, we finally get an analysis set and an assessment set with 100% accuracy. This shows that adding some complexity to the model can help.\n\n\nFold 2 Model 2\nThis follows the same pattern as the previous fold.\n\n\n\n\n\n\nFold 2 - Assessment Set - Model 2 - Graphic\n\n\n\n\n\n\n\nFold 2 - Assessment Set - Model 2 - Picture\n\n\n\n\n\n\n\n\n\n\n\nFold 2 - Analysis Set - Model 2 - Graphic\n\n\n\n\n\n\n\nFold 2 - Analysis Set - Model 2 - Picture\n\n\n\n\n\n\n\nFold 3 Model 2\nIn this fold, we do have an error in the assessment set. I like to see at least one error for each model to demonstrate that it can be impossible to get everything correct.\n\n\n\n\n\n\nFold 3 - Assessment Set - Model 2 - Graphic\n\n\n\n\n\n\n\nFold 3 - Assessment Set - Model 2 - Picture\n\n\n\n\n\n\n\n\n\n\n\nFold 3 - Analysis Set - Model 2 - Graphic\n\n\n\n\n\n\n\nFold 3 - Analysis Set - Model 2 - Picture\n\n\n\n\n\n\n\nFold 1 Model 3\nWe have the longest ribbon for the most flexible model for the final model. The exercise follows the previous steps.\n\n\n\n\n\n\nFold 1 - Assessment Set - Model 3 - Graphic\n\n\n\n\n\n\n\nFold 1 - Assessment Set - Model 3 - Picture\n\n\n\n\n\n\n\n\n\n\n\nFold 1 - Analysis Set - Model 3 - Graphic\n\n\n\n\n\n\n\nFold 1 - Analysis Set - Model 3 - Picture\n\n\n\n\n\n\n\nFold 2 Model 3\n\n\n\n\n\n\nFold 2 - Assessment Set - Model 3 - Graphic\n\n\n\n\n\n\n\nFold 2 - Assessment Set - Model 3 - Picture\n\n\n\n\n\n\n\n\n\n\n\nFold 2 - Analysis Set - Model 3 - Graphic\n\n\n\n\n\n\n\nFold 2 - Analysis Set - Model 3 - Picture\n\n\n\n\n\n\n\nFold 3 Model 3\nThere is more room for the final model to move the ribbon. The flexibility can create new patterns that don’t capture the data as well.\n\n\n\n\n\n\nFold 3 - Assessment Set - Model 3 - Graphic\n\n\n\n\n\n\n\nFold 3 - Assessment Set - Model 3 - Picture\n\n\n\n\n\n\n\n\n\n\n\nFold 3 - Analysis Set - Model 3 - Graphic\n\n\n\n\n\n\n\nFold 3 - Analysis Set - Model 3 - Picture\n\n\n\n\n\n\n\n\nOverall Accuracy\n\n\n\nWe can wrap up the exercise by looking at the models’ overall accuracy. For this setup, Model 2 performed the best. This outcome shows that having some flexibility can fit the data better, but adding too much complexity to the model can hurt.\n\n\n\nAccuracy Results\n\n\n\nThe last topic to mention is the toys used. I got African mammals and dinosaurs because they were at the store near me, but I think it’s a great combination. Most people know enough about them to have a conversation on variables that can help separate them. At the same time, it’s not a serious or business-related topic. So, it’s more interesting than just using colored points, but you can also avoid people getting too caught up in the weeds. Feel free to change it to whatever you want."
  },
  {
    "objectID": "posts/2020-04-16-website-design-part-2/index.html",
    "href": "posts/2020-04-16-website-design-part-2/index.html",
    "title": "Website Design: Part 2",
    "section": "",
    "text": "This post continues the work from the previous one. This code takes the saved data and creates the images. The first two sections set up the rest of the work.\n\n#----------\n# Libraries\n#----------\nlibrary(here)\nlibrary(ggplot2)\nlibrary(grid)\n\n\n#-------------\n# Read in data\n#-------------\nfreqs <- read.csv(here::here(\"freqs.csv\"))\nfreqs\n\n\n#-------------\n# Color Scheme\n#-------------\nbackground <- \"#263121\"\n\ncol <- c(\n  \"Overall\" = \"#85B258\",\n  \"Data Science\" = \"#CAD3C1\",\n  \"Background\" = background\n)\n\npng(filename = here::here(\"images\", \"color_scheme.png\"))\nop <- par(mar = rep(0, 4))\nplot(0, 0, pch = NA, axes = F,\n     xlab = \"\", ylab = \"\",\n     xlim = c(0, 1), ylim = c(0, 1),\n     asp = 1)\nfor(i in seq_len(length(col))) {\n  rect((i / length(col)) - (1 / length(col)), 0, (i / length(col)), 1,\n       col = col[i], border = background)\n}\npar(op)\ndev.off()\n\n\n\nThe previous section picks the color scheme for the graphics. The colors come from the Calathea theme found here. The color scheme sets a dark green for the background with two lighter greens for the bars.\nThe next section creates the image for the background. Clicking the hamburger button at the top left of this website will show the graphic.\n\n\n\n\n\n\n#-------\n# Cover\n#-------\n\n# Flip one set of freqs to get back-to-back\nfreqs[freqs$Source == \"Overall\", ]$Frequency <-\n  -freqs[freqs$Source == \"Overall\", ]$Frequency\n\n# Actual plot\np <- ggplot(freqs, aes(x = rev(Letter), y = Frequency, fill = Source)) +\n  geom_bar(stat = \"identity\", color = background, alpha = .5) +\n  scale_fill_manual(values = col) +\n  # No guides since just for looks\n  guides(fill = FALSE) +\n  # Double y-max limit size so the site's title doesn't block the image\n  scale_y_continuous(expand = c(0, 0), limits = c(-15, 75), breaks = 0) +\n  scale_x_discrete(limits = rev(levels(freqs$Letter))) +\n  coord_flip() +\n  theme(\n    panel.background = element_rect(fill = background),\n    text = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    title = element_blank())\n\n# How to just get the plot\n# http://stackoverflow.com/questions/14313285/ggplot2-theme-with-no-axes-or-grid\ngt <- ggplot_gtable(ggplot_build(p))\nge <- subset(gt$layout, name == \"panel\")\ngrid.draw(gt[ge$t:ge$b, ge$l:ge$r])\n\n# Cover needs to be 1920 x 1080 according to image used on demo site\npng(filename = here::here(\"images\", \"cover.png\"),\n    width = 1920, height = 1080)\ngrid.draw(gt[ge$t:ge$b, ge$l:ge$r])\ndev.off()\n\n\nThe right side of the image is left blank because it is not seen. The size comes from the demo site documentation. As a standard Hugo/blogdown site using the Tranquilpeak theme, the cover image is set in the config.toml file as coverImage = \"images/website/cover.png\".\nFinally, the next section creates the icon that appears on the tab on the home page.\n\n#-----------\n# Icon\n#-----------\n# Undo \"Flip one set of freqs to get back-to-back\"\nfreqs[freqs$Source == \"Overall\", ]$Frequency <-\n  -freqs[freqs$Source == \"Overall\", ]$Frequency\n\n# reset plot\nplot(0, 0,\n     pch = NA, axes = F,\n     xlab = \"\", ylab = \"\",\n     xlim = c(0, 1), ylim = c(0, 1))\n\n# Just use \"Tyler\"\nfreqs_sub <- freqs[freqs$Letter %in% c(\"t\", \"y\", \"l\", \"e\", \"r\"), ]\n\n# Get in right order\nfreqs_sub$Letter <- factor(freqs_sub$Letter,\n                           levels = c(\"t\", \"y\", \"l\", \"e\", \"r\"))\np <- ggplot(freqs_sub, aes(x = Letter, y = Frequency, fill = Source)) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  scale_color_manual(values = col) +\n  scale_fill_manual(values = col) +\n  geom_hline(yintercept = 0, color = background) +\n  # No guides since just for looks\n  guides(fill = FALSE) +\n  # Double y-max limit size so the site's title doesn't block the image\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 15), breaks = 0) +\n  theme(\n    # transparent background, so it matches any background\n    panel.background = element_rect(fill = \"transparent\", color = NA),\n    text = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    title = element_blank(),\n    axis.line.x = element_line(color = background))\n\n# http://stackoverflow.com/questions/14313285/ggplot2-theme-with-no-axes-or-grid\ngt <- ggplot_gtable(ggplot_build(p))\nge <- subset(gt$layout, name == \"panel\")\ngrid.draw(gt[ge$t:ge$b, ge$l:ge$r])\n\n# Site Icon needs to be 512x512 according to site design\n# transparent background, so it matches any background\npng(filename = here::here(\"images\", \"icon.png\"),\n    width = 512,\n    height = 512,\n    bg = \"transparent\")\ngrid.draw(gt[ge$t:ge$b, ge$l:ge$r])\ndev.off()\n\n\nThe background is transparent so that it doesn’t appear out of place when moved. The bars come from the same data as the background but only using the letters of my name. The config.toml setting is favicon = \"images/website/icon.png\". The source code file resides here."
  },
  {
    "objectID": "posts/2021-06-28-sql-server-machine-learning-services-presentation/index.html",
    "href": "posts/2021-06-28-sql-server-machine-learning-services-presentation/index.html",
    "title": "SQL Server’s Machine Learning Services Presentation",
    "section": "",
    "text": "I initially gave this presentation in November 2020 for the Asheville SQL Meetup. It covers the basics of SQL Server’s Machine Learning Services. Hopefully, it provides enough information to help people decide if this tech is a good solution for them.\n\n\n\n\n\nThis presentation covers some background on Machine Learning Services, like what it is and why you would want to use it. There’s a demo section that showcases three different ways to use R in production with SQL Server. Finally, there’s a section for some tips and tricks that helped me when I set up and use this tech.\n\n\n\n\n\nThe first section is just on the background.\n\n\n\n\n\nMachine Learning Services is a SQL Server feature that allows you to bring analytics into the database by running the analytics programming language in-database. It first started in 2016 with R and later added Python support. Some old documentation will call it R services because that was the name before adding Python. This is not a standalone server. It’s a regular SQL Server with added features. There is another option for a standalone server. That might be a better choice for your needs, but we’ll not cover that in this presentation.\n\n\n\n\n\nIf you’re wondering if this would be a good addition to your tech stack, here are some excellent use cases: forecasting next month’s revenue, customer churn to predict when types of customers are mostly like to leave, and finding potential fraud in claims data. Basically, anything that is a best guess or probability. It’s not for pulling data or summaries. You can do that, but it won’t be better than using SQL typically. You can do some predictive modeling with just SQL, but usually, using R or Python has more features and is easier to use. Also, for this presentation, when I say ‘model’, I’m talking about a predictive or statistical model, not a data model that’s commonly used in SQL settings.\n\n\n\n\n\nSpeeding up predictive model processing is the main reason to add this tech. By bringing the analysis to the data, you don’t have to move large data sets out of SQL Server to get the results you need. This can cut out a huge amount of time and processing power. You get to utilize SQL for the data prep and R or Python for the analysis. This brings you the best of both worlds. SQL tends to be a lot more stable and optimized for data prep, while R and Python can handle the large machine learning models that would too difficult to code in SQL. Also, Machine Learning Services outputs within SQL Server, which is easily used for reporting or an ETL.\n\n\n\n\n\nThis setup also allows you to have clean analytics projects in production. There’s no need for large wonky code sets to get everything glued together. You can use all the standard SQL objects, like stored procedures and jobs. So, you don’t need another tool for scheduling or error reporting. There is less pipeline work since you’re not moving data in and out of SQL Server. So there are fewer ETLs, permissions issues, and other places for things to break.\nI think the addition of ‘fewer things to break’ for this work is an important selling point. If you’re adding in tech, then you’re also adding in more options for failure. That’s made up for to get the predictive modeling capabilities but reducing all the other code helps put this option ahead of other predictive modeling tools. There are a variety of setups, but you can pick a specific one for your team or organization. The flexibility is nice, so this tool works for most companies. Then when you get a setup for your team, you can set standards, so everything follows a pattern for documentation and code.\n\n\n\n\n\nThis tech is a great setup because it can ease collaboration between data scientists and DBAs. It utilizes SQL permissions. So no one needs to learn a new setup for that, and it can fit nicely into your current structure. It has Microsoft-approved security. You’re not bringing in unvalidated software on your server. And it has options for resource controls. If you’re worried about data science work taking over everything, you can put up gates to prevent that. As a side note, I’ve never needed to use this feature. Most resource-intensive data science work happens during development, not running them in production, and I build the models in a different environment.\n\n\n\n\n\nThe real-time analysis can be too slow. We’ll compare different options during the demo section, so you’ll see the fast ones. If you can wait a few seconds to return results or run everything in a batch the night before you need them, you’re good to go. If machine learning services are too slow, then SQL Server is probably too slow.\nThe big data operations can be too small. This is more of a limitation of SQL Server again. Depending on your data size, this might not be big enough, but then SQL Server is probably not the right tool anyways.\nThis doesn’t work as well with Power BI and other interfaces as you might expect, especially for interactivity. If you can’t run a stored procedure or run all the options before pulling out the data, you’ll have a lot of limitations here.\n\n\n\n\n\nWhen you’re planning out setting this up, there are a few things to figure out. You should have some idea of the roles you want involved. Machine learning services gives teams a whole spectrum of responsibilities that can be used to set expectations and tasks between DBAs and data scientist. So, you can have a setup with a ton of DBA involvement or very little work, depending on what you want at your company. You’ll want to know about the use cases you’ll have. In the demo section, we’ll walk through a few options. Determining which setup is best for your team is pretty much determined by your use cases. The number of people involved can also play a factor. You can easily interfere with other people’s work if you’re not careful, so some setups are better with larger teams. Overall, there is a lot of flexibility but no strong consensus on how to use this tech.\n\n\n\n\n\nNow for the demo section.\n\n\n\n\n\nYou need to have SQL Server installed first. I’m using the 2017 Developer Edition. You’ll go through normal installation for most of this. (https://www.microsoft.com/en-us/sql-server/sql-server-2017 - ‘Free download’ link near the bottom)\n\n\n\n\n\nThe main catch is making sure to check the boxes for Machine Learning Services (In-Database). You can pick R, Python, or both.\nDo not pick Machine Learning Server (Standalone). That’s a different product.\n\n\n\n\n\nYou’ll go through a few more options and restarts. You can test that it’s working with a little ‘Hello World’ script.\n\n\n\n\n\nThe primary function you’ll use is sp_execute_external_script. It’s the way to access an external language. There are a bunch of parameters. The specific language (R or Python). The actual script to run. The input data set. The output data that gets returned. And a lot of other options too.\n\n\n\n\n\nFor a big disclaimer. I’ll run some code that creates predictive models, but I’m taking a lot of shortcuts. I’m not working through choosing a model, testing, and checking. I’m just getting a small model setup for the demo. The SQL objects and functions are fine to use. That’s the main focus of this presentation and can be adapted very easily for your company.\n\n\n\n\n\n– run 1_set_up_checks.sql\n        \n\n/* EXEC sp_configure  'external scripts enabled', 1\nRECONFIGURE WITH OVERRIDE */\n\n-- EXECUTE sp_configure  'external scripts enabled'\n\nEXEC sp_execute_external_script  @language = N'R',\n@script = N'\nOutputDataSet <- InputDataSet;\n',\n@input_data_1 = N'SELECT 1 AS hello'\nWITH RESULT SETS (([hello] int not null));\nGO\n\n-- Check Version\nEXECUTE sp_execute_external_script @language = N'R'\n    , @script = N'print(version)';\nGO\n\n-- R Packages\nEXEC sp_execute_external_script @language = N'R'\n    , @script = N'\nOutputDataSet <- data.frame(installed.packages()[,c(\"Package\", \"Version\", \"Depends\", \"License\", \"LibPath\")]);'\nWITH result sets((\n            Package NVARCHAR(255)\n            , Version NVARCHAR(100)\n            , Depends NVARCHAR(4000)\n            , License NVARCHAR(1000)\n            , LibPath NVARCHAR(2000)\n            ));\nThis shows that everything looks good.\n– run 2_create_mtcars.sql\n-- mtcars database\nCREATE TABLE dbo.mtcars(\n    mpg decimal(10, 1) NOT NULL,\n    cyl int NOT NULL,\n    disp decimal(10, 1) NOT NULL,\n    hp int NOT NULL,\n    drat decimal(10, 2) NOT NULL,\n    wt decimal(10, 3) NOT NULL,\n    qsec decimal(10, 2) NOT NULL,\n    vs int NOT NULL,\n    am int NOT NULL,\n    gear int NOT NULL,\n    carb int NOT NULL\n);\n\nINSERT INTO dbo.mtcars\nEXEC sp_execute_external_script @language = N'R'\n    , @script = N'MTCars <- mtcars;'\n    , @input_data_1 = N''\n    , @output_data_1_name = N'MTCars';\n\nSELECT * FROM dbo.mtcars;\n\n--DROP TABLE dbo.mtcars;\nThis builds out a test data set for us to use.\n– run 3_mtcars_sql.sql\n-- Input data set\nSELECT mpg, cyl, hp, wt FROM dbo.mtcars;\n-- 1974 Motor Trends\n-- miles per gallon\n-- number of cylinders\n-- horse power\n-- weight (1000 lbs)\n\n-- Build the model -----------------------------------------------------------------------\nEXEC sp_execute_external_script\n    @language = N'R'\n    , @script = N'cars_model <- lm(mpg ~ cyl + hp + wt, data = mtcars_data);\n        trained_model <- data.frame(model = as.raw(serialize(cars_model, connection=NULL)));'\n    , @input_data_1 = N'SELECT mpg, cyl, hp, wt FROM dbo.mtcars'\n    , @input_data_1_name = N'mtcars_data'\n    , @output_data_1_name = N'trained_model'\n    WITH RESULT SETS ((model VARBINARY(max)))\n\nCREATE TABLE predictive_models (\n    model_name varchar(30) not null default('default model') primary key,\n    model varbinary(max) not null\n);\n\nINSERT INTO predictive_models(model)\nEXEC sp_execute_external_script\n    @language = N'R'\n    , @script = N'cars_model <- lm(mpg ~ cyl + hp + wt, data = mtcars_data);\n        trained_model <- data.frame(model = as.raw(serialize(cars_model, connection=NULL)));'\n    , @input_data_1 = N'SELECT mpg, cyl, hp, wt FROM MTCars'\n    , @input_data_1_name = N'mtcars_data'\n    , @output_data_1_name = N'trained_model';\n\nSELECT * FROM [master].[dbo].[predictive_models];\n\nUPDATE predictive_models\nSET model_name = 'lm_' + format(getdate(), 'dd-MM-yy')\nWHERE model_name = 'default model'\n\nSELECT * FROM [master].[dbo].[predictive_models];\n\n--DROP TABLE dbo.predictive_models;\n\n-- Run model ---------------------------------------------------------------------------\nDECLARE @selected_lmmodel varbinary(max) = \n    (SELECT model FROM dbo.predictive_models WHERE model_name = 'lm_17-11-20');\n\nEXEC sp_execute_external_script\n    @language = N'R'\n    , @script = N'\n            current_model <- unserialize(as.raw(lmmodel));\n            new <- data.frame(mtcars_data);\n            predicted.am <- predict(current_model, new);\n            str(predicted.am);\n            OutputDataSet <- cbind(new, predicted.am);\n            '\n    , @input_data_1 = N'SELECT cyl, hp, wt FROM dbo.mtcars'\n    , @input_data_1_name = N'mtcars_data'\n    , @params = N'@lmmodel varbinary(max)'\n    , @lmmodel = @selected_lmmodel\nWITH RESULT SETS ((cyl INT, hp INT, wt DECIMAL(10, 2), predicted_mpg DECIMAL(10, 2)));\nThis builds a model, saves it to a new table, and scores data with the new model. You can add more models to this table or update the existing ones. This is the most basic setup. Everything’s pretty much kept in SQL Server with basic functionalities.\n\n\n\n\nThere is an option for Native Scoring. This doesn’t use the overhead of R and Python but uses another format. The trade-off is there is a smaller list of possible models, but these are the most common. You’ll use RevoScaleR or revoscalepy, depending on if you’re using R or Python. The main function here is PREDICT. So look for that in the code. (Open Neural Network Exchange (ONNX))\n\n\n\n\n\n– run 4_mtcars_nativescoring.sql\n        \n\n-- Input data set\nSELECT mpg, cyl, hp, wt FROM dbo.mtcars;\n\n-- Build the model -----------------------------------------------------------------------\nDECLARE @model varbinary(max);\nEXECUTE sp_execute_external_script\n  @language = N'R'\n  , @script = N'\n    cars_model <- rxLinMod(mpg ~ cyl + hp + wt, data = mtcars_data)\n    model <- rxSerializeModel(cars_model, realtimeScoringOnly = TRUE)'\n  , @input_data_1 = N'SELECT mpg, cyl, hp, wt FROM dbo.mtcars'\n  , @input_data_1_name = N'mtcars_data'\n  , @params = N'@model varbinary(max) OUTPUT'\n  , @model = @model OUTPUT\n  INSERT [dbo].[predictive_models]([model_name], [model])\n  VALUES('native_scoring', @model) ;\n\nSELECT * FROM [master].[dbo].[predictive_models];\n\n--DROP TABLE dbo.predictive_models;\n\n-- Run model ---------------------------------------------------------------------------\nDECLARE @model varbinary(max) = (\n  SELECT model\n  FROM [master].[dbo].[predictive_models]\n  WHERE model_name = 'native_scoring');\n\nSELECT d.*, p.*\n  FROM PREDICT(MODEL = @model, DATA = dbo.mtcars as d)\n  WITH(mpg_Pred float) as p;\n\n--DROP TABLE dbo.predictive_models;\nThis builds the model, adds it to the table, and scores some data. You can see it is very similar to the other setup.\n\n\n\n\nThe last way to run a model in production I’ll talk about is using an R package. R packages are a pretty common mechanism to wrap up a lot of R code. You’ll see them for almost everything. This provides the most separation between R and SQL by wrapping up most of the R code in the package then using SQL to pass data to it. You can make changes to the package to change the model instead of changing any SQL code. You’ll just need to use an ALTER statement to update it. This is probably the cleanest route for when a lot of R code needs to run.\n\n\n\n\n\nWhen you’re creating a model to use in a package, there might be some versioning issues. Using these settings might resolve them.\n\n\n\n\n\n– run 5_mtcars_rpackage.sql\n        \n\nCREATE EXTERNAL LIBRARY mtcarsmodel\nFROM (CONTENT = 'mtcarsmodel_0.1.0.zip') WITH (LANGUAGE = 'R'); --pull from GitHub\n\nEXEC sp_execute_external_script @language = N'R'\n    , @script = N'\nOutputDataSet <- data.frame(installed.packages()[,c(\"Package\", \"Version\", \"Depends\", \"License\", \"LibPath\")]);'\nWITH result sets((\n            Package NVARCHAR(255)\n            , Version NVARCHAR(100)\n            , Depends NVARCHAR(4000)\n            , License NVARCHAR(1000)\n            , LibPath NVARCHAR(2000)\n            ));\n-- !!! NOTE THE DIFFERENT LIBPATH !!! ---\n\n-- DROP EXTERNAL LIBRARY mtcarsmodel;\n\nEXEC sp_execute_external_script \n@language =N'R', \n@script=N'library(mtcarsmodel)';\n-- Note the warning message about R 4.0.0\n\n-- Run model ---------------------------------------------------------------------------\nEXEC sp_execute_external_script\n    @language = N'R'\n    , @script = N'\n            library(mtcarsmodel)\n            OutputDataSet <- mtcarsmodel::predict_mtcars(mtcars_data);'\n    , @input_data_1 = N'SELECT cyl, hp, wt FROM dbo.mtcars'\n    , @input_data_1_name = N'mtcars_data'\nWITH RESULT SETS ((cyl INT, hp INT, wt DECIMAL(10, 2), predicted_mpg DECIMAL(10, 2)));\n–(Make sure to have the package pulled from GitHub and saved in as a zipfile.)\nThis code uploads an R package that contains a pre-built model. It then uses machine learning services to score data with that package.\n\n\n\n\n– run 6_mt_cars_big.sql\n        \n\n-- mtcars big table\nCREATE TABLE dbo.mtcars_big(\n    mpg decimal(10, 1) NOT NULL,\n    cyl int NOT NULL,\n    disp decimal(10, 1) NOT NULL,\n    hp int NOT NULL,\n    drat decimal(10, 2) NOT NULL,\n    wt decimal(10, 3) NOT NULL,\n    qsec decimal(10, 2) NOT NULL,\n    vs int NOT NULL,\n    am int NOT NULL,\n    gear int NOT NULL,\n    carb int NOT NULL\n);\n\nINSERT INTO dbo.mtcars_big\nEXEC sp_execute_external_script @language = N'R'\n    , @script = N'MTCars <- mtcars[sample(1:nrow(mtcars), 1000000, replace = TRUE), ];'\n    , @input_data_1 = N''\n    , @output_data_1_name = N'MTCars';\n\n--DROP TABLE dbo.mtcars_big;\n\nCREATE TABLE #mtcars_big_predictions (\n    cyl INT, \n    hp INT, \n    wt DECIMAL(10, 2), \n    mpg_Pred FLOAT)\n\n\n/* SQL */ -------------------------------------------------------------------------------------------      \nDECLARE @lmmodel varbinary(max) = \n    (SELECT model FROM dbo.predictive_models WHERE model_name = 'lm_17-11-20'); -- MAKE SURE TO CHANGE THE MODEL NAME\n\nINSERT INTO #mtcars_big_predictions\n\nEXEC sp_execute_external_script\n    @language = N'R'\n    , @script = N'\n            current_model <- unserialize(as.raw(lmmodel));\n            new <- data.frame(mtcars_data);\n            predicted.am <- predict(current_model, new, type = \"response\");\n            str(predicted.am);\n            OutputDataSet <- cbind(new, predicted.am);\n            '\n    , @input_data_1 = N'SELECT cyl, hp, wt FROM dbo.mtcars_big'\n    , @input_data_1_name = N'mtcars_data'\n    , @params = N'@lmmodel varbinary(max)'\n    , @lmmodel = @lmmodel;\n\n-- Seconds: 16\n\n-- check results \nSELECT TOP 1000 * FROM #mtcars_big_predictions;\nTRUNCATE TABLE #mtcars_big_predictions;\n\n\n/* Native Scoring */ -------------------------------------------------------------------------------------------\nDECLARE @model varbinary(max) = (\n  SELECT model\n  FROM [master].[dbo].[predictive_models]\n  WHERE model_name = 'native_scoring');\n\nINSERT INTO #mtcars_big_predictions\n\nSELECT d.cyl, d.hp, d.wt, p.*\n  FROM PREDICT(MODEL = @model, DATA = dbo.mtcars_big as d)\n  WITH(mpg_Pred float) as p;\n\n-- Seconds: 8\n\n-- check results \nSELECT TOP 1000 * FROM #mtcars_big_predictions;\nTRUNCATE TABLE #mtcars_big_predictions;\n\n/* R Package */ ---------------------------------------------------------------------------------------------\nINSERT INTO #mtcars_big_predictions\n\nEXEC sp_execute_external_script\n    @language = N'R'\n    , @script = N'\n            library(mtcarsmodel)\n            OutputDataSet <- mtcarsmodel::predict_mtcars(mtcars_data);'\n    , @input_data_1 = N'SELECT cyl, hp, wt FROM dbo.mtcars_big'\n    , @input_data_1_name = N'mtcars_data';\n--WITH RESULT SETS ((cyl INT, hp INT, wt DECIMAL(10, 2), predicted_mpg DECIMAL(10, 2)));\n\n-- Seconds: 14\n\n-- check results \nSELECT TOP 1000 * FROM #mtcars_big_predictions;\nTRUNCATE TABLE #mtcars_big_predictions;\n\n-- DROP TABLE dbo.mtcars_big;\n\n-- Try this with clearing cache\nThis code builds out a bigger data set to see run time comparisons. It’s good for a lot of batch analytics, and the native scoring is the best.\n\n\n\n\nNow for the tips and tricks section.\n\n\n\n\n\nSmall catches and errors will definitely be the most painful part. You’ll hit a lot of minor mistakes the first time you try to get something working. Here are a few to watch out for in your work. R and Python are case-sensitive. So the input data names have to match exactly, unlike SQL. Input/output formats can trouble. This includes the number of columns, names, and data types. R, Python, and SQL Server have slightly different data types. So, you might hit a weird issue, especially with conversions. Error messaging can be cloudy at best. You’ll get a lot of weird messages that are either a downstream effect of the real problem or are just some esoteric response. You can get this if there’s something wrong in the R code, but you’ll get the final issue as a SQL problem.\nThere is also a whole world of versioning that we won’t get to in this presentation. You’ll need to look into upgrading versions of Machine Learning Services. Probably choose something using miniCran, R Open, and the checkpoint package. Finally, you’ll need to figure out any dependencies for the packages of predictive models you might have. The tech on the last bullet point can help you out here.\n\n\n\n\n\nA lot of documentation and blog posts are hit or miss because they deal with outdated practices or features that don’t work with your setup. If you search around enough, you will find some help eventually though.\nYou can use all the standard SQL objects/tools. Everything can be wrapped up in a stored procedure, which can be very nice. You can save and update models in a temporal table to compare model performance over time. And you can use a separate for all models or a different database for each project to keep data pulls, reference tables, and outputs altogether.\n\n\n\n\n\nMake sure to write documentation as you go. Then at the end, delete the setup and restart following the documentation. You’ll probably want to delete and restart several times. There will be little catches you’ll need to remember later, and having them written down will save you.\nI like to script everything. I find it easier for reproducibility and to pass off stuff to coworkers. If you mainly have an R shop, the sqlmlutils package will be helpful. If you’re mainly SQL, you can use standard SQL code for everything. At my job, everything gets passed off to another developer for putting models on our production server. I’ve got scripts that pick up the R packages, place them in the right spot, and test everything.\n\n\n\n\n\nLastly, sometimes in development Machine Learning Services will stop working. The main place to check is restarting the launchpad. This will fix it most of the time. There’s nothing really special about this slide. Just to know to do this because you’ll almost definitely run into this issue."
  },
  {
    "objectID": "posts/2021-08-09-choosing-nearby-colors-part-1/index.html",
    "href": "posts/2021-08-09-choosing-nearby-colors-part-1/index.html",
    "title": "Choosing Nearby Colors Part 1",
    "section": "",
    "text": "Now for the R code\n\n\n\nThese libraries set up data manipulation, combining graphs, and using the HCL color space.\n\n##---------\n# Libraries\n##---------\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(colorspace)\n\nWe’ll set up some code to get a green base color based on H, C, and L. We’ll also create a data set called color_points. This will only have one observation for right now, but we will add in more rows later. It’ll serve as a placeholder for functions that graph different shapes.\n\n##------------\n# Pick a color\n##------------\nH_point <- 112\nC_point <- 60\nL_point <- 68\n\ncolor_hex <- hcl(H_point, \n                 C_point,\n                 L_point,\n                 fixup = FALSE)\n\ncolor_points <- data.frame(x = C_point * cos(H_point * pi/180),\n                           y = C_point * sin(H_point * pi/180),\n                           z = L_point,\n                           H = H_point,\n                           C = C_point,\n                           L = L_point,\n                           color_value = color_hex,\n                           perpendicular_from_C_L = 0,\n                           parallel_along_C_L = 0,\n                           row_value = 0,\n                           col_value = 0)\n\nLet’s see where this base color exists in the HCL color space and create some helper functions.\nWe’ll start with looking at the Chroma-Luminance plane (C-L Plane). We want to graph slicing the HCL color space in half from top to bottom along the line H = 112. (H_point = 112)\n\n##--------------------------------\n# See color in H, C, L color space \n##--------------------------------\n# C-L Plane ----\nget_C_L_plane <- function(H_point) {\n  expand_grid(H = H_point,\n              C = seq(0, 180, .5),\n              L = seq(1, 100, .5)) %>%\n    mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n    filter(!is.na(color_value))\n}\nC_L_plane <- get_C_L_plane(H_point)\n\ngraph_C_L_plane <- function(C_L_plane, color_points, color_hex) {\n  ggplot() +\n    geom_point(data = C_L_plane,\n               aes(C, L, color = color_value, fill = color_value)) +\n    geom_point(data = color_points,\n               aes(C, L, color = \"white\", fill = \"white\")) +\n    scale_x_continuous(labels = abs) +\n    scale_color_identity() +\n    scale_fill_identity() +\n    geom_point(aes(x = C_point,\n                   y = L_point),\n               color = 'black',\n               fill = color_hex,\n               shape = 21,\n               size = 2) +\n    coord_equal()\n}\ngraph_C_L_plane(C_L_plane, color_points, color_hex)\n\n\n\n\nBase Color: C-L Plane\n\n\nWe can see as the Luminance increases, the shades move from darker to lighter, and as Chroma moves out from 0, the color is more intense. The get_C_L_plane function returns a data set with points on that plane. For this blog post, functions that start with “get” return points we’re going to graph while functions that start with “graph” display them appropriately. This plane actually extends to the left, where the H value would be the current H + 180. I’m not graphing that section because this project will keep values close to the base color without changing the Hue too much.\nFrom here down, I’ll hide some of the code similar to previous sections to shorten the post. You can click on [texts] to show code if you want to see it.\nNext, we can see the color falls with all the other colors for the same Chroma value. I think about Chroma values as tree rings. So, this image takes the HCL color space, drills out the center for lower Chroma values, then has you stand in the middle facing the Hue value, pulling the shape away from behind you and laying it flat.\n\n\n[H-L Curve Code]\n# H-L Curve ----\nget_H_L_curve <- function(C_point) {\n  expand_grid(H = seq(1, 360, 1),\n              C = C_point,\n              L = seq(1, 100, .5)) %>%\n    mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n    filter(!is.na(color_value))\n}\nH_L_curve <- get_H_L_curve(C_point)\n\nlabel_H_center <- function(H_point, ...) {\n  function(x) {(x + (180 - H_point)) %% 360}\n}\n\ngraph_H_L_curve <- function(H_L_curve, color_points, color_hex, H_point) {\n  ggplot() +\n    geom_point(data = H_L_curve,\n               aes((H + (180 - H_point)) %% 360, L, \n                   color = color_value, fill = color_value)) +\n    geom_point(data = color_points,\n               aes((H + (180 - H_point)) %% 360, L, \n                   color = \"white\", fill = \"white\")) +\n    scale_color_identity() +\n    scale_fill_identity() +\n    geom_point(aes(x = 180, # Because we rotated points to not drop over edge\n                   y = L_point),\n               color = 'black',\n               fill = color_hex,\n               shape = 21,\n               size = 2) +\n    scale_x_reverse('H', # Like you're standing on the inside\n                    labels = label_H_center(H_point = H_point),\n                    limits = c(360, 0)) +\n    coord_equal()\n}\ngraph_H_L_curve(H_L_curve, color_points, color_hex, H_point)\n\n\n\n\n\nBase Color: H-L Curve\n\n\nYou can see the odd shape of the HCL color space where the different Hues don’t stretch their Chroma values out at different Luminance values. This is the only graph that shows a flattened curve. Everything else displays a sharp slice. For all the H-L curve graphs, the Hue value is rotated to the center.\nNow we can look at cutting horizontally through the HCL color space where L = 68. (L_point = 68)\n\n\n[H-C Plane Code]\n# H-C plane ----\nget_H_C_plane <- function(L_point){\n  expand_grid(H = seq(1, 360, 1),\n              C = seq(0, 180, .5),\n              L = L_point) %>%\n    mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n    filter(!is.na(color_value))\n}\nH_C_plane <- get_H_C_plane(L_point)\n\ngraph_H_C_plane <- function(H_C_plane, color_points, color_hex) {\n  ggplot() +\n    geom_point(data = H_C_plane,\n               aes(H, C, color = color_value, fill = color_value)) +\n    geom_point(data = color_points,\n               aes(H, C, color = \"white\", fill = \"white\")) +\n    scale_color_identity() +\n    scale_fill_identity() +\n    scale_x_continuous(breaks = seq(45, 360, 45),\n                       minor_breaks = seq(0, 315, 45) + 45/2,\n                       labels = c('45', '90', '135', '180', \n                                  '225', '270', '315', '0|360')) +\n    scale_y_continuous(limits = c(0, 180)) +\n    geom_point(aes(x = H_point,\n                   y = C_point),\n               color = 'black',\n               fill = color_hex,\n               shape = 21,\n               size = 2) +\n    coord_polar(start = 270 * pi / 180,\n                direction = -1)\n}\ngraph_H_C_plane(H_C_plane, color_points, color_hex)\n\n\n\n\n\nBase Color: H-C Plane\n\n\nWe can see the different colors as Hue moves around a circle and their increased intensity as Chroma moves out to the edges. The shape is not circular because the HCL color space isn’t.\nThere is one more graph to see. We looked at cutting the HCL color space along the Hue, Chroma, and Luminance values, but the Chroma image was a flattened curve. So instead, we can cut a plane at the Chroma value but tangent to the circle a constant Chroma value creates. The following image sets up the explanation, and then we’ll see the actual plane.\n\n\n[C Tangent Plane Setup Code]\n# C tangent plane ----\nC_circle <- data.frame(H = seq(1, 360),\n                       C = C_point,\n                       color_value = \"white\")\nC_tangent_plane <- expand_grid(x = C_point, # Plane perpendicular to H at C\n                       perpendicular_from_C_L = \n                         seq(-sqrt(180^2 - C_point^2), sqrt(180^2 - C_point^2)),\n                       L = seq(1, 100, 1)) %>%\n  mutate(x_rotate = x * cos(H_point * pi/180) -  # rotate\n           perpendicular_from_C_L * sin(H_point * pi/180),\n         y_rotate = x * sin(H_point * pi/180) + \n           perpendicular_from_C_L * cos(H_point * pi/180)) %>%\n  mutate(x = x_rotate,\n         y = y_rotate) %>%\n  select(-x_rotate, -y_rotate)  %>%\n  mutate(H = (atan2(y, x) * 180/pi) %% 360,\n         C = sqrt(x^2 + y^2)) %>%\n  mutate(color_value = \"white\")\nggplot(data = H_C_plane,\n       aes(H, C, color = color_value, fill = color_value)) +\n  geom_point() +\n  scale_color_identity() +\n  scale_fill_identity() +\n  scale_x_continuous(breaks = seq(45, 360, 45),\n                     minor_breaks = seq(0, 315, 45) + 45/2,\n                     labels = c('45', '90', '135', '180', \n                                '225', '270', '315', '0|360')) +\n  scale_y_continuous(limits = c(0, 180)) +\n  geom_path(data = C_circle) +\n  geom_segment(x = H_point,\n               y = 0,\n               xend = H_point,\n               yend = C_point,\n               col = \"white\") +\n  geom_point(data = C_tangent_plane, col = \"black\") +\n  geom_point(x = H_point,\n             y = C_point,\n             color = 'black',\n             fill = color_hex,\n             shape = 21) +\n  coord_polar(start = 270 * pi / 180,\n              direction = -1)\n\n\n\n\n\nC Tangent Plane setup\n\n\nChroma and Luminance move in straight lines, but Hue is circular. The image shows this with the white circle where Chroma and Luminance are constant, but Hue moves around the circle. This means looking at graphs of shapes can be distorted when graphing them flat. So we might want to see what happens as we move away from our specific color in a straight line perpendicular to the C-L Plane. That’s the black line. We’re going to cut the HCL space from top to bottom along this line.\n\n\n[C Tangent Plane Code]\nget_C_tangent_plane <- function(H_point, C_point) {\n  expand_grid(x = C_point, # Plane perpendicular to H at C\n              perpendicular_from_C_L = seq(-180, 180, .5),\n              L = seq(1, 100, .5)) %>%\n    mutate(x_rotate = x * cos(H_point * pi/180) -  # rotate\n             perpendicular_from_C_L * sin(H_point * pi/180),\n           y_rotate = x * sin(H_point * pi/180) + \n             perpendicular_from_C_L * cos(H_point * pi/180)) %>%\n    mutate(x = x_rotate,\n           y = y_rotate) %>%\n    select(-x_rotate, -y_rotate)  %>%\n    mutate(H = (atan2(y, x) * 180/pi) %% 360,\n           C = sqrt(x^2 + y^2)) %>%\n    mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n    filter(!is.na(color_value))\n}\nC_tangent_plane <- get_C_tangent_plane(H_point, C_point)\n\ngraph_C_tangent_plane <- function(C_tangent_plane, color_points, color_hex) {\n  ggplot() +\n    geom_point(data = C_tangent_plane,\n               aes(perpendicular_from_C_L, L, \n                   color = color_value, fill = color_value)) +\n    geom_point(data = color_points,\n               aes(perpendicular_from_C_L, L, \n                   color = \"white\", fill = \"white\")) +\n    scale_color_identity() +\n    scale_fill_identity() +\n    scale_x_reverse(\"Distance Perpendicular to C-L Plane\",\n                    labels = abs) +\n    geom_point(aes(x = 0,\n                   y = L_point),\n               color = 'black',\n               fill = color_hex,\n               shape = 21,\n               size = 2) +\n    coord_equal()\n}\ngraph_C_tangent_plane(C_tangent_plane, color_points, color_hex)\n\n\n\n\n\nBase Color: Plane Perpendicular to C-L Plane\n\n\nThis shows the plane perpendicular to the C-L Plane at C = 60. A horizontal line drawn at the point on this image matches where the black intersects the H-C plane in the previous graph. This image isn’t super helpful here, but it will be when we check shapes later.\n\n\n\nNearby points in a sphere\n\n\n\nLet’s start with drawing random points inside a sphere with a width of 5 centered on our base color.\nThe code below draws random points in a unit sphere, stretches it to be the right size, then moves it to our base color. After that, the code converts it to HCL coordinates, converts these to a color, then creates some nice variables to use for plotting later.\n\n##------\n# Sphere\n##------\nradius <- 5\nn_points <- 250^2\n\ncolor_points <- data.frame(x = rnorm(n = n_points),\n                           y = rnorm(n = n_points),\n                           z = rnorm(n = n_points),\n                           U = runif(n = n_points)^(1/3)) %>%\n  mutate(normalize = sqrt(x^2 + y^2 + z^2)) %>%\n  mutate(x = x * U / normalize,\n         y = y * U / normalize,\n         z = z * U / normalize) %>%\n  select(-U, -normalize) %>% # have random points in a sphere here\n  mutate(x = x * radius, # stretch\n         y = y * radius,\n         z = z * radius) %>%\n  mutate(x = x + C_point * cos(H_point * pi/180), # move\n         y = y + C_point * sin(H_point * pi/180),\n         z = z + L_point) %>%\n  mutate(H = (atan2(y, x) * 180/pi) %% 360,\n         C = sqrt(x^2 + y^2),\n         L = z) %>%\n  mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n  mutate(perpendicular_from_C_L = x * sin(-H_point * pi/180) + \n           y * cos(-H_point * pi/180),\n         parallel_along_C_L = x * cos(-H_point * pi/180) - \n           y * sin(-H_point * pi/180)) %>%\n  mutate(row_value = sample(row_number(), n()),\n         col_value = ceiling(row_value / sqrt(n_points))) %>%\n  mutate(row_value = (row_value %% sqrt(n_points)) + 1)\n\nHere we see the values, base color, and sample points.\n\n\n[Sphere Info Code]\ngraph_info <- function(H_point, C_point, L_point) {\n  color_hex <- hcl(H_point, \n                   C_point,\n                   L_point,\n                   fixup = FALSE)\n  \n  ggplot() +\n    geom_rect(aes(xmin = 0, xmax = 1,\n                  ymin = 0, ymax = .5), col = color_hex, fill = color_hex) +\n    geom_text(data = data.frame(x = 0,\n                                y = seq(1.5, .75, -.25),\n                                label = c(paste(\"HEX Value:\", color_hex), \n                                          paste(\"H Value:\", H_point),\n                                          paste(\"C Value:\", C_point),\n                                          paste(\"L Value:\", L_point))),\n              aes(x, y, label = label), hjust = 0, size = 4) +\n    coord_equal() +\n    theme_void()\n}\n\ngraph_sample <- function(color_points) {\n  ggplot(data = color_points,\n         aes(x = row_value,\n             y = col_value,\n             fill = color_value)) +\n    geom_tile() +\n    coord_equal() +\n    scale_fill_identity() +\n    theme_void()\n}\n\np1 <- graph_info(H_point, C_point, L_point)\np2 <- graph_sample(color_points)\np1 + p2\n\n\n\n\n\nSphere: Base Color and Sample Points\n\n\nWe can reuse the previous functions to graph the planes with our sphere in white to get the outline shape by using the new points as the color_points parameter. Then we can just graph the new points to see how they look. For example, in the following image, the left side has the previous C-L Plane image with points from the sphere blocked out in white. However, the right side has those same points with the correct color.\n\n\n[C-L Plane Code]\ngraph_C_L <- function(color_points) {\n  ggplot(data = color_points, aes(C, L, col = color_value, fill = color_value)) +\n    geom_point() +\n    scale_color_identity() +\n    scale_fill_identity() +\n    coord_equal() +\n    theme(axis.line=element_blank(), axis.text.x=element_blank(),\n          axis.text.y=element_blank(), axis.ticks=element_blank(),\n          axis.title.x=element_blank(), axis.title.y=element_blank())\n}\n\np1 <- graph_C_L_plane(C_L_plane, color_points, color_hex)\np2 <- graph_C_L(color_points)\np1 + p2\n\n\n\n\n\nSphere: C-L Plane\n\n\nNow, we can continue with the others, starting with the H-L Curve. It’s hard to tell in this image, but the shape isn’t a perfect circle. It’s slightly off because the Hue values curve through the sphere, then that intersection is flattened in the graph. This distortion is more evident for different values.\n\n\n[H-L Curve Code]\ngraph_H_L <- function(color_points, H_point) {\n  ggplot(data = color_points, aes((H + (180 - H_point)) %% 360, L, \n                                  col = color_value, fill = color_value)) +\n    geom_point() +\n    scale_color_identity() +\n    scale_fill_identity() +\n    scale_x_reverse() +\n    coord_equal() +\n    theme(axis.line=element_blank(), axis.text.x=element_blank(),\n          axis.text.y=element_blank(), axis.ticks=element_blank(),\n          axis.title.x=element_blank(), axis.title.y=element_blank())\n}\n\np1 <- graph_H_L_curve(H_L_curve, color_points, color_hex, H_point)\np2 <- graph_H_L(color_points, H_point)\np1 + p2\n\n\n\n\n\nSphere: H-L Curve”\n\n\nThe next shape is a perfect circle because the plane perpendicular to the C-L Plane is already flat.\n\n\n[C Tangent Plane Code]\ngraph_perpendicular_from_C_L <- function(color_points) {\n  ggplot(data = color_points, aes(perpendicular_from_C_L, L, \n                                  color = color_value, fill = color_value)) +\n    geom_point() +\n    scale_color_identity() +\n    scale_fill_identity()  +\n    scale_x_reverse() +\n    coord_equal() +\n    theme(axis.line=element_blank(), axis.text.x=element_blank(),\n          axis.text.y=element_blank(), axis.ticks=element_blank(),\n          axis.title.x=element_blank(), axis.title.y=element_blank())\n}\n\np1 <- graph_C_tangent_plane(C_tangent_plane, color_points, color_hex)\np2 <- graph_perpendicular_from_C_L(color_points)\np1 + p2\n\n\n\n\n\nSphere: Plane Perpendicular to C-L Plane\n\n\nThe last image is of the H-C Plane and the sphere based on x-y coordinates. The black line on the right-side is at H = 112.\n\n\n[H-C Plane Code]\ngraph_x_y <- function(color_points, H_point) {\n  ggplot(data = color_points, aes(x, y, \n                                  col = color_value, fill = color_value)) +\n    geom_abline(slope = c(tan(-67.5 * pi/180), \n                          tan(-45 * pi/180), \n                          tan(-22.5 * pi/180),\n                          0, \n                          100000,\n                          tan(22.5 * pi/180), \n                          tan(45 * pi/180), \n                          tan(67.5 * pi/180)), \n                intercept = 0,\n                color = \"white\") +\n    geom_abline(slope = tan(H_point * pi/180), \n                intercept = 0,\n                color = \"black\") +\n    geom_point() +\n    scale_color_identity() +\n    scale_fill_identity() +\n    coord_equal() +\n    theme(axis.line=element_blank(), axis.text.x=element_blank(),\n          axis.text.y=element_blank(), axis.ticks=element_blank(),\n          axis.title.x=element_blank(), axis.title.y=element_blank(),\n          panel.grid.major=element_blank(), panel.grid.minor=element_blank())\n}\n\np1 <- graph_H_C_plane(H_C_plane, color_points, color_hex)\np2 <- graph_x_y(color_points, H_point)\np1 + p2\n\n\n\n\n\nSphere: H-C Plane\n\n\nNow to expand this technique out a little more, we can stretch the sphere in different ways.\n\n\n\nNearby points in an ellipse\n\n\n\nThe following code starts and ends with the same lines as the previous code for points in a sphere. There’s are just two changes: stretching the sphere based on different amounts and rotating the points to line up the axes correctly. The radius parameter gets broken into three: H_radius, C_radius, and L_radius. The C_radius and L_radius stretch the sphere along those directions from the point. The H_radius is a slight misnomer because it’s stretching perpendicular to the C-L Plane, which is similar to how Hue changes but doesn’t exactly match the curve.\n\n##-------\n# Ellipse\n##-------\nH_radius <- 2.5\nC_radius <- 5\nL_radius <- 10\n\ncolor_points <- data.frame(x = rnorm(n = n_points),\n                           y = rnorm(n = n_points),\n                           z = rnorm(n = n_points),\n                           U = runif(n = n_points)^(1/3)) %>%\n  mutate(normalize = sqrt(x^2 + y^2 + z^2)) %>%\n  mutate(x = x * U / normalize,\n         y = y * U / normalize,\n         z = z * U / normalize) %>%\n  select(-U, -normalize) %>% # have random points in a sphere here\n  mutate(x = x * C_radius, # stretch\n         y = y * H_radius,\n         z = z * L_radius) %>%\n  mutate(x_turn = x * cos(H_point * pi/180) - \n           y * sin(H_point * pi/180), # rotate\n         y_turn = x * sin(H_point * pi/180) + \n           y * cos(H_point * pi/180)) %>%\n  mutate(x = x_turn,\n         y = y_turn) %>%\n  select(-x_turn, -y_turn) %>%\n  mutate(x = x + C_point * cos(H_point * pi/180), # move\n         y = y + C_point * sin(H_point * pi/180),\n         z = z + L_point) %>%\n  mutate(H = (atan2(y, x) * 180/pi) %% 360,\n         C = sqrt(x^2 + y^2),\n         L = z) %>%\n  filter(L >= 0 & L <= 100 & C >= 0) %>%\n  mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n  mutate(perpendicular_from_C_L = x * sin(-H_point * pi/180) + \n           y * cos(-H_point * pi/180),\n         parallel_along_C_L = x * cos(-H_point * pi/180) - \n           y * sin(-H_point * pi/180)) %>%\n  mutate(row_value = sample(row_number(), n()),\n         col_value = ceiling(row_value / sqrt(n_points))) %>%\n  mutate(row_value = (row_value %% sqrt(n_points)) + 1)\n\n\n\n[Ellipse Info Code]\np1 <- graph_info(H_point, C_point, L_point)\np2 <- graph_sample(color_points)\np1 + p2\n\n\n\n\n\nEllipse: Base Color and Sample Points\n\n\nIn this section’s images, we can see how the sphere gets stretched. If you’re standing in the center of the HCL color space and face the point, the sphere was extended to your left and right by the H_radius amount, to and away from you by C_radius, and vertically by L_radius.\n\n\n[C-L Plane Code]\np1 <- graph_C_L_plane(C_L_plane, color_points, color_hex)\np2 <- graph_C_L(color_points)\np1 + p2\n\n\n\n\n\nEllipse: C-L Plane\n\n\nIn the previous image, we can see the ellipse is 2 * L_radius tall and 2 * C_radius wide. In the following image, the ellipse is also 2 * L_radius tall. It changed horizontally but not by 2 * H_radius. This image displays the C_point radius circles as H changes through the HCL color space, so the distance of stretching is a function of the arc length of those circles. The next image has a width of 2 * H_radius.\n\n\n[H-L Curve Code]\np1 <- graph_H_L_curve(H_L_curve, color_points, color_hex, H_point)\np2 <- graph_H_L(color_points, H_point)\np1 + p2\n\n\n\n\n\nEllipse: H-L Curve\n\n\n\n\n[C Tangent Plane Code]\np1 <- graph_C_tangent_plane(C_tangent_plane, color_points, color_hex)\np2 <- graph_perpendicular_from_C_L(color_points)\np1 + p2\n\n\n\n\n\nEllipse: Plane Perpendicular to C-L Plane\n\n\nFinally, we see the H-C plane with an ellipse with one axis 2 * C_radius long and the other of 2 * H_radius. It’s pointing to the center, so which axis appears as height and width would change as H turns.\n\n\n[H-C Plane Code]\np1 <- graph_H_C_plane(H_C_plane, color_points, color_hex)\np2 <- graph_x_y(color_points, H_point)\np1 + p2\n\n\n\n\n\nEllipse: H-C Plane\n\n\nWhile we’re transforming the original sphere, we can add in tilting.\n\n\n\nNearby points in a tilted ellipse\n\n\n\nI’m focusing on the Hue value for this project, so we’ll always tilt along the C-L Plane. Points with Hue = H_point will keep the same Hue as we rock the top and bottom either closer or farther from the center. Other points will change their Hue because they’ll move parallel to the C-L Plane and Hue is at an angle to this plane. We’ll change the Chroma and Luminance values for all points except the base color as we tilt.\nThe following code is the same as the previous one but adds tilting the ellipse by tilt_theta, the new parameter for the degree of tilt. In addition, the radius parameters have changed to map to the axis that is tilted, theta_radius, the other radius on the C-L Plane, other_C_L_radius, and the one perpendicular to the other two, perpendicular_C_L_radius (which fixes the H_radius misnomer).\nA little code also finds the tilt_theta that points the ellipse towards the farthest point on the C-L Plane. This helps stretch the ellipse without hitting an edge (but any theta can be used).\n\n##--------------\n# Tilted Ellipse\n##--------------\ntheta_radius <- 10\nother_C_L_radius <- 3\nperpendicular_C_L_radius <- 5\n\n# Try rotating to point major axis to max chroma value\nmax_chromas <- max_chroma(h = H_point, l = seq(1, 100, .5))\ntilt_theta <- atan2(seq(1, 100, .5)[max(max_chromas) == max_chromas] - L_point,\n                     max(max_chromas) - C_point)\n\ncolor_points <- data.frame(x = rnorm(n = n_points),\n           y = rnorm(n = n_points),\n           z = rnorm(n = n_points),\n           U = runif(n = n_points)^(1/3)) %>%\n  mutate(normalize = sqrt(x^2 + y^2 + z^2)) %>%\n  mutate(x = x * U / normalize,\n         y = y * U / normalize,\n         z = z * U / normalize) %>%\n  select(-U, -normalize) %>% # have random points in a sphere here\n  mutate(x = x * theta_radius, # stretch\n         y = y * other_C_L_radius,\n         z = z * perpendicular_C_L_radius) %>%\n  mutate(z_tilt = z * cos(tilt_theta) + x * sin(tilt_theta), # tilt\n         x_tilt = z * -sin(tilt_theta) + x * cos(tilt_theta)) %>%\n  mutate(x = x_tilt,\n         z = z_tilt) %>%\n  select(-x_tilt, -z_tilt) %>%\n  mutate(x_turn = x * cos(H_point * pi/180) - \n           y * sin(H_point * pi/180), # rotate\n         y_turn = x * sin(H_point * pi/180) + \n           y * cos(H_point * pi/180)) %>%\n  mutate(x = x_turn,\n         y = y_turn) %>%\n  select(-x_turn, -y_turn) %>%\n  mutate(x = x + C_point * cos(H_point * pi/180), # move\n         y = y + C_point * sin(H_point * pi/180),\n         z = z + L_point) %>%\n  mutate(H = (atan2(y, x) * 180/pi) %% 360,\n         C = sqrt(x^2 + y^2),\n         L = z) %>%\n  filter(L >= 0 & L <= 100 & C >= 0) %>%\n  mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n  mutate(perpendicular_from_C_L = x * sin(-H_point * pi/180) + \n           y * cos(-H_point * pi/180),\n         parallel_along_C_L = x * cos(-H_point * pi/180) - \n           y * sin(-H_point * pi/180)) %>%\n  mutate(row_value = sample(row_number(), n()),\n         col_value = ceiling(row_value / sqrt(n_points))) %>%\n  mutate(row_value = (row_value %% sqrt(n_points)) + 1)\n\n\n\n[Tilted Ellipse Info Code]\np1 <- graph_info(H_point, C_point, L_point)\np2 <- graph_sample(color_points)\np1 + p2\n\n\n\n\n\nTilted Ellipse: Base Color and Sample Points\n\n\nThe following image shows the tilt the best. We can see that it now points to the tip of the triangle.\n\n\n[C-L Plane Code]\np1 <- graph_C_L_plane(C_L_plane, color_points, color_hex)\np2 <- graph_C_L(color_points)\np1 + p2\n\n\n\n\n\nTilted Ellipse: C-L Plane\n\n\nThe right side isn’t quite symmetric across a horizontal line in the middle in the following image. The top is a little thinner than the bottom, so it’s more of an egg shape. That happens because the H values curve through the ellipse and flatten into this image. Different H values are obtained for different C values, and C and L are correlated in this shape. So in this image, as L changes, C also changes, which affects the H values reached by the edges. This graph won’t always result in an egg shape, but it does in this case.\n\n\n[H-L Curve Code]\np1 <- graph_H_L_curve(H_L_curve, color_points, color_hex, H_point)\np2 <- graph_H_L(color_points, H_point)\np1 + p2\n\n\n\n\n\nTilted Ellipse: H-L Curve\n\n\nThe next one is symmetric across a horizontal line in the middle. Checking this feature is one of the main reasons for this graph.\n\n\n[C Tangent Plane Code]\np1 <- graph_C_tangent_plane(C_tangent_plane, color_points, color_hex)\np2 <- graph_perpendicular_from_C_L(color_points)\np1 + p2\n\n\n\n\n\nTilted Ellipse: Plane Perpendicular to C-L Plane\n\n\nFinally, we see the H-C Plane. The length along the C-L Plane line is a function of the radii and tilt amount, but the perpendicular length is just perpendicular_C_L_radius.\n\n\n[H-C Plane Code]\np1 <- graph_H_C_plane(H_C_plane, color_points, color_hex)\np2 <- graph_x_y(color_points, H_point)\np1 + p2\n\n\n\n\n\nTilted Ellipse: H-C Plane\n\n\nSo far, this setup has a lot of flexibility but is also fragile. So, we’ll bulk up the sampling function.\n\n\n\nClean up the final function\n\n\n\nThere are a couple of ways to get points that don’t have actual values, such as outside useable Chroma values or Luminance values outside [0, 100]. To handle this, we’ll add in a feature that over samples points, only keeps the ones that have a color, then samples down to the desired amount. The new oversample parameter adds in the extra points. Of course, this could be down in a while loop, but this is fast enough and normally works.\nWe might also want to limit the Hue values. For example, if we only want red colors without going into purple or orange, we can block samples too far away based on their H values, even if our perpendicular_C_L_radius is too large. We’ll just crop any points out that go past those bounds.\nFinally, the HCL color space is oddly shaped, so it’s possible to sample points distributed unevenly along either side of the H_point value. That could shift the overall average Hue. To prevent that, I’m adding a catch that if the point couldn’t exist on the other side of the Hue values, then discard it. That’ll make the final regions trimmed out of the ellipse to be symmetric across H_point.\n\n##--------\n# Clean up\n##--------\n# sampling, hitting edges\n# within h bounds, this also catches C on the other side\n# symmetric on H\n\nH_bound <- 3 # up to 90\nget_color_points <- function(n_points, oversample,\n                             H_point, C_point, L_point,\n                             theta_radius, other_C_L_radius, \n                             perpendicular_C_L_radius,\n                             tilt_theta, H_bound) {\n  data.frame(x = rnorm(n = n_points * oversample), # over sample in case some points fail\n             y = rnorm(n = n_points * oversample),\n             z = rnorm(n = n_points * oversample),\n             U = runif(n = n_points * oversample)^(1/3)) %>%\n    mutate(normalize = sqrt(x^2 + y^2 + z^2)) %>%\n    mutate(x = x * U / normalize,\n           y = y * U / normalize,\n           z = z * U / normalize) %>%\n    select(-U, -normalize) %>% # have random points in a sphere here\n    mutate(x = x * theta_radius, # stretch\n           y = y * other_C_L_radius,\n           z = z * perpendicular_C_L_radius) %>%\n    mutate(z_tilt = z * cos(tilt_theta) + x * sin(tilt_theta), # tilt\n           x_tilt = z * -sin(tilt_theta) + x * cos(tilt_theta)) %>%\n    mutate(x = x_tilt,\n           z = z_tilt) %>%\n    select(-x_tilt, -z_tilt) %>%\n    mutate(x_turn = x * cos(H_point * pi/180) - y * sin(H_point * pi/180), # rotate\n           y_turn = x * sin(H_point * pi/180) + y * cos(H_point * pi/180)) %>%\n    mutate(x = x_turn,\n           y = y_turn) %>%\n    select(-x_turn, -y_turn) %>%\n    mutate(x = x + C_point * cos(H_point * pi/180), # move\n           y = y + C_point * sin(H_point * pi/180),\n           z = z + L_point) %>%\n    mutate(H = (atan2(y, x) * 180/pi) %% 360,\n           C = sqrt(x^2 + y^2),\n           L = z) %>%\n    filter(L >= 0 & L <= 100 & C >= 0) %>%\n    mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n    filter(!is.na(color_value)) %>% # check if exists\n    mutate(H_diff = (180 - abs(abs(H - H_point) - 180)) * \n             sign(180 - abs(H - H_point)) * sign(H - H_point)) %>% # H diff, check if crosses 360\n    filter(abs(H_diff) <= H_bound) %>% # check in H bound\n    filter(!is.na(hcl(H_point - H_diff, C, L, fixup = FALSE))) %>% # symmetric\n    select(!H_diff) %>%\n    sample_n(n_points) %>% # sample down to desired amount\n    mutate(perpendicular_from_C_L = x * sin(-H_point * pi/180) + \n             y * cos(-H_point * pi/180),\n           parallel_along_C_L = x * cos(-H_point * pi/180) - \n             y * sin(-H_point * pi/180)) %>%\n    mutate(row_value = sample(row_number(), n()),\n           col_value = ceiling(row_value / sqrt(n_points))) %>%\n    mutate(row_value = (row_value %% sqrt(n_points)) + 1)\n}\n\ncolor_points <- get_color_points(250^2, 10,\n                                 H_point, C_point, L_point,\n                                 theta_radius, other_C_L_radius, \n                                 perpendicular_C_L_radius,\n                                 tilt_theta, H_bound)\n\n\n\n\nCompare previous samples\n\n\n\nNow we can try this function with the previous values to confirm how it works.\n\n##-------\n# Compare\n##-------\nH_bound <- 90\ncolor_points_sphere <- get_color_points(250^2, 10,\n                                        H_point, C_point, L_point,\n                                        theta_radius = radius, \n                                        other_C_L_radius = radius, \n                                        perpendicular_C_L_radius = radius,\n                                        tilt_theta = 0, H_bound)\ncolor_points_ellipse <- get_color_points(250^2, 10,\n                                         H_point, C_point, L_point,\n                                         theta_radius = C_radius, \n                                         other_C_L_radius = H_radius, \n                                         perpendicular_C_L_radius = L_radius,\n                                         tilt_theta = 0, H_bound)\ncolor_points_tilted_ellipse <- get_color_points(250^2, 10,\n                                                H_point, C_point, L_point,\n                                                theta_radius, \n                                                other_C_L_radius, \n                                                perpendicular_C_L_radius,\n                                                tilt_theta, H_bound)\n\np1 <- graph_sample(color_points_sphere)\np2 <- graph_sample(color_points_ellipse)\np3 <- graph_sample(color_points_tilted_ellipse)\np1 + p2 + p3\n\n\n\n\nCompare: Sample Points\n\n\nHere, we can see the differences in samples based on the different parameters. By changing the parameters, we can get a variety of color sampling, even when starting with the same base point. The rest of the graphs can be created for comparing the outputs, but they aren’t that interesting since they’re just repeats of the previous image.\n\n\n\nTrying some other parameters\n\n\n\nNow that we have all our functions set up let’s try them out on two more examples. The first one will be on an ellipse that hits the edge.\n\n#--------------------\n# Try some other ones\n#--------------------\n\n# Outside Edge ----\nH_point <- 63\nC_point <- 93\nL_point <- 81\n\ncolor_hex <- hcl(H_point, \n                 C_point,\n                 L_point,\n                 fixup = FALSE)\n\nC_L_plane <- get_C_L_plane(H_point)\nH_L_curve <- get_H_L_curve(C_point)\nC_tangent_plane <- get_C_tangent_plane(H_point, C_point)\nH_C_plane <- get_H_C_plane(L_point)\n\ntheta_radius <- 40\nother_C_L_radius <- 3\nperpendicular_C_L_radius <- 5\n\ntilt_theta <- 0\nH_bound <- 90\n\ncolor_points <- get_color_points(250^2, 10,\n                                 H_point, C_point, L_point,\n                                 theta_radius, other_C_L_radius, \n                                 perpendicular_C_L_radius,\n                                 tilt_theta, H_bound)\n\n\n\n[Outside Edge Info Code]\np1 <- graph_info(H_point, C_point, L_point)\np2 <- graph_sample(color_points)\np1 + p2\n\n\n\n\n\nOutside Edge: Base Color and Sample Points\n\n\nThe following image shows that the ellipse should go outside the bounds, but there aren’t any color values for those points. So, the ellipse is clipped off by that bound. However, the square on the right in the previous image is filled in completely. If we started with a sample of the size we wanted at the end, the clipped points would be missing. The previous image worked because the original set of points was bigger, clipped, then sampled to the desired amount.\n\n\n[C-L Plane Code]\np1 <- graph_C_L_plane(C_L_plane, color_points, color_hex)\np2 <- graph_C_L(color_points)\np1 + p2\n\n\n\n\n\nOutside Edge: C-L Plane\n\n\nThe next few images are the same kind that we have seen previously.\n\n\n[H-L Curve Code]\np1 <- graph_H_L_curve(H_L_curve, color_points, color_hex, H_point)\np2 <- graph_H_L(color_points, H_point)\np1 + p2\n\n\n\n\n\nOutside Edge: H-L Curve\n\n\n\n\n[C Tangent Plane Code]\np1 <- graph_C_tangent_plane(C_tangent_plane, color_points, color_hex)\np2 <- graph_perpendicular_from_C_L(color_points)\np1 + p2\n\n\n\n\n\nOutside Edge: Plane Perpendicular to C-L Plane\n\n\n\n\n[H-C Plane Code]\np1 <- graph_H_C_plane(H_C_plane, color_points, color_hex)\np2 <- graph_x_y(color_points, H_point)\np1 + p2\n\n\n\n\n\nOutside Edge: H-C Plane\n\n\nOne important detail to catch in the above image is the sharp end on the top right. This clipping occurs because we check that the image is symmetric across Hue = H_point. If we didn’t have that check, the right side would stretch up farther to match the boundary of the H-C Plane on the left side.\nFinally, we can end this post with one more example. For this one, we’ll place the ellipse near the inside of the HCL color space.\n\n# Inside Edge ----\nH_point <- 319\nC_point <- 10\nL_point <- 50\n\ncolor_hex <- hcl(H_point, \n                 C_point,\n                 L_point,\n                 fixup = FALSE)\n\nC_L_plane <- get_C_L_plane(H_point)\nH_L_curve <- get_H_L_curve(C_point)\nC_tangent_plane <- get_C_tangent_plane(H_point, C_point)\nH_C_plane <- get_H_C_plane(L_point)\n\ntheta_radius <- 40\nother_C_L_radius <- 5\nperpendicular_C_L_radius <- 20\n\ntilt_theta <- 90 * pi/180\nH_bound <- 45\n\ncolor_points <- get_color_points(250^2, 10,\n                                 H_point, C_point, L_point,\n                                 theta_radius, other_C_L_radius, \n                                 perpendicular_C_L_radius,\n                                 tilt_theta, H_bound)\n\n\n\n[Inside Edge Info Code]\np1 <- graph_info(H_point, C_point, L_point)\np2 <- graph_sample(color_points)\np1 + p2\n\n\n\n\n\nInside Edge: Base Color and Sample Points\n\n\n\n\n[C-L Plane Code]\np1 <- graph_C_L_plane(C_L_plane, color_points, color_hex)\np2 <- graph_C_L(color_points)\np1 + p2\n\n\n\n\n\nInside Edge: C-L Plane\n\n\nThe H-L Curve is very different from previous ones because of how close to the center the C_point is. Values close to the center span a larger region of Hue values than compared to points farther away to the outside, which creates a new shape.\n\n\n[H-L Curve Code]\np1 <- graph_H_L_curve(H_L_curve, color_points, color_hex, H_point)\np2 <- graph_H_L(color_points, H_point)\np1 + p2\n\n\n\n\n\nInside Edge: H-L Curve\n\n\n\n\n[C Tangent Plane Code]\np1 <- graph_C_tangent_plane(C_tangent_plane, color_points, color_hex)\np2 <- graph_perpendicular_from_C_L(color_points)\np1 + p2\n\n\n\n\n\nInside Edge: Plane Perpendicular to C-L Plane\n\n\nFor the final image, we can see that the H_bound cuts up the ellipse. The boundary prevents the ellipse from stretching across the HCL color space’s middle.\n\n\n[H-C Plane Code]\np1 <- graph_H_C_plane(H_C_plane, color_points, color_hex)\np2 <- graph_x_y(color_points, H_point)\np1 + p2\n\n\n\n\n\nInside Edge: H-C Plane\n\n\n\nThere are a few options to continue this work further. This code uses the tidyverse with many mutate steps, but it can be done squished together or done with matrix multiplication/other more efficient techniques. There are also different parameterizations, such as the two foci for an ellipse. There could also be options for other clipping or not including some of the current clipping (like keeping symmetric across H_point)."
  },
  {
    "objectID": "posts/2022-05-03-pandemic-mazes/index.html",
    "href": "posts/2022-05-03-pandemic-mazes/index.html",
    "title": "Pandemic Mazes",
    "section": "",
    "text": "Background\n\n\n\nThis art project came out of my experiences during the pandemic. I thought the signs on grocery store floors were an interesting physical manifestation of the COVID rules. Especially since some places had clear signs but some didn’t, some people followed the signs but some didn’t, and some signs were consistent for a long time but some weren’t. This same situation is reflected in the other rules where some are clear, followed, and consistent, but others aren’t.\nSo I decided to make a generative art project that creates random paths based on pandemic signs. For each image created, the maze generated has the same start and endpoints, but the path in between can be complicated or straightforward. On one end of the spectrum, the maze follows mostly straight lines into the middle and then back out of the maze with consistent signage. The other end has the maze meander all over with many different signs. There’s a parameter for all of this code that causes that. Changes in this parameter can express experiences or match data for different regions/times.\nThe program works in five steps:\n\nCreate a base maze\nUpdate the maze to double all the edges\nMove through all edges in one path\nAdd in the images for signs\nFinally, add the background and save\n\n\n\n\nCreate a base maze\n\n\n\nA five-by-five square of nodes sets up each maze. A data set of edges connect the vertical and horizontal neighbors. The set for the labyrinth is marked here. The maze always starts at the bottom middle node. After that, edges are selected, checked if they can be included or if it would cause the maze to connect in on itself, and marked as part of the maze or as discard.\nA parameter ranging from 0 to 1 determines how much the maze turns. A value of 0 yields a completely random maze and 1 for following a smooth labyrinth that spirals in and then back out to the start. Any value in between sets the probability of choosing a new step at random versus moving along the labyrinth.\n\n\n[get_maze function]\nget_maze <- function(structure_parameter) {\n\n  # Sets up base data set of potential edges for the maze\n  # size is always 5 for right now\n  size <- 5\n  edges <- CJ(\n    x1 = rep(seq(1, size), 2),\n    y1 = seq(1, size)\n  )\n  edges[, \":=\"(x2 = ifelse(.I %% 2 == 0, x1 + 1, x1),\n    y2 = ifelse(.I %% 2 == 1, y1 + 1, y1))]\n  edges <- edges[x2 <= size & y2 <= size, ]\n  edges[, id := seq(1, nrow(edges))]\n  edges[, \":=\"(node1 = (x1 - 1) * size + (y1 - 1) + 1,\n    node2 = (x2 - 1) * size + (y2 - 1) + 1)]\n  setkey(edges, id)\n\n  # Set up the path for the labyrinth\n  edges[, labyrinth := 0]\n  edges[\n    .(c(20, 29, 37, 38, 39, 40, \n        36, 27, 18,  9,  7,  5, \n         3,  1,  2, 10, 12, 14, \n        17, 26, 32, 30, 22, 21)),\n    labyrinth := seq(1, 24)\n  ]\n\n  # data set of nodes\n  nodes <- unique(rbind(edges[, .(id = node1)], edges[, .(id = node2)]))\n  nodes[, connected := 0]\n  setkey(nodes, id)\n\n  # data set of node id to edge ids\n  nodes_edges <- unique(rbind(edges[, .(id = node1, edge = id)][], \n                              edges[, .(id = node2, edge = id)]))\n  setkey(nodes_edges, id)\n\n  # location : 1 for maze, 0 for frontier, -1 for uncharted, -2 for discarded\n  # starting point : bottom middle\n  # include bottom middle then either off to the sides or up\n  if (runif(1, 0, 1) <= structure_parameter) {\n    starting_edge <- edges[x1 == 3 & y1 == 1 & x2 == 4 & y2 == 1, ]\n  } else {\n    starting_edge <- edges[(x1 == 3 & y1 == 1) |\n      (x1 == 2 & y1 == 1 & x2 == 3 & y2 == 1), ][sample(.N, 1), ]\n  }\n\n  # Set up base columns\n  edges[, \":=\"(location = -1,\n    probability = 0)]\n  edges[.(starting_edge$id), \":=\"(location = 1,\n    probability = 0)]\n  nodes[.(c(starting_edge$node1, starting_edge$node2)), connected := 1]\n  edges[.(nodes_edges[.(c(starting_edge$node1, starting_edge$node2)), \"edge\"]), \":=\"\n  (location = fifelse(location == -1, 0, location),\n    probability = fifelse(location == -1, 1, probability))]\n\n  #### Loop through maze generation ----\n  num_edges <- 1\n  while (num_edges < (size^2 - 1)) {\n\n    # select next edge\n    if (runif(1, 0, 1) <= structure_parameter) {\n      selected_edge <- edges[edges$location == 0, \n                             ][max(labyrinth) == labyrinth, \n                               ][sample(.N, 1), ]\n    } else {\n      selected_edge <- edges[sample(.N, 1, prob = probability), ]\n    }\n\n    ## if it's good, then\n    # add it to the maze\n    # add connecting edges to the frontier\n    # else add it to discard\n    if (any(nodes[.(c(selected_edge$node1, selected_edge$node2))\n                  , connected] == 0)) {\n\n      # add to maze\n      edges[.(selected_edge$id), \":=\"(location = 1,\n        probability = 0)]\n\n      # update nodes\n      nodes[.(c(selected_edge$node1, selected_edge$node2)), connected := 1]\n\n      # update frontier\n      edges[.(nodes_edges[.(c(selected_edge$node1, selected_edge$node2))\n                          , \"edge\"]), \":=\"\n      (location = fifelse(location == -1, 0, location),\n        probability = fifelse(location == -1, 1, probability))]\n\n      num_edges <- num_edges + 1\n    } else {\n      # drop from frontier\n      edges[.(selected_edge$id), \":=\"(location = -2,\n        probability = 0)]\n    }\n  }\n\n  return(edges[location == 1, ])\n}\n\n\nThe following image displays the base maze layout. The edges are numbered with their x1/y1 coordinates at their left/bottom and x2/y2 at their right/top. The labyrinth pattern is highlighted in blue.\n\n\n\nMaze edges with labyrinth highlighted\n\n\n\n\n\nUpdate the maze to double all the edges\n\n\n\nFor the final image, the path needs to start at one point, move through all the nodes, then exit at one other point. To ensure this capability, the path will travel through the maze and backtrack to the starting point. This is typically done using a graph setup, but I wanted to keep the table data structure. So instead of following those instructions, I take every edge and double for each direction. The code works by replacing all possible edges with either two parallel edges (if the edge was in the maze) or two perpendicular ones (if the edge was discarded).\n\n\n[update_maze function]\nupdate_maze <- function(edges) {\n\n  # list out all possible edges\n  # (basically same code as setting up the maze)\n  # plus adds edges that stick out on the outside\n  all_possible_edges <- CJ(\n    x1 = rep(seq(1, 6), 2) - 1,\n    y1 = seq(1, 6) - 1\n  )\n  all_possible_edges[, \":=\"(x2 = ifelse(.I %% 2 == 0, x1 + 1, x1),\n    y2 = ifelse(.I %% 2 == 1, y1 + 1, y1))]\n  all_possible_edges <- all_possible_edges[(x1 != 0 | x2 != 0) &\n    (y1 != 0 | y2 != 0), ]\n\n  edges <- edges[, .(x1, x2, y1, y2, id)]\n\n  # merge maze and all possible edges to see which ones weren't used\n  all_possible_edges <- merge(all_possible_edges, edges,\n    by = c(\"x1\", \"y1\", \"x2\", \"y2\"),\n    all.x = TRUE\n  )\n\n  # This function subs in the new edges appropriately\n  # basically, any path edge needs to be updated to two edges so the maze\n  # starts at the bottom middle, travels through the maze, and back to the start\n  create_new_edges <- function(x1, y1, x2, y2, id) {\n    # if no edges, add block\n    if (is.na(id)) {\n      if (y1 == y2) { # horizontal edge\n        list(\n          x1_1 = 2 * x1,\n          y1_1 = 2 * y1 - 1,\n          x2_1 = 2 * x1,\n          y2_1 = 2 * y1,\n          x1_2 = 2 * x2 - 1,\n          y1_2 = 2 * y2 - 1,\n          x2_2 = 2 * x2 - 1,\n          y2_2 = 2 * y2\n        )\n      } else { # vertical edge\n        list(\n          x1_1 = 2 * x1 - 1,\n          y1_1 = 2 * y1,\n          x2_1 = 2 * x1,\n          y2_1 = 2 * y1,\n          x1_2 = 2 * x2 - 1,\n          y1_2 = 2 * y2 - 1,\n          x2_2 = 2 * x2,\n          y2_2 = 2 * y2 - 1\n        )\n      }\n    } else { # has edge, add connections\n      if (y1 == y2) { # horizontal edge\n        list(\n          x1_1 = 2 * x1,\n          y1_1 = 2 * y1 - 1,\n          x2_1 = 2 * x2 - 1,\n          y2_1 = 2 * y2 - 1,\n          x1_2 = 2 * x1,\n          y1_2 = 2 * y1,\n          x2_2 = 2 * x2 - 1,\n          y2_2 = 2 * y2\n        )\n      } else { # vertical edge\n        list(\n          x1_1 = 2 * x1 - 1,\n          y1_1 = 2 * y1,\n          x2_1 = 2 * x2 - 1,\n          y2_1 = 2 * y2 - 1,\n          x1_2 = 2 * x1,\n          y1_2 = 2 * y1,\n          x2_2 = 2 * x2,\n          y2_2 = 2 * y2 - 1\n        )\n      }\n    }\n  }\n\n  # fill in blocks and paths\n  all_possible_edges[, c(\n    \"x1_1\", \"y1_1\", \"x2_1\", \"y2_1\",\n    \"x1_2\", \"y1_2\", \"x2_2\", \"y2_2\"\n  ) := create_new_edges(x1, y1, x2, y2, id),\n  by = seq_len(nrow(all_possible_edges))\n  ]\n\n  # clean everything up\n  all_possible_edges[, \":=\"(x1 = NULL,\n    y1 = NULL,\n    x2 = NULL,\n    y2 = NULL,\n    id = NULL)]\n\n  all_possible_edges <- melt(all_possible_edges,\n    measure.vars = patterns(\"x1\", \"y1\", \"x2\", \"y2\"),\n    value.name = c(\"x1\", \"y1\", \"x2\", \"y2\")\n  )[, variable := NULL]\n\n  all_possible_edges <- all_possible_edges[(x1 > 0 &\n    y1 > 0 &\n    x2 < 11 &\n    y2 < 11) &\n    (x1 != 5 |\n      y1 != 1 |\n      x2 != 6 |\n      y2 != 1), ]\n\n  all_possible_edges <- rbind(\n    all_possible_edges,\n    data.table(\n      x1 = c(5, 6),\n      y1 = c(0, 0),\n      x2 = c(5, 6),\n      y2 = c(1, 1)\n    )\n  )\n}\n\n\nThe following two images show a randomly generated maze and output for doubling the edges. The image on the left has the maze in red with all possible edges in blue. Note the extra blue edges are pointing out of the original five-by-five square. These will provide the walls for paths on the maze’s outside edge. The image on the right displays the result of substituting every edge with a parallel set for maze edges and a perpendicular set for non-maze edges. So, each red line has two new black lines running next to it, while each blue line has two new black lines cutting through it. Note that the coordinates’ ranges have changed from one to five to zero to eleven (everything is times two then minus one).\n\n\n\n\n\n\nRandomly generated maze\n\n\n\n\n\n\n\nDoubling maze edges\n\n\n\n\n\nThis image shows the cleaned-up final output for this function. The unconnected outside edges are removed. The start and end edges are added to the bottom middle with the connection between them severed.\n\n\n\nCleaned-up output\n\n\n\n\n\nMove through all edges in one path\n\n\n\nNow, the edges from the previous step can be connected into one path. The path starts at the bottom middle and then moves to the next node. After that, the path connects to the unconnected node. Because of the previous setup, each node (except starting and end ones) connects to exactly two other nodes. So, we don’t have to worry about hitting dead ends.\n\n\n[maze_to_path function]\nmaze_to_path <- function(edges) {\n  # set up id\n  edges[, id := .I]\n  setkey(edges, id)\n\n  # set up nodes data set\n  nodes <- unique(rbind(edges[, .(x = x1, y = y1)]\n                        , edges[, .(x = x2, y = y2)]))\n  nodes[, id := .I]\n  setkey(nodes, id)\n\n  # add node ids to edges data set\n  edges <- merge(edges, nodes,\n    by.x = c(\"x1\", \"y1\"), by.y = c(\"x\", \"y\"),\n    suffixes = c(\"\", \"_node_1\"), all.x = TRUE\n  )\n  edges <- merge(edges, nodes,\n    by.x = c(\"x2\", \"y2\"), by.y = c(\"x\", \"y\"),\n    suffixes = c(\"\", \"_node_2\"), all.x = TRUE\n  )\n\n  # nodes to edges look up table\n  nodes_edges <- unique(rbind(\n    edges[, .(id = id_node_1, edge = id, connecting_node = id_node_2)],\n    edges[, .(id = id_node_2, edge = id, connecting_node = id_node_1)]\n  ))\n  setkey(nodes_edges, id)\n\n  # save spot for path\n  path <- vector(mode = \"numeric\")\n\n  # variables to keep track of progress through the mase\n  last_node <- nodes[y == 0 & x == 5, id]\n  current_node <- nodes[y == 0 & x == 6, id]\n\n  # update path\n  path <- append(path, current_node)\n  previous_node <- current_node\n\n  # keep going to unexplored nodes\n  current_node <- nodes_edges[.(current_node), \n                              ][connecting_node != previous_node\n                                , connecting_node]\n\n  # continue through the whole path\n  while (length(current_node) > 0) {\n    path <- append(path, current_node)\n    future_node <- nodes_edges[.(current_node), \n                               ][connecting_node != previous_node\n                                 , connecting_node]\n    previous_node <- current_node\n    current_node <- future_node\n  }\n\n  path <- data.table(\n    order = seq(1, length(path)),\n    node = path\n  )\n  path <- merge(path, nodes, by.x = c(\"node\"), by.y = c(\"id\"))\n}\n\n\n\n\n\nAdd in the images for signs\n\n\n\nCreating the signs takes place before creating the maze. Each image name has three parts: sign, course, and direction. The sign contains essential information on what the image is. The course is how the path moves (straight, turn right, etc.). Direction is where the path is coming from (east, north, etc.).\n\n\n[first part of create_signs code]\nlibrary(data.table)\nlibrary(ggplot2)\n\n# do not enter\nggplot() +\n  geom_polygon(aes(\n    x = cos(seq(0, 2 * pi, pi / 4) + pi / 8) * 1.082,\n    y = sin(seq(0, 2 * pi, pi / 4) + pi / 8) * 1.082\n  ), color = \"#90091E\", fill = \"#90091E\") +\n  geom_text(aes(x = 0, y = 0, label = \"DO NOT\\nENTER\"), \n            color = \"white\", size = 3) +\n  scale_x_continuous(limits = c(-1, 1)) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  theme_void() +\n  coord_equal()\nggsave(paste0(\"signs/do_not_enter_straight_south.png\"),\n  height = 1,\n  width = 1\n)\n\n\nYou can see the rest of the code here.\n\n\n[second part of create_signs code]\ndirections <- data.table(\n  direction = c(\n    \"east\",\n    \"north\",\n    \"west\",\n    \"south\"\n  ),\n  angle = c(270, 0, 90, 180)\n)\n\ncircle <- data.table(\n  x = cos(seq(0, 2 * pi, length.out = 360)),\n  y = sin(seq(0, 2 * pi, length.out = 360))\n)\n\n# square\nsquare <- data.table(\n  x = c(-1, 1, 1, -1),\n  y = c(-1, -1, 1, 1)\n)\n\n# diamond\ndiamond <- data.table(\n  x = c(0, 1, 0, -1),\n  y = c(-1, 0, 1, 0)\n)\n\nfor (i in 1:nrow(directions)) {\n  current_direction <- directions[i, direction]\n  current_angle <- directions[i, angle]\n\n  # light green dot\n  ggplot() +\n    geom_polygon(data = circle, aes(x = x * .5, y = y * .5), \n                 color = \"#6FBD4B\", fill = \"#6FBD4B\") +\n    scale_x_continuous(limits = c(-1, 1)) +\n    scale_y_continuous(limits = c(-1, 1)) +\n    theme_void() +\n    coord_equal()\n  ggsave(paste0(\"signs/light_green_dot_straight_\", \n                current_direction, \".png\"),\n    height = 1,\n    width = 1\n  )\n\n  # dark green dot\n  ggplot() +\n    geom_polygon(data = circle, aes(x = x * .75, y = y * .75), \n                 color = \"#235C09\", fill = \"#235C09\") +\n    scale_x_continuous(limits = c(-1, 1)) +\n    scale_y_continuous(limits = c(-1, 1)) +\n    theme_void() +\n    coord_equal()\n  ggsave(paste0(\"signs/dark_green_dot_straight_\", \n                current_direction, \".png\"),\n    height = 1,\n    width = 1\n  )\n\n  # yellow tape\n  if (current_direction %in% c(\"north\", \"south\")) {\n    ggplot() +\n      geom_rect(aes(\n        xmin = -.5, ymin = -.075,\n        xmax = .5, ymax = .075\n      ),\n      color = \"#EDE24C\",\n      fill = \"#EDE24C\"\n      ) +\n      scale_x_continuous(limits = c(-1, 1)) +\n      scale_y_continuous(limits = c(-1, 1)) +\n      theme_void() +\n      coord_equal()\n    ggsave(paste0(\"signs/yellow_tape_straight_\", \n                  current_direction, \".png\"),\n      height = 1,\n      width = 1\n    )\n  } else {\n    ggplot() +\n      geom_rect(aes(\n        xmin = -.075, ymin = -.5,\n        xmax = .075, ymax = .5\n      ),\n      color = \"#EDE24C\",\n      fill = \"#EDE24C\"\n      ) +\n      scale_x_continuous(limits = c(-1, 1)) +\n      scale_y_continuous(limits = c(-1, 1)) +\n      theme_void() +\n      coord_equal()\n    ggsave(paste0(\"signs/yellow_tape_straight_\", \n                  current_direction, \".png\"),\n      height = 1,\n      width = 1\n    )\n  }\n\n  # wait here\n  ggplot() +\n    geom_polygon(data = square, aes(x = x, y = y), \n                 color = \"#4D8235\", fill = \"#4D8235\") +\n    geom_text(aes(x = 0, y = 0, label = \"WAIT\\nHERE\"),\n      color = \"white\", size = 6,\n      angle = current_angle\n    ) +\n    scale_x_continuous(limits = c(-1, 1)) +\n    scale_y_continuous(limits = c(-1, 1)) +\n    theme_void() +\n    coord_equal()\n  ggsave(paste0(\"signs/wait_here_straight_\", \n                current_direction, \".png\"),\n    height = 1,\n    width = 1\n  )\n\n  for (turn in c(\"right\", \"left\")) {\n    ggplot() +\n      geom_polygon(data = square, aes(x = x, y = y), \n                   color = \"#4D8235\", fill = \"#4D8235\") +\n      geom_text(aes(x = 0, y = 0, label = \"WAIT\\nHERE\"),\n        color = \"white\", size = 6,\n        angle = current_angle\n      ) +\n      scale_x_continuous(limits = c(-1, 1)) +\n      scale_y_continuous(limits = c(-1, 1)) +\n      theme_void() +\n      coord_equal()\n    ggsave(paste0(\"signs/wait_here_\", turn, \"_\", \n                  current_direction, \".png\"),\n      height = 1,\n      width = 1\n    )\n  }\n\n  # wash your hands\n  ggplot() +\n    geom_polygon(data = circle, aes(x = x, y = y), \n                 color = \"#4D4D7A\", fill = \"#4D4D7A\") +\n    geom_text(aes(x = 0, y = 0, label = \"WASH YOUR\\nHANDS\"),\n      color = \"white\", size = 3,\n      angle = current_angle\n    ) +\n    scale_x_continuous(limits = c(-1, 1)) +\n    scale_y_continuous(limits = c(-1, 1)) +\n    theme_void() +\n    coord_equal()\n  ggsave(paste0(\"signs/wash_your_hands_straight_\", \n                current_direction, \".png\"),\n    height = 1,\n    width = 1\n  )\n\n  for (turn in c(\"right\", \"left\")) {\n    ggplot() +\n      geom_polygon(data = circle, aes(x = x, y = y), \n                   color = \"#4D4D7A\", fill = \"#4D4D7A\") +\n      geom_text(aes(x = 0, y = 0, label = \"WASH YOUR\\nHANDS\"),\n        color = \"white\", size = 3,\n        angle = current_angle\n      ) +\n      scale_x_continuous(limits = c(-1, 1)) +\n      scale_y_continuous(limits = c(-1, 1)) +\n      theme_void() +\n      coord_equal()\n    ggsave(paste0(\"signs/wash_your_hands_\", turn, \"_\", \n                  current_direction, \".png\"),\n      height = 1,\n      width = 1\n    )\n  }\n\n  # wear a mask\n  ggplot() +\n    geom_polygon(data = circle, aes(x = x, y = y), \n                 color = \"#538479\", fill = \"#538479\") +\n    geom_text(aes(x = 0, y = 0, label = \"WEAR A\\nMASK\"),\n      color = \"white\", size = 3,\n      angle = current_angle\n    ) +\n    scale_x_continuous(limits = c(-1, 1)) +\n    scale_y_continuous(limits = c(-1, 1)) +\n    theme_void() +\n    coord_equal()\n  ggsave(paste0(\"signs/wear_a_mask_straight_\", \n                current_direction, \".png\"),\n    height = 1,\n    width = 1\n  )\n\n  for (turn in c(\"right\", \"left\")) {\n    ggplot() +\n      geom_polygon(data = circle, aes(x = x, y = y), \n                   color = \"#538479\", fill = \"#538479\") +\n      geom_text(aes(x = 0, y = 0, label = \"WEAR A\\nMASK\"),\n        color = \"white\", size = 3,\n        angle = current_angle\n      ) +\n      scale_x_continuous(limits = c(-1, 1)) +\n      scale_y_continuous(limits = c(-1, 1)) +\n      theme_void() +\n      coord_equal()\n    ggsave(paste0(\"signs/wear_a_mask_\", turn, \"_\", \n                  current_direction, \".png\"),\n      height = 1,\n      width = 1\n    )\n  }\n\n  # wait six feet\n  ggplot() +\n    geom_polygon(data = circle, aes(x = x, y = y), \n                 color = \"#54707C\", fill = \"#54707C\") +\n    geom_text(aes(x = 0, y = 0, label = \"WAIT \\nSIX FEET\"),\n      color = \"white\", size = 3,\n      angle = current_angle\n    ) +\n    scale_x_continuous(limits = c(-1, 1)) +\n    scale_y_continuous(limits = c(-1, 1)) +\n    theme_void() +\n    coord_equal()\n  ggsave(paste0(\"signs/wait_six_feet_straight_\", \n                current_direction, \".png\"),\n    height = 1,\n    width = 1\n  )\n\n  for (turn in c(\"right\", \"left\")) {\n    ggplot() +\n      geom_polygon(data = circle, aes(x = x, y = y), \n                   color = \"#54707C\", fill = \"#54707C\") +\n      geom_text(aes(x = 0, y = 0, label = \"WAIT \\nSIX FEET\"),\n        color = \"white\", size = 3,\n        angle = current_angle\n      ) +\n      scale_x_continuous(limits = c(-1, 1)) +\n      scale_y_continuous(limits = c(-1, 1)) +\n      theme_void() +\n      coord_equal()\n    ggsave(paste0(\"signs/wait_six_feet_\", turn, \"_\", \n                  current_direction, \".png\"),\n      height = 1,\n      width = 1\n    )\n  }\n\n  # 6 feet\n  if (current_direction %in% c(\"north\", \"south\")) {\n    ggplot() +\n      geom_polygon(data = diamond, aes(x = x, y = y), \n                   color = \"#E1AD0F\", fill = \"#E1AD0F\") +\n      geom_text(aes(x = 0, y = 0, label = \"6 FEET\"),\n        color = \"black\", size = 3,\n        angle = current_angle\n      ) +\n      geom_segment(aes(\n        x = 0,\n        y = c(.25, -.25),\n        xend = 0,\n        yend = c(.8, -.8)\n      ),\n      lineend = \"butt\",\n      linejoin = \"mitre\",\n      color = \"black\",\n      arrow = arrow(length = unit(0.05, \"npc\"), \n                    angle = 45, type = \"closed\"),\n      size = 2\n      ) +\n      scale_x_continuous(limits = c(-1, 1)) +\n      scale_y_continuous(limits = c(-1, 1)) +\n      theme_void() +\n      coord_equal()\n    ggsave(paste0(\"signs/six_feet_straight_\", \n                  current_direction, \".png\"),\n      height = 1,\n      width = 1\n    )\n  } else {\n    ggplot() +\n      geom_polygon(data = diamond, aes(x = x, y = y), \n                   color = \"#E1AD0F\", fill = \"#E1AD0F\") +\n      geom_text(aes(x = 0, y = 0, label = \"6 FEET\"),\n        color = \"black\", size = 3,\n        angle = current_angle\n      ) +\n      geom_segment(aes(\n        x = c(.25, -.25),\n        y = 0,\n        xend = c(.8, -.8),\n        yend = 0\n      ),\n      lineend = \"butt\",\n      linejoin = \"mitre\",\n      color = \"black\",\n      arrow = arrow(length = unit(0.05, \"npc\"), \n                    angle = 45, type = \"closed\"),\n      size = 2\n      ) +\n      scale_x_continuous(limits = c(-1, 1)) +\n      scale_y_continuous(limits = c(-1, 1)) +\n      theme_void() +\n      coord_equal()\n    ggsave(paste0(\"signs/six_feet_straight_\", \n                  current_direction, \".png\"),\n      height = 1,\n      width = 1\n    )\n  }\n\n  # one way\n  ggplot() +\n    geom_polygon(data = square, aes(x = x, y = y), \n                 color = \"#AB4343\", fill = \"#AB4343\") +\n    geom_text(aes(x = 0, y = 0, label = \"ONE\\nWAY\"),\n      color = \"white\", size = 6,\n      angle = current_angle\n    ) +\n    scale_x_continuous(limits = c(-1, 1)) +\n    scale_y_continuous(limits = c(-1, 1)) +\n    theme_void() +\n    coord_equal()\n  ggsave(paste0(\"signs/one_way_straight_\", \n                current_direction, \".png\"),\n    height = 1,\n    width = 1\n  )\n\n  for (turn in c(\"right\", \"left\")) {\n    ggplot() +\n      geom_polygon(data = square, aes(x = x, y = y), \n                   color = \"#AB4343\", fill = \"#AB4343\") +\n      geom_text(aes(x = 0, y = 0, label = \"ONE\\nWAY\"),\n        color = \"white\", size = 6,\n        angle = current_angle\n      ) +\n      scale_x_continuous(limits = c(-1, 1)) +\n      scale_y_continuous(limits = c(-1, 1)) +\n      theme_void() +\n      coord_equal()\n    ggsave(paste0(\"signs/one_way_\", turn, \"_\", \n                  current_direction, \".png\"),\n      height = 1,\n      width = 1\n    )\n  }\n}\n\n# arrow\nggplot() +\n  geom_polygon(aes(\n    x = c(-.25, .25, .25, .5, 0, -.5, -.25),\n    y = c(-.9, -.9, .25, .25, .9, .25, .25)\n  ), color = \"#444444\", fill = \"#444444\") +\n  scale_x_continuous(limits = c(-1, 1)) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  theme_void() +\n  coord_equal()\nggsave(paste0(\"signs/arrow_straight_north.png\"),\n  height = 1,\n  width = 1\n)\nggsave(paste0(\"signs/arrow_right_north.png\"),\n  height = 1,\n  width = 1\n)\nggsave(paste0(\"signs/arrow_left_north.png\"),\n  height = 1,\n  width = 1\n)\n\nggplot() +\n  geom_polygon(aes(\n    x = c(-.25, .25, .25, .5, 0, -.5, -.25),\n    y = c(.9, .9, -.25, -.25, -.9, -.25, -.25)\n  ), color = \"#444444\", fill = \"#444444\") +\n  scale_x_continuous(limits = c(-1, 1)) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  theme_void() +\n  coord_equal()\nggsave(paste0(\"signs/arrow_straight_south.png\"),\n  height = 1,\n  width = 1\n)\nggsave(paste0(\"signs/arrow_right_south.png\"),\n  height = 1,\n  width = 1\n)\nggsave(paste0(\"signs/arrow_left_south.png\"),\n  height = 1,\n  width = 1\n)\n\nggplot() +\n  geom_polygon(aes(\n    x = c(-.9, -.9, .25, .25, .9, .25, .25),\n    y = c(-.25, .25, .25, .5, 0, -.5, -.25)\n  ), color = \"#444444\", fill = \"#444444\") +\n  scale_x_continuous(limits = c(-1, 1)) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  theme_void() +\n  coord_equal()\nggsave(paste0(\"signs/arrow_straight_east.png\"),\n  height = 1,\n  width = 1\n)\nggsave(paste0(\"signs/arrow_right_east.png\"),\n  height = 1,\n  width = 1\n)\nggsave(paste0(\"signs/arrow_left_east.png\"),\n  height = 1,\n  width = 1\n)\n\nggplot() +\n  geom_polygon(aes(\n    x = c(.9, .9, -.25, -.25, -.9, -.25, -.25),\n    y = c(-.25, .25, .25, .5, 0, -.5, -.25)\n  ),\n  color = \"#444444\", fill = \"#444444\"\n  ) +\n  scale_x_continuous(limits = c(-1, 1)) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  theme_void() +\n  coord_equal()\nggsave(paste0(\"signs/arrow_straight_west.png\"),\n  height = 1,\n  width = 1\n)\nggsave(paste0(\"signs/arrow_right_west.png\"),\n  height = 1,\n  width = 1\n)\nggsave(paste0(\"signs/arrow_left_west.png\"),\n  height = 1,\n  width = 1\n)\n\n\nWe’ll need to determine how the path moves and which images display that to add the images. Using information from the previous and next steps, we can determine which direction the path moves and whether it turns or stays straight. This data can be paired with information from the image names to choose images that fit in the maze.\nFor each node on the path, several images could work. So, we select one at random from a bag. A parameter determines the number of available images in the bag. A value of 0 has all possible images on each draw. A value of 1 only yields one image for each course (straight vs. turn).\n\n\n[get_image_data function]\nget_image_data <- function(path, structure_parameter) {\n\n  # Set up variables used to determine which images can be used\n  path <- path[order(order), ]\n  path[, \":=\"(previous_x = shift(x, type = \"lag\"),\n    previous_y = shift(y, type = \"lag\"),\n    next_x = shift(x, type = \"lead\"),\n    next_y = shift(y, type = \"lead\"))]\n\n  path[, \":=\"(course = fifelse(\n    previous_x == next_x | previous_y == next_y, \"straight\",\n    fifelse((y > previous_y & next_x > x) |\n      (x > previous_x & next_y < y) |\n      (y < previous_y & next_x < x) |\n      (x < previous_x & next_y > y), \"right\", \"left\")\n  ),\n  direction = fifelse(\n    x < next_x, \"east\",\n    fifelse(\n      y < next_y, \"north\",\n      fifelse(\n        x > next_x, \"west\",\n        \"south\"\n      )\n    )\n  ))]\n\n  # set up bag to hold the images\n  bag_pull <- function(this_course, this_direction, bag) {\n    bag[course == this_course & direction == this_direction, \n        ][sample(.N, 1), file]\n  }\n\n  # fill bag\n  bag <- data.table(file = list.files(\"signs\", full.names = TRUE))\n  bag <- bag[file != \"signs/do_not_enter_straight_south.png\", ]\n\n  bag[, c(\"sign\", \"direction\") := \n        tstrsplit(gsub(\"signs/|.png\", \"\", file), \"_(?!.*_)\", perl = TRUE)]\n  bag[, c(\"sign\", \"course\") := tstrsplit(sign, \"_(?!.*_)\", perl = TRUE)]\n  bag[, \":=\"(sub_course = fifelse(course == \"straight\", \"straight\", \"turn\"))]\n\n  # filter down to a smaller amount if the structure_parameter is large\n  bag_subset <- unique(bag[, c(\"sign\", \"sub_course\")])\n  bag_subset <- unique(bag_subset[\n    , .SD[sample(.N, ceiling(\n      (1 - (structure_parameter)^(1 / 4)) * (.N - 1) + 1))]\n    , by = sub_course][, c(\"sign\", \"sub_course\")])\n  bag <- merge(bag, bag_subset, by = c(\"sign\", \"sub_course\"))\n\n  # add images from bag\n  path[, image := bag_pull(course, direction, bag), by = seq_len(nrow(path))]\n\n  # Start and end\n  path[y == 0 & x == 6, image := \"signs/wait_here_straight_north.png\"]\n  path[y == 0 & x == 5, image := \"signs/do_not_enter_straight_south.png\"]\n}\n\n\n\n\n\nFinally, add the background and save\n\n\n\nAfter setting up the appropriate images, we only need to add a background. The background works by stacking small gray rectangles of various sizes with a low alpha value. This is supposed to resemble a grocery store floor.\n\n\n[final code]\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(ggimage)\n\nsource(\"get_maze.R\")\nsource(\"update_maze.R\")\nsource(\"maze_to_path.R\")\nsource(\"get_image_data.R\")\n\nset.seed(1)\nfor (structure_parameter in seq(0, 1, by = .25)) {\n  for (num in 1:2) {\n    edges <- get_maze(structure_parameter)\n    edges <- update_maze(edges)\n    path <- maze_to_path(edges)\n    path <- get_image_data(path, structure_parameter)\n\n    # set up background floor\n    floor <- CJ(\n      x = seq(.5, 10.5, length.out = 360),\n      y = seq(-.5, 10.5, length.out = 360)\n    )\n    floor[, color := sample(c(seq(5, 9), c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\")), 1),\n      by = seq_len(nrow(floor))\n    ]\n    floor[, color := paste0(\"#\", color, color, color, color, color, color)]\n    floor[, \":=\"(xmin = x - runif(1, 0, .25),\n      ymin = y - runif(1, 0, .25),\n      xmax = x + runif(1, 0, .25),\n      ymax = y + runif(1, 0, .25)),\n    by = seq_len(nrow(floor))\n    ]\n\n    ggplot() +\n      geom_rect(\n        data = floor,\n        aes(\n          xmin = xmin, ymin = ymin,\n          xmax = xmax, ymax = ymax,\n          fill = color\n        ),\n        color = NA,\n        alpha = .01\n      ) +\n      scale_fill_identity() +\n      geom_image(\n        data = path,\n        aes(x, y, image = image)\n      ) +\n      theme_void() +\n      theme(aspect.ratio = 1)\n    ggsave(paste0(\"output/image_\", structure_parameter * 100, \n                  \"_\", num, \".jpeg\"),\n      width = 8,\n      height = 8,\n      bg = \"#F3F3F3\"\n    )\n  }\n}\n\n\nNow we can see the final outputs.\n\n\n\n\n\n\nStructure Parameter = 0, Run 1\n\n\n\n\n\n\n\nStructure Parameter = 0, Run 2\n\n\n\n\n\n\n\n\n\nStructure Parameter = .25, Run 1\n\n\n\n\n\n\n\nStructure Parameter = .25, Run 2\n\n\n\n\n\n\n\n\n\nStructure Parameter = .5, Run 1\n\n\n\n\n\n\n\nStructure Parameter = .5, Run 2\n\n\n\n\n\n\n\n\n\nStructure Parameter = .75, Run 1\n\n\n\n\n\n\n\nStructure Parameter = .75, Run 2\n\n\n\n\n\n\n\n\n\nStructure Parameter = 1, Run 1\n\n\n\n\n\n\n\nStructure Parameter = 1, Run 2"
  },
  {
    "objectID": "posts/2022-03-01-cohesive-graphs/index.html",
    "href": "posts/2022-03-01-cohesive-graphs/index.html",
    "title": "Cohesive Graphs",
    "section": "",
    "text": "I think this subject isn’t covered in many visualization courses because they tend to focus on one chart at a time. You can see this in a lot of infographics where they only feature one large detailed graphic. These are helpful for many settings but less useful in business scenarios. Large singular images can help you answer detailed questions on one subject. For example, a line chart of revenue by day can show the change from May 5th to June 7th or which Monday had the highest amount. So you end up with one big graph that answers a lot of small questions.\nHowever, you probably need a lot of small charts that answer one big vague question, like “how’s this initiative going?” or “what’s the effect of a changing population?”. These questions don’t need detailed information on one metric but smoother information on several. When you have multiple metrics, you’ll end up with many graphs, and they’ll need to work together to answer the big question.\nThis post won’t cover how much detail to remove since that’s dependent on the situation (sometimes you do need daily data), but it will focus on smoothing out differences in a set of charts. I’ll cover colors, order, and text. The process is pretty similar to branding, and following advice on that (here and here) can help.\nThe graphs will appear side-by-side in the blog post but not in your actual document. So, there will be some redundancy (like having the legend in each one) on display. For each chart, imagine it’s multiple pages or clicks away from the other ones.\n\n\n\nColor\n\n\n\nPeople like your colors to match across all your graphs. The colors could be for discrete groups (like purple and green for different subpopulations), continuous scales (blue for low and yellow for high amounts), or emphasis (red for highlighting concern). We’ll compare inconsistent graphs to those that match in the following examples.\nFor discrete color matches across charts, make sure to use the same color for the same entity. In this example, classes A and B’s colors should be the same for all charts.\n\n\n\n\n\n\nAdding new levels will often break the matching colors. Here Class B changes drastically.\n\n\n\n\n\n\nFor continuous color (and other scales), ensure the limits match across charts. If the charts need to be comparable, the color limits (and other limits, like the x-axis range) should match.\n\n\n\n\n\n\nThis one is more of a judgment call than for discrete color. Sometimes you’ll want to focus on some detail, and having the broader limits won’t show that. So having different limits might be the better call as long as end-users can tell the differences quickly.\n\n\n\nOrder\n\n\n\nEven if color is cohesive or unused, the order also needs to stick.\n\n\n\n\n\n\nThere are options for determining the order that might change across graphs. As long as it’s evident that the change occurred because of the ordering process and that the process is more important than consistency, it should be fine.\n\nYou can set the main entity first if it’s the most important or serves as a baseline.\n\n\n\n\nText\n\n\n\nFinally, ensuring consistency of the text across graphs smooths out transitions and easily builds up a big picture. There are fewer minor differences to compare, so important ones stand out.\nIn the following example, titles contain synonyms for “Customers” and “by Quarter.” These might be incorrect if members, accounts, customers, and users are different, especially for various end-users. In addition, the disparity for “by Quarter” can add confusion if end-users believe there is a difference when there isn’t one. Having consistency in the graphs’ text reduces the cognitive load and makes focusing on what the charts are showing easy.\n\n\n\n\n\n\n\n\n\n\n\n\n[Code]\nlibrary(tidyverse)\nlibrary(patchwork)\n\n## Colors\n\n# Color schemes\ndf <- data.frame(Class = c(\"A\", \"B\"),\n                 Value = c(56, 87))\n\nplot_1 <- ggplot(data = df,\n                 aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"Bad Plot 1\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\ndf_next <- data.frame(Class = c(\"A\", \"B\"),\n                      Value = c(36, 78))\n\nplot_2 <- ggplot(data = df_next,\n                 aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  ylab(\"Other Value\") +\n  labs(title = \"Bad Plot 2\",\n       subtitle = \"The colors are inconsistent.\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_1 + plot_spacer() + plot_2 + plot_layout(widths = c(4, 1, 4))\n\nggsave(\"plot_1.png\",\n       width = 5,\n       height = 2.5)\n\ndf <- data.frame(Class = c(\"A\", \"B\"),\n                 Value = c(56, 87))\n\nplot_1 <- ggplot(data = df,\n                 aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"Good Plot 1\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\ndf_next <- data.frame(Class = c(\"A\", \"B\"),\n                      Value = c(36, 78))\n\nplot_2 <- ggplot(data = df_next,\n                 aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  ylab(\"Other Value\") +\n  labs(title = \"Good Plot 2\",\n       subtitle = \"The colors match across charts.\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_1 + plot_spacer() + plot_2 + plot_layout(widths = c(4, 1, 4))\n\nggsave(\"plot_2.png\",\n       width = 5,\n       height = 2.5)\n\n# Adding new level\ndf <- data.frame(Class = c(\"A\", \"B\"),\n                 Value = c(56, 87))\n\nplot_1 <- ggplot(data = df,\n       aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Bad Plot 1\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\ndf_next <- data.frame(Class = c(\"A\", \"B\", \"C\", \"D\"),\n                 Value = c(36, 78, 65, 45))\n\nplot_2 <- ggplot(data = df_next,\n       aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  ylab(\"Other Value\") +\n  labs(title = \"Bad Plot 2\",\n       subtitle = \"The colors change when adding classes.\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_1 + plot_spacer() + plot_2 + plot_layout(widths = c(4, 1, 4))\n\nggsave(\"plot_3.png\",\n       width = 5,\n       height = 2.5)\n\ndf <- data.frame(Class = c(\"A\", \"B\"),\n                 Value = c(56, 87))\n\nplot_1 <- ggplot(data = df,\n                 aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"Good Plot 1\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\ndf_next <- data.frame(Class = c(\"A\", \"B\", \"C\", \"D\"),\n                      Value = c(36, 78, 65, 45))\n\nplot_2 <- ggplot(data = df_next,\n                 aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  ylab(\"Other Value\") +\n  labs(title = \"Good Plot 2\",\n       subtitle = \"The colors stay constant.\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_1 + plot_spacer() + plot_2 + plot_layout(widths = c(4, 1, 4))\n\nggsave(\"plot_4.png\",\n       width = 5,\n       height = 2.5)\n\n# increasing/decreasing scales across graphs\nset.seed(1)\ndf <- data.frame(x = runif(25, 0, 1)) %>%\n  mutate(y = x^2 + runif(25, 0, .01),\n         color = x^2 + runif(25, 0, .01))\n\nplot_1 <- ggplot(data = df,\n       aes(x, y, color = color)) +\n  geom_point() +\n  labs(title = \"Bad Plot 1\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\ndf_next <- data.frame(x = runif(5, 1, 1.5)) %>%\n  mutate(y = x^2 + runif(5, 0, .01),\n         color = x^2 + runif(5, 0, .01)) %>%\n  rbind(df)\n\nplot_2 <- ggplot(data = df_next,\n                 aes(x, y, color = color)) +\n  geom_point() +\n  labs(title = \"Bad Plot 2\",\n       subtitle = \"The scales change across charts.\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_1 + plot_spacer() + plot_2 + plot_layout(widths = c(4, 1, 4))\n\nggsave(\"plot_5.png\",\n       width = 5,\n       height = 2.5)\n\ndf <- data.frame(x = runif(25, 0, 1)) %>%\n  mutate(y = x^2 + runif(25, 0, .01),\n         color = x^2 + runif(25, 0, .01))\n\nplot_1 <- ggplot(data = df,\n                 aes(x, y, color = color)) +\n  geom_point() + \n  scale_x_continuous(limits = c(0, 1.5)) +\n  scale_y_continuous(limits = c(0, 2)) +\n  scale_color_continuous(limits = c(0, 2)) +\n  labs(title = \"Good Plot 1\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\ndf_next <- data.frame(x = runif(5, 1, 1.5)) %>%\n  mutate(y = x^2 + runif(5, 0, .01),\n         color = x^2 + runif(5, 0, .01)) %>%\n  rbind(df)\n\nplot_2 <- ggplot(data = df_next,\n                 aes(x, y, color = color)) +\n  geom_point() + \n  scale_x_continuous(limits = c(0, 1.5)) +\n  scale_y_continuous(limits = c(0, 2)) +\n  scale_color_continuous(limits = c(0, 2)) +\n  labs(title = \"Good Plot 2\",\n       subtitle = \"The scales match between charts.\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_1 + plot_spacer() + plot_2 + plot_layout(widths = c(4, 1, 4))\n\nggsave(\"plot_6.png\",\n       width = 5,\n       height = 2.5)\n\n## Category Order\n# same order across graphs\ndf <- data.frame(Class = c(\"A\", \"B\"),\n                 Value = c(56, 87))\n\nplot_1 <- ggplot(data = df,\n                 aes(Class, Value)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Bad Plot 1\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\ndf_next <- data.frame(Class = c(\"A\", \"B\"),\n                      Value = c(36, 78)) %>%\n  mutate(Class = factor(Class, levels = c(\"B\", \"A\")))\n\nplot_2 <- ggplot(data = df_next,\n                 aes(Class, Value)) +\n  geom_bar(stat = \"identity\") +\n  ylab(\"Other Value\") +\n  labs(title = \"Bad Plot 2\",\n       subtitle = \"The class ordering changes.\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_1 + plot_spacer() + plot_2 + plot_layout(widths = c(4, 1, 4))\n\nggsave(\"plot_7.png\",\n       width = 5,\n       height = 2.5)\n\ndf <- data.frame(Class = c(\"A\", \"B\"),\n                 Value = c(56, 87))\n\nplot_1 <- ggplot(data = df,\n                 aes(Class, Value)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Good Plot 1\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\ndf_next <- data.frame(Class = c(\"A\", \"B\"),\n                      Value = c(36, 78))\n\nplot_2 <- ggplot(data = df_next,\n                 aes(Class, Value)) +\n  geom_bar(stat = \"identity\") +\n  ylab(\"Other Value\") +\n  labs(title = \"Good Plot 2\",\n       subtitle = \"The ordering is constant\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_1 + plot_spacer() + plot_2 + plot_layout(widths = c(4, 1, 4))\n\nggsave(\"plot_8.png\",\n       width = 5,\n       height = 2.5)\n\n# Numeric (Values) vs Alphabet vs Importance\ndf <- data.frame(Class = c(\"A\", \"B\", \"C\", \"D\"),\n                      Value = c(36, 78, 65, 45))\n\nplot_1 <- ggplot(data = df,\n                 aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"Alphabetical\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\ndf <- df %>%\n  mutate(Class = fct_reorder(Class, Value, .desc = TRUE))\n\nplot_2 <- ggplot(data = df,\n                 aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"Numerical\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_1 + plot_spacer() + plot_2 + plot_layout(widths = c(4, 1, 4))\n\nggsave(\"plot_9.png\",\n       width = 5,\n       height = 2.5)\n\ndf <- df %>%\n  mutate(Class = factor(Class, c(\"A\", \"B\", \"C\", \"D\"))) %>%\n  mutate(Class = relevel(Class, \"C\"))\n\nplot_1 <- ggplot(data = df,\n                 aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"C is on the left\",\n       subtitle = \"Then alphabetical\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\ndf <- df %>%\n  mutate(Class = fct_reorder(Class, Value, .desc = TRUE)) %>%\n  mutate(Class = relevel(Class, \"C\"))\n\nplot_2 <- ggplot(data = df,\n                 aes(Class, Value, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"C is on the left\",\n       subtitle = \"Then numerical\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_1 + plot_spacer() + plot_2 + plot_layout(widths = c(4, 1, 4))\n\nggsave(\"plot_10.png\",\n       width = 5,\n       height = 2.5)\n\n## Text\ndf <- data.frame(Quarter = c(1, 2, 3, 4),\n                 New = c(2000, 1000, 1500, 3000),\n                 Left = c(1000, 1500, 500, 1250),\n                 Stayed = c(50000, 45000, 57000, 54000)) %>%\n  mutate(Total = Stayed + New - Left)\n\nplot_1 <- ggplot(data = df,\n       aes(Quarter, Total)) +\n  geom_line() +\n  expand_limits(y = 0) +\n  labs(title = \"Members by Quarter\") +\n  ylab(\"Members\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_2 <- ggplot(data = df,\n                 aes(Quarter, New)) +\n  geom_line() +\n  expand_limits(y = 0) +\n  labs(title = \"New Accounts Over Time\") +\n  ylab(\"New Accounts\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_3 <- ggplot(data = df,\n                 aes(Quarter, Left)) +\n  geom_line() +\n  expand_limits(y = 0) +\n  labs(title = \"Customer Attrition Trend\") +\n  ylab(\"Customers\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_4 <- ggplot(data = df,\n                 aes(Quarter, Stayed)) +\n  geom_line() +\n  expand_limits(y = 0) +\n  labs(title = \"Retained Users\") +\n  ylab(\"Users\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\n(plot_1 + plot_2) / (plot_3 + plot_4) +\n  plot_annotation(title = \"Bad Plots\",\n                  subtitle = \"\")\n\nggsave(\"plot_11.png\",\n       width = 5,\n       height = 5)\n\nplot_1 <- ggplot(data = df,\n                 aes(Quarter, Total)) +\n  geom_line() +\n  expand_limits(y = 0) +\n  labs(title = \"Customers by Quarter\") +\n  ylab(\"Customers\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_2 <- ggplot(data = df,\n                 aes(Quarter, New)) +\n  geom_line() +\n  expand_limits(y = 0) +\n  labs(title = \"New Customers by Quarter\") +\n  ylab(\"New Customers\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_3 <- ggplot(data = df,\n                 aes(Quarter, Left)) +\n  geom_line() +\n  expand_limits(y = 0) +\n  labs(title = \"Customer Attrition by Quarter\") +\n  ylab(\"Customers Lost\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\nplot_4 <- ggplot(data = df,\n                 aes(Quarter, Stayed)) +\n  geom_line() +\n  expand_limits(y = 0) +\n  labs(title = \"Retained Customers by Quarter\") +\n  ylab(\"Customers Retained\") +\n  theme_linedraw() +\n  theme(text = element_text(size = 7))\n\n(plot_1 + plot_2) / (plot_3 + plot_4) +\n  plot_annotation(title = \"Good Plots\",\n                  subtitle = \"The plot text matches.\")\n\nggsave(\"plot_12.png\",\n       width = 5,\n       height = 5)"
  },
  {
    "objectID": "posts/2021-10-25-penrsose-tiling/index.html",
    "href": "posts/2021-10-25-penrsose-tiling/index.html",
    "title": "Penrose Tiling",
    "section": "",
    "text": "R package\n\n\n\nI recently worked on a generative art project around Penrose tiling patterns. As I worked through the functions, I decided to throw them into a package, penrosetiling. You can find it here. This package has base data sets and functions to substitute the pattern down through the shapes. There are also vignettes to help get started, use purrr for looping, and clean up images. Hopefully, it will work for you.\n\n\n\npenrosetiling hex sticker\n\n\n\n\n\nOutputs\n\n\n\nThe outputs from the project are displayed below. The code can be found here.\n\n\n\nVariations 1\n\n\n\n\n\nVariations 2\n\n\n\n\n\nCathedral\n\n\n\n\n\nWarnings\n\n\n\n\n\nVacay\n\n\n\n\n\nSBWN (Standard Black and White Noise)"
  },
  {
    "objectID": "posts/2020-04-16-website-design-part-1/index.html",
    "href": "posts/2020-04-16-website-design-part-1/index.html",
    "title": "Website Design: Part 1",
    "section": "",
    "text": "This blog post and the next one cover the code for making the site images. The first post pulls the data, and the second one creates the images. The setup first pulls the contents from the Data Science page of Wikipedia to get the letter frequencies. The code then pulls the overall letter frequencies for the English language. Finally, it saves them together to graph later.\nThis first section just lists out the R libraries. The here and ggplot2 libraries are standard for most of my projects. The main additional library is rvest, which pulls the content from web pages.\n\n#----------\n# Libraries\n#----------\n\n# Set up\nlibrary(here)\nlibrary(ggplot2)\n\n# Library for getting html\nlibrary(rvest)\nhelp(html_text)\n\nThe second section pulls the Data Science Wikipedia page, cleans up the result a little, and graphs the data to check it.\n\n#--------------------\n# 'Data Science' data\n#--------------------\n\n# Scrape the Data Science data\ndata_science <- read_html(\"https://en.wikipedia.org/wiki/Data_science\")\n\n# Pull out main title\nheading1 <- html_text(html_nodes(data_science, \"h1\"))\nheading1\n\n# Pull out body text\nbody <- html_text(html_nodes(data_science, \"p\"))\nhead(body)\n\n# Add main title\nbody[1] <- paste(heading1, body[1])\nbody[1]\n\n# Crush together\nbody <- paste(body, collapse = \" \")\n\n# barplot of counts\nds_letter <- data.frame(table(\n  strsplit(tolower(gsub(\"[^[:alpha:]]\", \"\", body)), \"\")))\nnames(ds_letter) <- c(\"Letter\", \"Frequency\")\nds_letter$Frequency <- ds_letter$Frequency / sum(ds_letter$Frequency) * 100\nds_letter\n\nggplot(data = ds_letter, aes(Letter, Frequency)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"Data Science Wikipedia Page\")\nggsave(filename = here::here(\"images\", \"ds_letter.png\"))\n\n\nThe next section is like the previous one. Instead of pulling the text from a Wikipedia page, the code pulls data out of a table for letter frequencies.\n\n#---------------\n# Overall data\n#---------------\n\n# Get overall letter counts page\nletter_frequency <-\n  xml2::read_html(\"https://en.wikipedia.org/wiki/Letter_frequency\")\n\n# Find the right table\n# Thanks Robert Lewand and Pavel Micka\ntables <- html_nodes(letter_frequency, \"table\")\noverall_letter <- html_table(tables[1], fill = TRUE)[[1]]\noverall_letter <- overall_letter[c(1, 3)]\nnames(overall_letter) <- c(\"Letter\", \"Frequency\")\noverall_letter\n\nggplot(data = overall_letter, aes(Letter, Frequency)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"Overall\")\nggsave(filename = here::here(\"images\", \"overall_letter.png\"))\n\n\nThe last two sections combine the data and save it for future use.\n\n#-------------\n# Combine data\n#-------------\noverall_letter$Source <- \"Overall\"\nds_letter$Source <- \"Data Science\"\nfreqs <- rbind(overall_letter, ds_letter)\nfreqs\n\n\n\n\n\nThe final data set\n \n  \n    Letter \n    Frequency \n    Source \n  \n \n\n  \n    a \n    8.50 \n    Overall \n  \n  \n    b \n    1.49 \n    Overall \n  \n  \n    c \n    2.20 \n    Overall \n  \n  \n    d \n    4.25 \n    Overall \n  \n  \n    e \n    11.16 \n    Overall \n  \n  \n    f \n    2.23 \n    Overall \n  \n  \n    g \n    2.02 \n    Overall \n  \n  \n    h \n    6.09 \n    Overall \n  \n  \n    i \n    7.55 \n    Overall \n  \n  \n    j \n    0.15 \n    Overall \n  \n  \n    k \n    1.29 \n    Overall \n  \n  \n    l \n    4.03 \n    Overall \n  \n  \n    m \n    2.41 \n    Overall \n  \n  \n    n \n    6.75 \n    Overall \n  \n  \n    o \n    7.51 \n    Overall \n  \n  \n    p \n    1.93 \n    Overall \n  \n  \n    q \n    0.10 \n    Overall \n  \n  \n    r \n    7.59 \n    Overall \n  \n  \n    s \n    6.33 \n    Overall \n  \n  \n    t \n    9.36 \n    Overall \n  \n  \n    u \n    2.76 \n    Overall \n  \n  \n    v \n    0.98 \n    Overall \n  \n  \n    w \n    2.56 \n    Overall \n  \n  \n    x \n    0.15 \n    Overall \n  \n  \n    y \n    1.99 \n    Overall \n  \n  \n    z \n    0.08 \n    Overall \n  \n  \n    a \n    10.72 \n    Data Science \n  \n  \n    b \n    1.31 \n    Data Science \n  \n  \n    c \n    5.06 \n    Data Science \n  \n  \n    d \n    5.08 \n    Data Science \n  \n  \n    e \n    11.53 \n    Data Science \n  \n  \n    f \n    2.01 \n    Data Science \n  \n  \n    g \n    1.94 \n    Data Science \n  \n  \n    h \n    2.87 \n    Data Science \n  \n  \n    i \n    9.38 \n    Data Science \n  \n  \n    j \n    0.22 \n    Data Science \n  \n  \n    k \n    0.34 \n    Data Science \n  \n  \n    l \n    3.65 \n    Data Science \n  \n  \n    m \n    2.31 \n    Data Science \n  \n  \n    n \n    8.21 \n    Data Science \n  \n  \n    o \n    5.88 \n    Data Science \n  \n  \n    p \n    1.77 \n    Data Science \n  \n  \n    q \n    0.19 \n    Data Science \n  \n  \n    r \n    4.97 \n    Data Science \n  \n  \n    s \n    7.71 \n    Data Science \n  \n  \n    t \n    9.50 \n    Data Science \n  \n  \n    u \n    2.01 \n    Data Science \n  \n  \n    v \n    0.72 \n    Data Science \n  \n  \n    w \n    1.02 \n    Data Science \n  \n  \n    x \n    0.15 \n    Data Science \n  \n  \n    y \n    1.26 \n    Data Science \n  \n  \n    z \n    0.19 \n    Data Science \n  \n\n\n\n\n\n\n# Save the file in case it's needed later after Wikipedia updates\nwrite.csv(freqs,\n          file = here::here(\"freqs.csv\"),\n          row.names = FALSE)\n\nThe source code file is found here. This data will be read back into R to create the images in the next post."
  },
  {
    "objectID": "posts/2021-10-19-power-bi-and-missing-values/index.html",
    "href": "posts/2021-10-19-power-bi-and-missing-values/index.html",
    "title": "Power BI and Missing Values",
    "section": "",
    "text": "This presentation comes from a project I worked on involving these issues with missing values in Power BI. For this presentation, I’m assuming you have some familiarity with Power BI. So, I won’t walk through all the basics like pulling in data but start after the beginning steps.\n\n\n\n\n\nFor today’s setup, we’ll cover a background example that explains the issues, go through some basic strategies that might solve your problem, a better strategy that’s more work but a lot more robust, and then some additional options for polishing the dashboard.\n\n\n\n\n\nWe’ll start with an orders table called orders_small. Each row is an order that contains the order number, the product that was sold, a customer id, the date, and the dollar amount. The goal here is to get information on sales over time for each product. So we basically need to sum the dollar amount by date.\n\n\n\n\n\nShowing the data by day will be too granular, so we want to get it into months. We can add in some DAX code to create a new column for the month of each order. We’ll also just use the date as a date hierarchy and compare the methods.\n\n\n\n\n\nThis graph uses the month column we just created. You can see there are some missing values at the start and end of the dark blue line. However, the orange line is suspiciously straight.\n\n\n\n\n\nThis graph uses the date column from the data at the month level. You still see the missing values for the dark blue line, but now the orange line has missing values. This is because the other graph was just connecting the segments across January to November.\n\n\n\n\n\nLooking closely at the graph, we see that these three products sell at different times of the year. Product A sells year-round. Product B is only in the summer, while product C is only in the winter.\n\n\n\n\n\nWhat we really want to see are all the lines filled in down to zero. The poorly drawn lines display what we want to get at the end.\n\n\n\n\n\nWe can also see that this can mess up any calculations. In this example, the monthly_average is wrong because distinct_months is incorrect for selling over an entire year.\n\n\n\n\n\nFor this presentation, we’ll focus on creating an end product that has the following features. Line graphs and tables by product over time. Graphs and tables for dollar amounts, number of distinct customers, and number of orders. The ability to filter by product and month. And finally, fill in zeros when the product was available.\n\n\n\n\n\nThe following section will cover some basic strategies to fill in zeros for missing values. These techniques will handle a lot of problems and are pretty easy to implement.\n\n\n\n\n\nIf you click on the down arrow on your axis, there will be an option for ‘Show items with no data’. This is always the first option I try. Unfortunately, it only works in some situations and doesn’t work in this example.\n\n\n\n\n\nWe can just fix the math if you know it’s a constant. All we need to do is add 0 to the end of our measure, and it will fill in for us. But, unfortunately, this only fixes the graph and not the table.\n\n\n\n\n\nWe can fix the table by skipping the DISTINCTCOUNT on months and replace it all with 12. This will get the correct monthly_average. However, this only works when we know it’s always 12. Also, it doesn’t fix the graph.\n\n\n\n\n\nFinally, we can always just fix the issues in the data. For example, we can clean up data by importing monthly data from the start and importing rows with zeros. There are ways to do that in SQL and Power Query. We won’t go into them in this presentation, but it’s a great option if you know you’ll never need more granular data.\n\n\n\n\n\nThese strategies tend to work very easily. None of them took a lot of effort to implement from where we started. However, they tend to not be extendable. We have to mix and match these strategies to get everything, and that’s annoying. They can fall apart for more advanced calculations and become messy when trying to keep it all together. In my experience, if you just need basic graphs and tables, these are great. However, a more thorough setup can be better if you know you’ll need more advanced calculations and interactivity.\n\n\n\n\n\nThis next section goes through that more thorough setup. We’ll reset everything and build a stronger start that enables us to get any calculations we need.\n\n\n\n\n\nThere’s still a year of data but a lot more products. There are other trends, including decreasing or increasing over time, and even products that only sold for one month (those are the points). We’ll call this orders_big.\n\n\n\n\n\nThis is just a reminder of what we’re looking to create by the end. Line graphs and tables by product over time. Graphs and tables for dollar amounts, number of distinct customers, and number of orders. The ability to filter by product and month. And finally, fill in zeros when the product was available.\n\n\n\n\n\nThere are three parts to this section. The first part is setting up a calendar table to use as a date dimension.\n\n\n\n\n\nA calendar table is a table created in Power BI that contains all the dates you need and associated information. We’ll use the Calendar Dax function. This function requires a start and end date. It’ll then fill out all the dates in between (including the start and end dates). We’ll go through three examples.\n\n\n\n\n\nWe have three different ways to create a date table using the CALENDAR function. The first way provides specific dates. This is useful if you need the graph to keep the same date range even if your data changes. The second way uses the TODAY function to get the current date. This will keep your dashboard current no matter what happens with your data. The last way pulls the start and end dates from the data. This will keep your graphs matching the data even if it doesn’t refresh. I tend to use either of the last two depending on the business need.\nThere are other options like CALENDARAUTO or setting a date table, but we’ll stick with CALENDAR for this presentation. It’s definitely worth your time to spend an afternoon looking into the other options if you’re going to implement this at your work.\n\n\n\n\n\nSince we’ll want to group by month, we’ll add in a month column. We’ll also change the format to match our data. You typically always want it to match as this can speed up calculations.\n\n\n\n\n\nThen the last step here is connecting the date table to our data.\n\n\n\n\n\nThe next step for a great setup is using a star schema.\n\n\n\n\n\nThe star schema will work by placing the main orders_big table in the middle. Then have essential dimensions on the outside, like our date table. Often you’ll see this has a ‘suggestion’ from Microsoft, but it’s almost always a good idea from the beginning.\n\n\n\n\n\nLet’s add in two other dimension tables, product, and customer. We’ll just summarize the columns from the orders_big table. If they were in other tables in a database, you could bring them in separately. We can sort of see the star shape. It’s just missing the two legs at the bottom. This setup will make our data easy to use with functions.\n\n\n\n\n\nThat brings us to our last step with using the ADDMISSINGITEMS function. This will complete our setup.\n\n\n\n\n\nThis function is defined as ‘Adds rows with empty values to a table returned by SUMMARIZECOLUMNS.’ from the Microsoft documentation or ‘Add the rows with empty measure values back.’ from the Dax guide. I don’t find either of those definitions intuitive, but working through the parameters, output, and examples will help explain what it does.\n\n\n\n\n\nThere are four parameters. ShowAll_ColumnName is what you want to fill in. The Table parameter will be output from SUMMARIZECOLUMNS with the missing data. It’ll be based on orders_big for us. The GroupBy_ColumnName is also what you want to fill in unless you’re using ROLLUP. We won’t be using ROLLUP in any of the examples, so it’ll always match ShowAll_ColumnName. Finally, you can add filter tables to remove filled-in values you don’t actually want to add.\n\n\n\n\n\nFor our setup, we’ll want to fill in any missing combinations of month and product. We can pull these from our dimensions in the star schema. The Table parameter is the sum of the dollar_amount by month and product. Ideally, we’ll see all combinations of month and product with the dollar_amount summed if there were sales that month.\n\n\n\n\n\nPutting it all into the ADDMISSINGITEMS function, you can see ShowAll_ColumnName cal_month and product, the table is the SUMMARIZECOLUMNS, the GroupBy_ColumnName is the same as the ShowAll_ColumnName, no filters for now. This is basically a full join between calendar months and products, then a left join to our summarized data. Finally, we can see some of the missing months in the image.\n\n\n\n\n\nI like to add in the amount filled, but you can replace the amount if you want.\n\n\n\n\n\nAnd now we finally have the graph we wanted.\n\n\n\n\n\nIf you have a lot of data, you might get a warning about not seeing all the data in the top corner. There’s an option that samples the data to increase speed. You can turn that off to see more lines. There’s not a strict cutoff on data size. It just depends on what you’re graphing.\n\n\n\n\n\nNow we can compare the summary stats. We can see all the months are filled in, so the average monthly amount is now correct.\n\n\n\n\n\nNow that we have the main setup, we can move to the last big section on polishing up the dashboard.\n\n\n\n\n\nThe first part is filtering the data to exclude months where the product wasn’t available for sale. We should end up with filled-in months only when the product was an option.\n\n\n\n\n\nLet’s say different products were available for different months. We’ll have this in a table called product_dates. This would be saved in a database then pulled into Power BI.\n\n\n\n\n\nWe can join this to our products table. In the product_dates table, there are multiple rows per product for cases when there are jumps in the dates. So, for example, products available during the summer could have a row for January to March 2020 and a row for November to December 2020. Therefore you can’t just join to orders_big table because that would be a many-to-many join.\n\n\n\n\n\nWe’re going to build a table with months filled down by using CROSSJOIN on our date table and product_dates. Then filter down to the combinations we want. (There are other ways to do this, especially when bringing the data into Power BI)\n\n\n\n\n\nThen we just join to our other tables.\n\n\n\n\n\nTo use this table as a filter, we just need to add it to the end of our ADDMISSINGITEMS call. This will filter out combinations that weren’t in our product_availability table.\n\n\n\n\n\nSame as previous orders_zeros_added table, but with product_availability used as a filter.\n\n\n\n\n\nNow we can see gaps in the data where there are months without product availability. However, it’s still not quite right. Notice the orange line across the middle.\n\n\n\n\n\nTo fix the issue of joining over missing dates, the X-axis needs to be set to Categorical. And now, everything is good.\n\n\n\n\n\nChecking the summary tables shows everything is working here too.\n\n\n\n\n\nLet’s say you either don’t want the X-axis to be categorical, or you want all the months missing for no product availability. We’ll go back before we filter on availability and reset a little bit.\n\n\n\n\n\nWe’ll create a new star schema around our orders_big table. We can’t reuse the same tables, or we’ll get a circular dependency error. So, we’ll add in product and date dimensions and recreate the table to check product availability. We can reduce our date table to just months since we have already summarized the data.\n\n\n\n\n\nOn this slide, we can see the two star schemas.\n\n\n\n\n\nTo incorporate the product availability check, we’ll create a measure using LOOKUPVALUE to see if the month-product combination existed. Then build another measure to fill in zeros or keep the missing value.\n\n\n\n\n\nWe have our continuous axis graph. Note that you need to use cal_table_month for the axis.\n\n\n\n\n\nOur summary tables are still good.\n\n\n\n\n\nOne big catch for mixing missing and zeros in Power BI is that BLANK is not like SQL NULL. BLANK will convert to 0 in summations and substractions but will stay BLANK in division and multiplication. For example, if you had this equation and B is BLANK, the left side will equal 1 because A divided by B is BLANK, then 1 minus BLANK is 1. However, the right side is BLANK because B minus A is negative A, then negative A divided by B is BLANK. Even though these should match. So, depending on how you set up your measures, you can get different results. You’ll need to watch this in calculations.\n\n\n\n\n\nIn addition to the amount, we wanted measures for customers and orders.\n\n\n\n\n\nAll we need to add is DISTINCTCOUNT of customer_id and carry through, exactly like using amount.\n\n\n\n\n\nWe can see the appropriate graph.\n\n\n\n\n\nAnd the appropriate tables.\n\n\n\n\n\nAdding in the number of orders is also exactly the same.\n\n\n\n\n\nSimilar graphs.\n\n\n\n\n\nSimilar tables.\n\n\n\n\n\nIn some cases, you’ll need to use DISTINCTCOUNT after filling in the missing months. Like if you need more advanced filtering or calculations. For example, distinct customers over a year will only equal the sum of distinct customers every month if there are no repeated customers. To solve this problem, you can bring customer_id in the SUMMARIZECOLUMNS in ADDMISSINGITEMS but not place it in the ShowAll_ColumnName parameter. This will bring the individual customer_id’s through but not fill in missing months for them. You’ll end up with month-product rows that have missing values for customer_id and order_id.\n\n\n\n\n\nFinally, the last step is to add filters for product and month.\n\n\n\n\n\nYou can use built-in filters for the table either in the filter panel or slicers. You can also build out a star schema again if you want. That’s pretty much it. All the usual Power BI features work from here.\n\n\n\n\n\nAnd that’s the presentation. Thanks!"
  },
  {
    "objectID": "posts/2022-05-28-circuitry/index.html",
    "href": "posts/2022-05-28-circuitry/index.html",
    "title": "Circuitry",
    "section": "",
    "text": "Background\n\n\n\nThis small generative art project uses code from my previous work. I really liked the looped path objects and decided to try something else with them. This work expands the loop a little, adds a randomly generated color scheme, and uses a standard image style.\n\n\n\nCode\n\n\n\nThe first function is get_maze. This code creates a maze from a size-by-size grid. The final code uses a base size of ten. The maze starts at the bottom middle and randomly adds edges.\n\n\n[get_maze function]\nget_maze <- function(size) {\n  \n  # Sets up base data set of potential edges for the maze\n  # size is always 5 for right now\n  edges <- CJ(\n    x1 = rep(seq(1, size), 2),\n    y1 = seq(1, size)\n  )\n  edges[, \":=\"(x2 = ifelse(.I %% 2 == 0, x1 + 1, x1),\n               y2 = ifelse(.I %% 2 == 1, y1 + 1, y1))]\n  edges <- edges[x2 <= size & y2 <= size, ]\n  edges[, id := seq(1, nrow(edges))]\n  edges[, \":=\"(node1 = (x1 - 1) * size + (y1 - 1) + 1,\n               node2 = (x2 - 1) * size + (y2 - 1) + 1)]\n  setkey(edges, id)\n  \n  # data set of nodes\n  nodes <- unique(rbind(edges[, .(id = node1)], edges[, .(id = node2)]))\n  nodes[, connected := 0]\n  setkey(nodes, id)\n  \n  # data set of node id to edge ids\n  nodes_edges <- unique(rbind(edges[, .(id = node1, edge = id)][], \n                              edges[, .(id = node2, edge = id)]))\n  setkey(nodes_edges, id)\n  \n  # location : 1 for maze, 0 for frontier, -1 for uncharted, -2 for discarded\n  # starting point : bottom middle\n  # include bottom middle then either off to the sides or up\n  starting_edge <- edges[(x1 == 3 & y1 == 1) |\n                           (x1 == 2 & y1 == 1 & x2 == 3 & y2 == 1), ][sample(.N, 1), ]\n  \n  # Set up base columns\n  edges[, \":=\"(location = -1,\n               probability = 0)]\n  edges[.(starting_edge$id), \":=\"(location = 1,\n                                  probability = 0)]\n  nodes[.(c(starting_edge$node1, starting_edge$node2)), connected := 1]\n  edges[.(nodes_edges[.(c(starting_edge$node1, starting_edge$node2)), \"edge\"]), \":=\"\n        (location = fifelse(location == -1, 0, location),\n          probability = fifelse(location == -1, 1, probability))]\n  \n  #### Loop through maze generation ----\n  num_edges <- 1\n  while (num_edges < (size^2 - 1)) {\n    \n    # select next edge\n    selected_edge <- edges[sample(.N, 1, prob = probability), ]\n    \n    ## if it's good, then\n    # add it to the maze\n    # add connecting edges to the frontier\n    # else add it to discard\n    if (any(nodes[.(c(selected_edge$node1, selected_edge$node2))\n                  , connected] == 0)) {\n      \n      # add to maze\n      edges[.(selected_edge$id), \":=\"(location = 1,\n                                      probability = 0)]\n      \n      # update nodes\n      nodes[.(c(selected_edge$node1, selected_edge$node2)), connected := 1]\n      \n      # update frontier\n      edges[.(nodes_edges[.(c(selected_edge$node1, selected_edge$node2))\n                          , \"edge\"]), \":=\"\n            (location = fifelse(location == -1, 0, location),\n              probability = fifelse(location == -1, 1, probability))]\n      \n      num_edges <- num_edges + 1\n    } else {\n      # drop from frontier\n      edges[.(selected_edge$id), \":=\"(location = -2,\n                                      probability = 0)]\n    }\n  }\n  \n  return(edges[location == 1, ])\n}\n\n\nThe next function, update_maze, doubles the edges so that the maze follows a loop. The new connections trace the outline of the maze. This action mimics exploring the maze and following the path back to the start.\n\n\n[update_maze function]\nupdate_maze <- function(edges) {\n  \n  # list out all possible edges\n  # (basically same code as setting up the maze)\n  # plus adds edges that stick out on the outside\n  size <- max(edges$x1) + 1\n  all_possible_edges <- CJ(\n    x1 = rep(seq(1, size), 2) - 1,\n    y1 = seq(1, size) - 1\n  )\n  all_possible_edges[, \":=\"(x2 = ifelse(.I %% 2 == 0, x1 + 1, x1),\n                            y2 = ifelse(.I %% 2 == 1, y1 + 1, y1))]\n  all_possible_edges <- all_possible_edges[(x1 != 0 | x2 != 0) &\n                                             (y1 != 0 | y2 != 0), ]\n  \n  edges <- edges[, .(x1, x2, y1, y2, id)]\n  \n  # merge maze and all possible edges to see which ones weren't used\n  all_possible_edges <- merge(all_possible_edges, edges,\n                              by = c(\"x1\", \"y1\", \"x2\", \"y2\"),\n                              all.x = TRUE\n  )\n  \n  # This function subs in the new edges appropriately\n  # basically, any path edge needs to be updated to two edges so the maze\n  # starts at the bottom middle, travels through the maze, and back to the start\n  create_new_edges <- function(x1, y1, x2, y2, id) {\n    # if no edges, add block\n    if (is.na(id)) {\n      if (y1 == y2) { # horizontal edge\n        list(\n          x1_1 = 2 * x1,\n          y1_1 = 2 * y1 - 1,\n          x2_1 = 2 * x1,\n          y2_1 = 2 * y1,\n          x1_2 = 2 * x2 - 1,\n          y1_2 = 2 * y2 - 1,\n          x2_2 = 2 * x2 - 1,\n          y2_2 = 2 * y2\n        )\n      } else { # vertical edge\n        list(\n          x1_1 = 2 * x1 - 1,\n          y1_1 = 2 * y1,\n          x2_1 = 2 * x1,\n          y2_1 = 2 * y1,\n          x1_2 = 2 * x2 - 1,\n          y1_2 = 2 * y2 - 1,\n          x2_2 = 2 * x2,\n          y2_2 = 2 * y2 - 1\n        )\n      }\n    } else { # has edge, add connections\n      if (y1 == y2) { # horizontal edge\n        list(\n          x1_1 = 2 * x1,\n          y1_1 = 2 * y1 - 1,\n          x2_1 = 2 * x2 - 1,\n          y2_1 = 2 * y2 - 1,\n          x1_2 = 2 * x1,\n          y1_2 = 2 * y1,\n          x2_2 = 2 * x2 - 1,\n          y2_2 = 2 * y2\n        )\n      } else { # vertical edge\n        list(\n          x1_1 = 2 * x1 - 1,\n          y1_1 = 2 * y1,\n          x2_1 = 2 * x2 - 1,\n          y2_1 = 2 * y2 - 1,\n          x1_2 = 2 * x1,\n          y1_2 = 2 * y1,\n          x2_2 = 2 * x2,\n          y2_2 = 2 * y2 - 1\n        )\n      }\n    }\n  }\n  \n  # fill in blocks and paths\n  all_possible_edges[, c(\n    \"x1_1\", \"y1_1\", \"x2_1\", \"y2_1\",\n    \"x1_2\", \"y1_2\", \"x2_2\", \"y2_2\"\n  ) := create_new_edges(x1, y1, x2, y2, id),\n  by = seq_len(nrow(all_possible_edges))\n  ]\n  \n  # clean everything up\n  all_possible_edges[, \":=\"(x1 = NULL,\n                            y1 = NULL,\n                            x2 = NULL,\n                            y2 = NULL,\n                            id = NULL)]\n  \n  all_possible_edges <- melt(all_possible_edges,\n                             measure.vars = patterns(\"x1\", \"y1\", \"x2\", \"y2\"),\n                             value.name = c(\"x1\", \"y1\", \"x2\", \"y2\")\n  )[, variable := NULL]\n  \n  all_possible_edges <- all_possible_edges[(x1 > 0 &\n                                              y1 > 0 &\n                                              x2 < (2 * size - 1) &\n                                              y2 < (2 * size - 1)), ]\n}\n\n\nThe last external function, maze_to_path, puts all the edges in order from the bottom left corner, traveling through the loop and back to the beginning.\n\n\n[maze_to_path function]\nmaze_to_path <- function(edges) {\n  # set up id\n  edges[, id := .I]\n  setkey(edges, id)\n  \n  # set up nodes data set\n  nodes <- unique(rbind(edges[, .(x = x1, y = y1)]\n                        , edges[, .(x = x2, y = y2)]))\n  nodes[, id := .I]\n  setkey(nodes, id)\n  \n  # add node ids to edges data set\n  edges <- merge(edges, nodes,\n                 by.x = c(\"x1\", \"y1\"), by.y = c(\"x\", \"y\"),\n                 suffixes = c(\"\", \"_node_1\"), all.x = TRUE\n  )\n  edges <- merge(edges, nodes,\n                 by.x = c(\"x2\", \"y2\"), by.y = c(\"x\", \"y\"),\n                 suffixes = c(\"\", \"_node_2\"), all.x = TRUE\n  )\n  \n  # nodes to edges look up table\n  nodes_edges <- unique(rbind(\n    edges[, .(id = id_node_1, edge = id, connecting_node = id_node_2)],\n    edges[, .(id = id_node_2, edge = id, connecting_node = id_node_1)]\n  ))\n  setkey(nodes_edges, id)\n  \n  # save spot for path\n  path <- vector(mode = \"numeric\")\n  \n  # variables to keep track of progress through the maze\n  current_node <- nodes[y == 1 & x == 1, id]\n  first_node <- current_node\n  \n  # update path\n  path <- append(path, current_node)\n  previous_node <- current_node\n  \n  # keep going to unexplored nodes\n  current_node <- nodes_edges[.(current_node), \n  ][connecting_node != previous_node\n    , connecting_node][1]\n  \n  # continue through the whole path\n  while (current_node != first_node) {\n    path <- append(path, current_node)\n    future_node <- nodes_edges[.(current_node), \n    ][connecting_node != previous_node\n      , connecting_node]\n    previous_node <- current_node\n    current_node <- future_node\n  }\n  \n  path <- data.table(\n    order = seq(1, length(path)),\n    node = path\n  )\n  path <- merge(path, nodes, by.x = c(\"node\"), by.y = c(\"id\"))\n}\n\n\nThe main workhorse utilizes the previous functions to get a set of connections.\nThe color scheme code follows a random circle in the HCL color space. There are ten points equidistant around the circle. Sometimes the first random circle will produce invalid color values. So the code loops until it finds a complete set. The circle’s center is saved and used as a border for the images and fills small circles where the connections meet.\nFinally, a sine wave with a random number of waves determines the connections’ sizes.\n\n\n[generate_output code]\nlibrary(data.table)\nlibrary(ggplot2)\n\nsource(\"get_maze.R\")\nsource(\"update_maze.R\")\nsource(\"maze_to_path.R\")\n\nget_set <- function(size) {\n  set <- get_maze(size)\n  set <- update_maze(set)\n  set <- maze_to_path(set)\n  \n  set[, \":=\"(x = x - (size + .5),\n             y = y - (size + .5))]\n  \n  set <- set[order(order), ]\n  set[, ':=' (xend = shift(x, type = \"lead\", fill = 1 - (size + .5) ),\n              yend = shift(y, type = \"lead\", fill = 1 - (size + .5) ))]\n}\n\n# Get color scheme by finding a random circle in HCL color space\nget_colors <- function() {\n  \n  # two random vectors\n  v1 <- runif(3, -1, 1)\n  v2 <- runif(3, -1, 1)\n  # orthogonal\n  v2 <- v2 - c(v1 %*% v2 / v1 %*% v1 ) * v1\n  # unit\n  v1 <- v1 / sqrt(c(v1 %*% v1))\n  v2 <- v2 / sqrt(c(v2 %*% v2))\n  \n  # random point\n  p <- runif(3, -30, 30) + c(0, 0, 50)\n  \n  # random radius\n  r <- runif(1, 10, 30)\n  \n  # ten points around the circle\n  # need to keep both end points even though they're the same value\n  # for scale_color_gradientn to loop back around\n  t <- seq(0, 2 * pi, length.out = 11)\n  \n  colors <- data.table(p1 = p[1],\n                       p2 = p[2],\n                       p3 = p[3],\n                       r = r,\n                       t = t,\n                       v11 = v1[1],\n                       v12 = v1[2],\n                       v13 = v1[3],\n                       v21 = v2[1],\n                       v22 = v2[2],\n                       v23 = v2[3])\n  \n  colors[, ':=' (x = p1 + r * cos(t) * v11 + r * sin(t) * v21,\n                 y = p2 + r * cos(t) * v12 + r * sin(t) * v22,\n                 z = p3 + r * cos(t) * v13 + r * sin(t) * v23)]\n  colors[, \":=\" (H = (atan2(y, x) * 180/pi) %% 360,\n                 C = sqrt(x^2 + y^2),\n                 L = z)]\n  \n  \n  colors[, ':=' (hex_value = ifelse(L < 0 | L > 100, NA_character_, \n                                    hcl(H, C, L, fixup = FALSE))), \n         by = seq_len(nrow(colors))]\n\n  return(colors)\n}\n\n# Sometimes the circle will be out of range,\n# so try again until all the colors are valid\nget_color_scheme <- function() {\n  colors <- get_colors()\n\n  while(any(is.na(colors$hex_value))) {\n    colors <- get_colors()\n  }\n  \n  return(colors)\n}\n\n# Create 10 outputs\nset.seed(10101010)\nfor(i in 1:10) {\n  size <- 10\n  set <- get_set(size)\n  \n  colors <- get_color_scheme()\n  \n  # Save center of the circle\n  color_center <- hcl(h = (atan2(colors$p2[1], colors$p1[1]) * 180/pi) %% 360,\n                      c = sqrt(colors$p1[1]^2 + colors$p2[1]^2),\n                      l = colors$p3[1])\n  \n  color_scheme <- colors[[\"hex_value\"]]\n  \n  # Get boundaries of white background square\n  boundaries <- data.table(x = c(min(set$x), max(set$x), max(set$x), min(set$x)),\n                           y = c(min(set$y), min(set$y), max(set$y), max(set$y)))\n  boundaries[, ':=' (x = x * 1.1,\n                     y = y * 1.1)]\n  \n  # Set up connections size\n  # order goes from 1 to 400\n  # 400 / waves = num segments per wave\n  waves <- round(runif(1, 70, 130))\n  offset <- runif(1, 0, 2*pi)\n  set[, size := order / max(order) * 2*pi]\n  set[, size := (sin(waves * size + offset) + 1) * 2.5 ]\n  \n  ggplot() +\n    geom_polygon(data = boundaries,\n                 aes(x, y),\n                 color = \"white\",\n                 fill = \"white\") +\n    geom_segment(data = set,\n                 aes(x, y,\n                     xend = xend, yend = yend, \n                     color = order, size = size),\n                 alpha = .5,\n                 lineend = \"round\") +\n    geom_segment(data = set,\n                 aes(x, y,\n                     xend = xend, yend = yend, \n                     color = order, size = size / 5),\n                 alpha = 1,\n                 lineend = \"round\") +\n    geom_point(data = set,\n               aes(x, y), \n               color = color_center,\n               size = .5) +\n    scale_color_gradientn(colours = color_scheme) +\n    theme_void() +\n    theme(legend.position = \"none\") +\n    coord_equal()\n  ggsave(paste0(\"output/output_\", i, \".jpeg\"), height = 5, width = 5, bg = color_center)\n}\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\n\n\nOutput 1\n\n\n\n\n\n\n\nOutput 2\n\n\n\n\n\n\n\n\n\nOutput 3\n\n\n\n\n\n\n\nOutput 4\n\n\n\n\n\n\n\n\n\nOutput 5\n\n\n\n\n\n\n\nOutput 6\n\n\n\n\n\n\n\n\n\nOutput 7\n\n\n\n\n\n\n\nOutput 8\n\n\n\n\n\n\n\n\n\nOutput 9\n\n\n\n\n\n\n\nOutput 10"
  },
  {
    "objectID": "posts/2021-08-10-choosing-nearby-colors-part-2/index.html",
    "href": "posts/2021-08-10-choosing-nearby-colors-part-2/index.html",
    "title": "Choosing Nearby Colors Part 2",
    "section": "",
    "text": "This post looks into what happens if you just start with a “sphere” based on Hue, Chroma, Luminance values. So we’ll set up shapes that expand out from the base point along Hue, Chroma, and Luminance directly.1\n\n\n\nBase Point Location\n\n\n\n\n\n[A Lot of Setup Code]\n##---------\n# Libraries\n##---------\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(colorspace)\n\n##------------\n# Pick a color\n##------------\nH_point <- 322\nC_point <- 26\nL_point <- 69\n\ncolor_hex <- hcl(H_point, \n                 C_point, \n                 L_point,\n                 fixup = FALSE)\n\n##--------------------------------\n# See color in H, C, L color space \n##--------------------------------\n# C-L Plane ----\nget_C_L_plane <- function(H_point) {\n  expand_grid(H = H_point,\n              C = seq(0, 180, .5),\n              L = seq(1, 100, .5)) %>%\n    mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n    filter(!is.na(color_value))\n}\nC_L_plane <- get_C_L_plane(H_point)\n\ngraph_C_L_plane <- function(C_L_plane, color_hex) {\n  ggplot() +\n    geom_point(data = C_L_plane,\n               aes(C, L, color = color_value, fill = color_value)) +\n    scale_x_continuous(labels = abs) +\n    scale_color_identity() +\n    scale_fill_identity() +\n    geom_point(aes(x = C_point,\n                   y = L_point),\n               color = 'black',\n               fill = color_hex,\n               shape = 21,\n               size = 2) +\n    coord_equal()\n}\ngraph_C_L_plane(C_L_plane, color_hex)\n\n# H-L Curve ----\nget_H_L_curve <- function(C_point) {\n  expand_grid(H = seq(1, 360, 1),\n              C = C_point,\n              L = seq(1, 100, .5)) %>%\n    mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n    filter(!is.na(color_value))\n}\nH_L_curve <- get_H_L_curve(C_point)\n\nlabel_H_center <- function(H_point, ...) {\n  function(x) {(x + (180 - H_point)) %% 360}\n}\n\ngraph_H_L_curve <- function(H_L_curve, color_hex, H_point) {\n  ggplot() +\n    geom_point(data = H_L_curve,\n               aes((H + (180 - H_point)) %% 360, L, \n                   color = color_value, fill = color_value)) +\n    scale_color_identity() +\n    scale_fill_identity() +\n    geom_point(aes(x = 180, # Because we rotated points to not drop over edge\n                   y = L_point),\n               color = 'black',\n               fill = color_hex,\n               shape = 21,\n               size = 2) +\n    scale_x_reverse('H', # Like you're standing on the inside\n                    labels = label_H_center(H_point = H_point),\n                    limits = c(360, 0)) +\n    coord_equal()\n}\ngraph_H_L_curve(H_L_curve, color_hex, H_point)\n\n# H-C plane ----\nget_H_C_plane <- function(L_point){\n  expand_grid(H = seq(1, 360, 1),\n              C = seq(0, 180, .5),\n              L = L_point) %>%\n    mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n    filter(!is.na(color_value))\n}\nH_C_plane <- get_H_C_plane(L_point)\n\ngraph_H_C_plane <- function(H_C_plane, color_hex) {\n  ggplot() +\n    geom_point(data = H_C_plane,\n               aes(H, C, color = color_value, fill = color_value)) +\n    scale_color_identity() +\n    scale_fill_identity() +\n    scale_x_continuous(breaks = seq(45, 360, 45),\n                       minor_breaks = seq(0, 315, 45) + 45/2,\n                       labels = c('45', '90', '135', '180', \n                                  '225', '270', '315', '0|360')) +\n    scale_y_continuous(limits = c(0, 180)) +\n    geom_point(aes(x = H_point,\n                   y = C_point),\n               color = 'black',\n               fill = color_hex,\n               shape = 21,\n               size = 2) +\n    coord_polar(start = 270 * pi / 180,\n                direction = -1)\n}\ngraph_H_C_plane(H_C_plane, color_hex)\n\n# C tangent plane ----\nC_circle <- data.frame(H = seq(1, 360),\n                       C = C_point,\n                       color_value = \"white\")\nC_tangent_plane <- expand_grid(x = C_point, # Plane perpendicular to H at C\n                               perpendicular_from_C_L = \n                                 seq(-sqrt(180^2 - C_point^2), sqrt(180^2 - C_point^2)),\n                               L = seq(1, 100, 1)) %>%\n  mutate(x_rotate = x * cos(H_point * pi/180) -  # rotate\n           perpendicular_from_C_L * sin(H_point * pi/180),\n         y_rotate = x * sin(H_point * pi/180) + \n           perpendicular_from_C_L * cos(H_point * pi/180)) %>%\n  mutate(x = x_rotate,\n         y = y_rotate) %>%\n  select(-x_rotate, -y_rotate)  %>%\n  mutate(H = (atan2(y, x) * 180/pi) %% 360,\n         C = sqrt(x^2 + y^2)) %>%\n  mutate(color_value = \"white\")\nggplot(data = H_C_plane,\n       aes(H, C, color = color_value, fill = color_value)) +\n  geom_point() +\n  scale_color_identity() +\n  scale_fill_identity() +\n  scale_x_continuous(breaks = seq(45, 360, 45),\n                     minor_breaks = seq(0, 315, 45) + 45/2,\n                     labels = c('45', '90', '135', '180', \n                                '225', '270', '315', '0|360')) +\n  scale_y_continuous(limits = c(0, 180)) +\n  geom_path(data = C_circle) +\n  geom_segment(x = H_point,\n               y = 0,\n               xend = H_point,\n               yend = C_point,\n               col = \"white\") +\n  geom_point(data = C_tangent_plane, col = \"black\") +\n  geom_point(x = H_point,\n             y = C_point,\n             color = 'black',\n             fill = color_hex,\n             shape = 21) +\n  coord_polar(start = 270 * pi / 180,\n              direction = -1)\n\nget_C_tangent_plane <- function(H_point, C_point) {\n  expand_grid(x = C_point, # Plane perpendicular to H at C\n              perpendicular_from_C_L = seq(-180, 180, .5),\n              L = seq(1, 100, .5)) %>%\n    mutate(x_rotate = x * cos(H_point * pi/180) -  # rotate\n             perpendicular_from_C_L * sin(H_point * pi/180),\n           y_rotate = x * sin(H_point * pi/180) + \n             perpendicular_from_C_L * cos(H_point * pi/180)) %>%\n    mutate(x = x_rotate,\n           y = y_rotate) %>%\n    select(-x_rotate, -y_rotate)  %>%\n    mutate(H = (atan2(y, x) * 180/pi) %% 360,\n           C = sqrt(x^2 + y^2)) %>%\n    mutate(color_value = hcl(H, C, L, fixup = FALSE)) %>%\n    filter(!is.na(color_value))\n}\nC_tangent_plane <- get_C_tangent_plane(H_point, C_point)\n\ngraph_C_tangent_plane <- function(C_tangent_plane, color_hex) {\n  ggplot() +\n    geom_point(data = C_tangent_plane,\n               aes(perpendicular_from_C_L, L, \n                   color = color_value, fill = color_value)) +\n    scale_color_identity() +\n    scale_fill_identity() +\n    scale_x_reverse(\"Distance Perpendicular to C-L Plane\",\n                    labels = abs) +\n    geom_point(aes(x = 0,\n                   y = L_point),\n               color = 'black',\n               fill = color_hex,\n               shape = 21,\n               size = 2) +\n    coord_equal()\n}\ngraph_C_tangent_plane(C_tangent_plane, color_hex)\n\n\nLet’s first look at all our graphs for our base color. This time I’m picking pink. Next, we can see the C-L Plane, H-C Plane, and H-L Curve images to understand the color in the HCL color space.\n\n\n\n\n\nBase Color: C-L Plane\n\n\n\n\n\n\n\nBase Color: H-C Plane\n\n\n\n\n\n\n\nBase Color: H-L Curve\n\n\nThen we can look at the plane tangent to the H-L Curve to see the space perpendicular to the C-L plane.\n\n\n\n\n\nC Tangent Plane setup\n\n\n\n\n\n\n\nBase Color: Plane Perpendicular to C-L Plane\n\n\n\n\nAfter the base color is understood, we can see what happens if you extend out to a sphere.\n\n\n\nxyL Perimeter\n\n\n\nWe’ll only pull points on the perimeter for this code, unlike the previous post that had points randomly throughout the sphere. We’ll move away from the base point using the Cartesian coordinates of x, y, and L for the first main section. This is exactly what we did in the last post, so it should look familiar. The main new addition is cutting the shape into pieces and graphing them on facets.\n\n\n[xyL Perimeter Code]\n##------------------\n# x, y, L perimeter\n##------------------\nwidth <- 15\nn_color <- 50 ^ 2\n\n# http://extremelearning.com.au/how-to-evenly-distribute-points-on-a-sphere-more-effectively-than-the-canonical-fibonacci-lattice/#more-3069\nget_xyl_data <- function(H_point, C_point, L_point, width, n_color) {\n  data.frame(theta = 2 * pi * seq(0, n_color - 1) / ((1 + sqrt(5)) / 2),\n             phi = acos(1 - 2 * (seq(0, n_color - 1) + .5) / n_color)) %>%\n    mutate(x = cos(theta) * sin(phi),\n           y = sin(theta) * sin(phi),\n           L = cos(phi)) %>%\n    mutate(x = x * width + C_point * cos(H_point * pi/180),\n           y = y * width + C_point * sin(H_point * pi/180),\n           L = L * width + L_point) %>%\n    mutate(H = (atan2(y, x) * 180/pi) %% 360,\n           C = sqrt(x^2 + y^2)) %>%\n    filter(L >= 0 & L <= 100 & C >= 0) %>%\n    mutate(color_value = hcl(h = H,\n                             c = C,\n                             l = L)) %>%\n    mutate(x = C * cos(H * pi/180),\n           y = C * sin(H * pi/180)) %>%\n    mutate(perpendicular_from_C_L = x * sin(-H_point * pi/180) + \n             y * cos(-H_point * pi/180),\n           parallel_along_C_L = x * cos(-H_point * pi/180) - \n             y * sin(-H_point * pi/180)) %>%\n    mutate(row_value = sample(row_number(), n()),\n           col_value = ceiling(row_value / sqrt(n_color))) %>%\n    mutate(row_value = (row_value %% sqrt(n_color)) + 1) %>%\n    mutate(L_cut = as.character(\n      cut(L, breaks = c(-Inf, seq(L_point - width, \n                                  L_point + width, \n                                  length.out = 7), \n                        Inf),\n          labels = c(L_point - width, \n                     seq(L_point - (width * .8), \n                         L_point + (width * .8), \n                         length.out = 6), \n                     L_point + width))),\n      C_cut = as.character(\n        cut(C,\n            breaks = c(-Inf, \n                       seq(C_point - width, \n                           C_point + width, \n                           length.out = 7), \n                       Inf),\n            labels = c(C_point - width, \n                       seq(C_point - (width * .8), \n                           C_point + (width * .8), \n                           length.out = 6), \n                       C_point + width))),\n      H_cut = as.character(\n        cut((180 - abs(abs(H - H_point) - 180)) * \n              sign(180 - abs(H - H_point)) * \n              sign(H - H_point),\n            breaks = c(-Inf, \n                       seq(0 - width, \n                           0 + width, \n                           length.out = 7), \n                       Inf),\n            labels = c(H_point - width, \n                       seq(H_point - (width * .8), \n                           H_point + (width * .8), \n                           length.out = 6), \n                       H_point + width) %% 360)),\n      perpendicular_from_C_L_cut = as.character(\n        cut(perpendicular_from_C_L,\n            breaks = c(-Inf, seq(0 - width, \n                                 0 + width, \n                                 length.out = 7), \n                       Inf),\n            labels = c(0 - width, \n                       seq(0 - (width * .8), \n                           0 + (width * .8), \n                           length.out = 6), \n                       0 + width))),\n      parallel_along_C_L_cut = as.character(\n        cut(parallel_along_C_L,\n            breaks = c(-Inf, \n                       seq(C_point - width, \n                           C_point + width, \n                           length.out = 7), \n                       Inf),\n            labels = c(C_point - width, \n                       seq(C_point - (width * .8), \n                           C_point + (width * .8), \n                           length.out = 6), \n                       C_point + width)))) %>%\n    mutate(L_cut = as.factor(as.numeric(L_cut)),\n           C_cut = as.factor(as.numeric(C_cut)),\n           H_cut = as.factor(as.numeric(H_cut)),\n           perpendicular_from_C_L_cut = \n             as.factor(as.numeric(perpendicular_from_C_L_cut)),\n           parallel_along_C_L_cut = \n             as.factor(as.numeric(parallel_along_C_L_cut))) %>%\n    mutate(H_cut = fct_expand(H_cut, \n                              as.character(c(H_point - width, \n                                             seq(H_point - (width * .8), \n                                                 H_point + (width * .8), \n                                                 length.out = 6), \n                                             H_point + width) %% 360)),\n           perpendicular_from_C_L_cut = \n             fct_expand(perpendicular_from_C_L_cut,\n                        as.character(c(0 - width, \n                                       seq(0 - (width * .8), \n                                           0 + (width * .8), \n                                           length.out = 6), \n                                       0 + width)))) %>%\n    mutate(H_cut = fct_relevel(H_cut, \n                               as.character(c(H_point - width, \n                                              seq(H_point - (width * .8), \n                                                  H_point + (width * .8), \n                                                  length.out = 6), \n                                              H_point + width) %% 360)),\n           perpendicular_from_C_L_cut = \n             fct_relevel(perpendicular_from_C_L_cut,\n                         as.character(c(0 - width, \n                                        seq(0 - (width * .8), \n                                            0 + (width * .8), \n                                            length.out = 6), \n                                        0 + width)))) %>%\n    mutate(H_cut = fct_rev(H_cut),\n           perpendicular_from_C_L_cut = fct_rev(perpendicular_from_C_L_cut))\n}\n\nxyl <- get_xyl_data(H_point, C_point, L_point, width, n_color)\n\ngraph_info <- function(H_point, C_point, L_point) {\n  color_hex <- hcl(H_point, \n                   C_point,\n                   L_point,\n                   fixup = FALSE)\n  \n  ggplot() +\n    geom_rect(aes(xmin = 0, xmax = 1,\n                  ymin = 0, ymax = .5), col = color_hex, fill = color_hex) +\n    geom_text(data = data.frame(x = 0,\n                                y = seq(1.5, .75, -.25),\n                                label = c(paste(\"HEX Value:\", color_hex), \n                                          paste(\"H Value:\", H_point),\n                                          paste(\"C Value:\", C_point),\n                                          paste(\"L Value:\", L_point))),\n              aes(x, y, label = label), hjust = 0, size = 4) +\n    coord_equal() +\n    theme_void()\n}\n\ngraph_sample <- function(color_points) {\n  ggplot(data = color_points,\n         aes(x = row_value,\n             y = col_value,\n             fill = color_value)) +\n    geom_tile() +\n    coord_equal() +\n    scale_fill_identity() +\n    theme_void()\n}\n\np1 <- graph_info(H_point, C_point, L_point)\np2 <- graph_sample(xyl)\np1 + p2\n\n# C L plane by H\ngraph_C_L_by_H <- function(color_points) {\n  ggplot(data = color_points, aes(C, L, \n                                  col = color_value, fill = color_value)) +\n    geom_point() +\n    scale_color_identity() +\n    scale_fill_identity() +\n    facet_wrap(~ H_cut, nrow = 2) +\n    coord_equal() \n}\n\nget_C_L_plane_by_H <- function(color_points, H_point, width) {\n  map_dfr(as.numeric(as.character(unique(color_points$H_cut))), \n          get_C_L_plane) %>%\n    mutate(H_cut = as.factor(H)) %>%\n    mutate(H_cut = fct_expand(H_cut, \n                              as.character(c(H_point - width, \n                                             seq(H_point - (width * .8), \n                                                 H_point + (width * .8), \n                                                 length.out = 6), \n                                             H_point + width) %% 360))) %>%\n    mutate(H_cut = fct_relevel(H_cut, \n                               as.character(c(H_point - width, \n                                              seq(H_point - (width * .8), \n                                                  H_point + (width * .8), \n                                                  length.out = 6), \n                                              H_point + width) %% 360))) %>%\n    mutate(H_cut = fct_rev(H_cut))\n}\nC_L_plane_by_H <- get_C_L_plane_by_H(xyl, H_point, width)\n\ngraph_C_L_plane_by_H <- function(C_L_plane, color_points) {\n  ggplot() +\n    geom_point(data = C_L_plane,\n               aes(C, L, color = color_value, fill = color_value)) +\n    geom_point(data = color_points,\n               aes(C, L, color = \"white\", fill = \"white\")) +\n    scale_x_continuous(labels = abs) +\n    scale_color_identity() +\n    scale_fill_identity() +\n    facet_wrap(~ H_cut, nrow = 2) +\n    coord_equal()\n}\ngraph_C_L_plane_by_H(C_L_plane_by_H, xyl)\ngraph_C_L_by_H(xyl)\n\n# H L curve\ngraph_H_L_by_C <- function(color_points, H_point) {\n  ggplot(data = color_points, aes((H + (180 - H_point)) %% 360, L, \n                                  color = color_value, fill = color_value)) +\n    geom_point() +\n    scale_color_identity() +\n    scale_fill_identity() +\n    scale_x_reverse('H', # Like you're standing on the inside\n                    labels = label_H_center(H_point = H_point)) +\n    facet_wrap(~ C_cut, nrow = 2) +\n    coord_equal()\n}\n\nget_H_L_curve_by_C <- function(color_points) {\n  map_dfr(as.numeric(as.character(\n    unique(color_points$C_cut))), get_H_L_curve) %>%\n    mutate(C_cut = as.factor(C))\n}\nH_L_curve_by_C <- get_H_L_curve_by_C(xyl)\n\ngraph_H_L_curve_by_C <- function(H_L_curve_by_C, color_points, H_point) {\n  ggplot() +\n    geom_point(data = H_L_curve_by_C,\n               aes((H + (180 - H_point)) %% 360, L, \n                   color = color_value, fill = color_value)) +\n    geom_point(data = color_points,\n               aes((H + (180 - H_point)) %% 360, L, \n                   color = \"white\", fill = \"white\")) +\n    scale_color_identity() +\n    scale_fill_identity() +\n    scale_x_reverse('H', # Like you're standing on the inside\n                    labels = label_H_center(H_point = H_point),\n                    limits = c(360, 0)) +\n    facet_wrap(~ C_cut, nrow = 2) +\n    coord_equal()\n}\ngraph_H_L_curve_by_C(H_L_curve_by_C, xyl, H_point)\ngraph_H_L_by_C(xyl, H_point)\n\n# H C plane by L\ngraph_x_y_by_L <- function(color_points, H_point) {\n  ggplot(data = color_points, aes(x, y, \n                                  col = color_value, fill = color_value)) +\n    geom_abline(slope = c(tan(-67.5 * pi/180), \n                          tan(-45 * pi/180), \n                          tan(-22.5 * pi/180),\n                          0, 100000,\n                          tan(22.5 * pi/180), \n                          tan(45 * pi/180), \n                          tan(67.5 * pi/180)), \n                intercept = 0,\n                color = \"white\") +\n    geom_abline(slope = tan(H_point * pi/180), \n                intercept = 0,\n                color = \"black\") +\n    geom_point() +\n    scale_color_identity() +\n    scale_fill_identity() +\n    facet_wrap(~ L_cut, nrow = 2) +\n    coord_equal() +\n    theme(axis.line=element_blank(), axis.text.x=element_blank(),\n          axis.text.y=element_blank(), axis.ticks=element_blank(),\n          axis.title.x=element_blank(), axis.title.y=element_blank(),\n          panel.grid.major=element_blank(), panel.grid.minor=element_blank())\n}\n\nget_H_C_plane_by_L <- function(color_points) {\n  map_dfr(as.numeric(\n    as.character(unique(color_points$L_cut))), get_H_C_plane) %>%\n    mutate(L_cut = as.factor(L))\n}\nH_C_plane_by_L <- get_H_C_plane_by_L(xyl)\n\ngraph_H_C_plane_by_L <- function(H_C_plane_by_L, color_points) {\n  ggplot() +\n    geom_point(data = H_C_plane_by_L,\n               aes(H, C, color = color_value, fill = color_value)) +\n    geom_point(data = color_points,\n               aes(H , C, color = \"white\", fill = \"white\")) +\n    scale_color_identity() +\n    scale_fill_identity() +\n    scale_x_continuous(breaks = seq(45, 360, 45),\n                       minor_breaks = seq(0, 315, 45) + 45/2,\n                       labels = c('45', '90', '135', '180', \n                                  '225', '270', '315', '0|360')) +\n    scale_y_continuous(limits = c(0, 180)) +\n    facet_wrap(~ L_cut, nrow = 2) +\n    coord_polar(start = 270 * pi / 180,\n                direction = -1)\n}\ngraph_H_C_plane_by_L(H_C_plane_by_L, xyl)\ngraph_x_y_by_L(xyl, H_point)\n\n# C tangent plane\nlabel_perpendicular_from_C_L_cut<- function(x) {\n  as.character(abs(as.numeric(x)))\n}\n\ngraph_parallel_perpendicular_by_L <- function(color_points) {\n  ggplot(data = color_points, \n         aes(x = parallel_along_C_L,\n             y = perpendicular_from_C_L,\n             color = color_value)) +\n    geom_point() +\n    scale_color_identity() +\n    scale_y_continuous(\"Distance Perpendicular to C-L Plane\", # similar to H\n                       labels = abs) +\n    labs(x = \"Distance Parallel to C-L Plane\") + # similar to C\n    facet_wrap(~ L_cut, nrow = 2) +\n    coord_equal()\n}\ngraph_parallel_perpendicular_by_L(xyl)\n\ngraph_perpendicular_L_by_parallel <- function(color_points) {\n  ggplot(data = color_points, \n         aes(x = perpendicular_from_C_L,\n             y = L,\n             color = color_value)) +\n    geom_point() +\n    scale_color_identity() +\n    scale_x_continuous(\"Distance Perpendicular to C-L Plane\",\n                       labels = abs) +\n    scale_y_continuous(labels = abs) +\n    facet_wrap(~ parallel_along_C_L_cut, nrow = 2) +\n    coord_equal()\n}\ngraph_perpendicular_L_by_parallel(xyl)\n\ngraph_parallel_L_by_perpendicular <- function(color_points) {\n  ggplot(data = color_points, \n         aes(x = parallel_along_C_L,\n             y = L,\n             color = color_value)) +\n    geom_point() +\n    scale_color_identity() +\n    labs(x = \"Distance Parallel to C-L Plane\") +\n    facet_wrap(~ perpendicular_from_C_L_cut, nrow = 2,\n               labeller = as_labeller(label_perpendicular_from_C_L_cut)) +\n    coord_equal()\n}\ngraph_parallel_L_by_perpendicular(xyl)\n\n\n\n\n\nxyL: Info\n\n\nAbove, we see the basic information about the base color and the points sampled around the sphere’s perimeter (width is 15 units). Below, we see the sphere cut up by C-L Planes at different Hue values. We see that as we move across different values for our Hue, we slice through the sphere to get different shapes. For example, on the right image, the ends are moved left slightly, and the doughnuts in the middle values tend to have a little more width on the right side. The distortion occurs because our cuts are fanning across the sphere at an angle, not moving along in a straight line. So, one side gets a little more of the perimeter to plot.\n\n\n\n\n\n\nxyL: C-L Plane by H cuts\n\n\n\n\n\n\n\nxyL: C-L by H cuts\n\n\n\n\n\nThe next set shows the H-L Curve by different Chroma values. Here, we might expect to see circles, but instead, we get ovals. These shapes occur because the ring made by moving Hue through the sphere cuts along a curve, then it’s flattened for these facets. Also, technically the facets are a little off because the width of the graph should be increasing as Chroma increases. So even though they show the same Hue values, the arc length of the circle should be getting longer. Therefore the graphs should be getting wider.\n\n\n\n\n\n\nxyL: H-L Curve by C cuts\n\n\n\n\n\n\n\nxyL: H-L by C cuts\n\n\n\n\n\nIn the next set, we finally get our nice circles. As we slice through our sphere for different Luminance values, we get our circles in the H-C Planes.\n\n\n\n\n\n\nxyL: H-C Plane by L cuts\n\n\n\n\n\n\n\nxyL: H-C by L cuts\n\n\n\n\n\nThe following three images also have circles because we’re cutting our sphere along straight lines equal distances from each other. The first facet set looks at the sphere from up above as we cut through different Luminance values. This image is the same as the right side in the previous group, but each sub-image gets rotated, so the Hue angle moves flat left to right. The x-axis is the distance along the C-L Plane that cuts the sphere in half. This distance isn’t equivalent to Chroma because that moves at an angle determined by Hue, but it is related. The values are the Chroma values on the C-L Plane shifted in a straight line from the C-L Plane. The y-axis is the distance from the C-L Plane in either direction.\n\n\n\nxyL: Parallel-Perpendicular Distances by L cuts\n\n\nThe next two images both have the Luminance values along the y-axis but cut the sphere differently. The left side slices the sphere moving out from the center of the HCL color space along the Chroma values. Here we see the nice circles instead of ovals for the images of the H-L cut by C. The right side cuts are made parallel to the C-L Plane and move along a straight line. We’re also getting circles that aren’t a little lop-sided, unlike the C-L cut by H ones.\n\n\n\n\n\n\nxyL: Perpendicular Distance-L by Parallel Distance cuts\n\n\n\n\n\n\n\nxyL: Parallel Distance-L by Perpendicular Distance cuts\n\n\n\n\n\n\n\n\nHCL Perimeter\n\n\n\nWe’ll move away from the base color along Hue, Chroma, and Luminance for the second main section. So we’ll curve around when changing Hue values.\n\n\n[HCL Perimeter Code]\n##------------------\n# H, C, L, perimeter\n##------------------\nget_hcl_data <- function(H_point, C_point, L_point, width, n_color) {\n data.frame(theta = 2 * pi * seq(0, n_color - 1) / ((1 + sqrt(5)) / 2),\n                  phi = acos(1 - 2 * (seq(0, n_color - 1) + .5) / n_color)) %>%\n  mutate(H = cos(theta) * sin(phi),\n         C = sin(theta) * sin(phi),\n         L = cos(phi)) %>%\n  mutate(H = H * width + H_point,\n         C = C * width + C_point,\n         L = L * width + L_point) %>%\n  mutate(H = H %% 360) %>% # not really needed except for graphs\n  filter(L >= 0 & L <= 100 & C >= 0) %>%\n  mutate(color_value = hcl(h = H,\n                           c = C,\n                           l = L)) %>%\n  mutate(x = C * cos(H * pi/180),\n         y = C * sin(H * pi/180)) %>%\n  mutate(perpendicular_from_C_L = x * sin(-H_point * pi/180) + \n           y * cos(-H_point * pi/180), # not a C value since we didn't rotate by H\n         parallel_along_C_L = x * cos(-H_point * pi/180) - \n           y * sin(-H_point * pi/180)) %>%\n  mutate(row_value = sample(row_number(), n()),\n         col_value = ceiling(row_value / sqrt(n_color))) %>%\n  mutate(row_value = (row_value %% sqrt(n_color)) + 1) %>%\n  mutate(L_cut = as.character(\n    cut(L, breaks = c(-Inf, seq(L_point - width, \n                                L_point + width, \n                                length.out = 7), \n                      Inf),\n        labels = c(L_point - width, \n                   seq(L_point - (width * .8), \n                       L_point + (width * .8), \n                       length.out = 6), \n                   L_point + width))),\n         C_cut = as.character(\n           cut(C,\n               breaks = c(-Inf, \n                          seq(C_point - width, \n                              C_point + width, \n                              length.out = 7), \n                          Inf),\n               labels = c(C_point - width, \n                          seq(C_point - (width * .8), \n                              C_point + (width * .8), \n                              length.out = 6), \n                          C_point + width))),\n         H_cut = as.character(\n           cut((180 - abs(abs(H - H_point) - 180)) * \n                 sign(180 - abs(H - H_point)) * \n                 sign(H - H_point),\n               breaks = c(-Inf, \n                          seq(0 - width, \n                              0 + width, \n                              length.out = 7), \n                          Inf),\n               labels = c(H_point - width, \n                          seq(H_point - (width * .8), \n                              H_point + (width * .8), \n                              length.out = 6), \n                          H_point + width) %% 360)),\n         perpendicular_from_C_L_cut = as.character(\n           cut(perpendicular_from_C_L,\n               breaks = c(-Inf, seq(0 - width, \n                                    0 + width, \n                                    length.out = 7), \n                          Inf),\n               labels = c(0 - width, \n                          seq(0 - (width * .8), \n                              0 + (width * .8), \n                              length.out = 6), \n                          0 + width))),\n         parallel_along_C_L_cut = as.character(\n           cut(parallel_along_C_L,\n               breaks = c(-Inf, \n                          seq(C_point - width, \n                              C_point + width, \n                              length.out = 7), \n                          Inf),\n               labels = c(C_point - width, \n                          seq(C_point - (width * .8), \n                              C_point + (width * .8), \n                              length.out = 6), \n                          C_point + width)))) %>%\n  mutate(L_cut = as.factor(as.numeric(L_cut)),\n         C_cut = as.factor(as.numeric(C_cut)),\n         H_cut = as.factor(as.numeric(H_cut)),\n         perpendicular_from_C_L_cut = \n           as.factor(as.numeric(perpendicular_from_C_L_cut)),\n         parallel_along_C_L_cut = \n           as.factor(as.numeric(parallel_along_C_L_cut))) %>%\n  mutate(H_cut = fct_expand(H_cut, \n                            as.character(c(H_point - width, \n                                           seq(H_point - (width * .8), \n                                               H_point + (width * .8), \n                                               length.out = 6), \n                                           H_point + width) %% 360)),\n         perpendicular_from_C_L_cut = \n           fct_expand(perpendicular_from_C_L_cut,\n                      as.character(c(0 - width, \n                                     seq(0 - (width * .8), \n                                         0 + (width * .8), \n                                         length.out = 6), \n                                     0 + width)))) %>%\n  mutate(H_cut = fct_relevel(H_cut, \n                             as.character(c(H_point - width, \n                                            seq(H_point - (width * .8), \n                                                H_point + (width * .8), \n                                                length.out = 6), \n                                            H_point + width) %% 360)),\n         perpendicular_from_C_L_cut = \n           fct_relevel(perpendicular_from_C_L_cut,\n                      as.character(c(0 - width, \n                                     seq(0 - (width * .8), \n                                         0 + (width * .8), \n                                         length.out = 6), \n                                     0 + width)))) %>%\n  mutate(H_cut = fct_rev(H_cut),\n         perpendicular_from_C_L_cut = fct_rev(perpendicular_from_C_L_cut))\n}\n\nhcl <- get_hcl_data(H_point, C_point, L_point, width, n_color)\n\np1 <- graph_info(H_point, C_point, L_point)\np2 <- graph_sample(hcl)\np1 + p2\n\n# C L plane by H\nC_L_plane_by_H <- get_C_L_plane_by_H(hcl, H_point, width)\ngraph_C_L_by_H(hcl)\ngraph_C_L_plane_by_H(C_L_plane_by_H, hcl)\n\n# H L curve\nH_L_curve_by_C <- get_H_L_curve_by_C(hcl)\ngraph_H_L_by_C(hcl, H_point)\ngraph_H_L_curve_by_C(H_L_curve_by_C, hcl, H_point)\n\n# H C plane by L\nH_C_plane_by_L <- get_H_C_plane_by_L(hcl)\ngraph_x_y_by_L(hcl, H_point)\ngraph_H_C_plane_by_L(H_C_plane_by_L, hcl)\n\n# C tangent plane\ngraph_parallel_perpendicular_by_L(hcl)\n\ngraph_perpendicular_L_by_parallel(hcl)\n\ngraph_parallel_L_by_perpendicular(hcl)\n\n\n\n\n\nHCL: Info\n\n\nThis first set of graphs plots the C-L plane split by Hue values with both the perimeter and where it fits in the color space. We see these nice circles as the perimeter is cut moving along Hue values.\n\n\n\n\n\n\nHCL: C-L Plane by H cuts\n\n\n\n\n\n\n\nHCL: C-L by H cuts\n\n\n\n\n\nThe next set shows H-L Curve split by different Chroma values. We’re also getting nice circles, unlike the xyL section.\n\n\n\n\n\n\nHCL: H-L Curve by C cuts\n\n\n\n\n\n\n\nHCL: H-L by C cuts\n\n\n\n\n\nThe following graph breaks the trend of nice circles. Here we see teardrop shapes instead. The next few images pull this apart a little more.\n\n\n\n\n\n\nHCL: H-C Plane by L cuts\n\n\n\n\n\n\n\nHCL: H-C by L cuts\n\n\n\n\n\nHere we see the teardrop shapes in more detail. Because we created our “sphere” following polar coordinates, it doesn’t turn out as we might expect. This shape’s height is a function of what a sphere should be but augmented by the distance between Hue values increasing when Chroma increases. Notice that the center Chroma value is at 26. Not in the middle of the circular part, like at 30-ish.\n\n\n\nHCL: Parallel-Perpendicular Distances by L cuts\n\n\nWe also get a squished version for the image on the left. There are fewer cuts because the shape doesn’t reach out as far as the image on the right.\n\n\n\n\n\n\nHCL: Perpendicular Distance-L by Parallel Distance cuts\n\n\n\n\n\n\n\nHCL: Parallel Distance-L by Perpendicular Distance cuts\n\n\n\n\n\nIt is a little unwieldy to compare the different main sections apart, so we’ll add them together in the next one.\n\n\n\nCompare Perimeters\n\n\n\nFor the following images, the two sets we previously created are stuck together and then graphed to highlight the differences well.\n\n\n[Compare Code]\n##-------\n# Compare\n##-------\ncompare <- rbind(xyl[, c('H', 'C', 'L', 'x', 'y', \n                         'H_cut', 'C_cut', 'L_cut', \n                         'perpendicular_from_C_L', 'parallel_along_C_L', \n                         'perpendicular_from_C_L_cut', 'parallel_along_C_L_cut',\n                         'row_value', 'col_value', 'color_value')],\n                 hcl[, c('H', 'C', 'L', 'x', 'y', \n                         'H_cut', 'C_cut', 'L_cut', \n                         'perpendicular_from_C_L', 'parallel_along_C_L', \n                         'perpendicular_from_C_L_cut', 'parallel_along_C_L_cut',\n                         'row_value', 'col_value', 'color_value')]) %>%\n  mutate(setting = c(rep('XYL', n_color), rep('HCL', n_color))) \n\n# C L plane\ngraph_C_L_by_H_comparison <- function(color_points) {\n  ggplot(data = color_points, aes(C, L, col = setting)) +\n    geom_point(alpha = .5) +\n    facet_wrap(~ H_cut, nrow = 2) +\n    coord_equal() +\n    scale_color_discrete(\"\") +\n    theme(legend.position = \"bottom\")\n}\ngraph_C_L_by_H_comparison(compare)\n\n# H L curve\ngraph_H_L_by_C_comparison <- function(color_points, H_point) {\n  ggplot(data = color_points, \n         aes((H + (180 - H_point)) %% 360, L, color = setting)) +\n    geom_point(alpha = .5) +\n    scale_x_reverse('H',\n                    labels = label_H_center(H_point = H_point)) +\n    facet_wrap(~ C_cut, nrow = 2) +\n    coord_equal() +\n    scale_color_discrete(\"\") +\n    theme(legend.position = \"bottom\")\n}\ngraph_H_L_by_C_comparison(compare, H_point)\n\n# H C plane\ngraph_x_y_by_L_comparison <- function(color_points, H_point) {\n  ggplot(data = color_points, aes(x, y, col = setting)) +\n    geom_abline(slope = c(tan(-67.5 * pi/180), \n                          tan(-45 * pi/180), \n                          tan(-22.5 * pi/180),\n                          0, 100000,\n                          tan(22.5 * pi/180), \n                          tan(45 * pi/180), \n                          tan(67.5 * pi/180)), \n                intercept = 0,\n                color = \"white\") +\n    geom_abline(slope = tan(H_point * pi/180), \n                intercept = 0,\n                color = \"black\") +\n    geom_point(alpha = .5) +\n    facet_wrap(~ L_cut, nrow = 2) +\n    coord_equal() +\n    scale_color_discrete(\"\") +\n    theme(legend.position = \"bottom\",\n          axis.line=element_blank(), axis.text.x=element_blank(),\n          axis.text.y=element_blank(), axis.ticks=element_blank(),\n          axis.title.x=element_blank(), axis.title.y=element_blank(),\n          panel.grid.major=element_blank(), panel.grid.minor=element_blank())\n}\ngraph_x_y_by_L_comparison(compare, H_point)\n\n# C tangent plane\ngraph_parallel_perpendicular_by_L_comparison <- function(color_points) {\n  ggplot(data = color_points, \n         aes(x = parallel_along_C_L,\n             y = perpendicular_from_C_L,\n             color = setting)) +\n    geom_point(alpha = .5) +\n    scale_y_continuous(\"Distance Perpendicular to C-L Plane\",\n                       labels = abs) +\n    labs(x = \"Distance Parallel to C-L Plane\") +\n    facet_wrap(~ L_cut, nrow = 2) +\n    coord_equal() +\n    scale_color_discrete(\"\") +\n    theme(legend.position = \"bottom\")\n}\ngraph_parallel_perpendicular_by_L_comparison(compare)\n\ngraph_perpendicular_L_by_parallel_comparison <- function(color_points) {\n  ggplot(data = color_points, \n         aes(x = perpendicular_from_C_L,\n             y = L,\n             color = setting)) +\n    geom_point(alpha = .5) +\n    scale_x_continuous(\"Distance Perpendicular to C-L Plane\", \n            labels = abs) +\n    scale_y_continuous(labels = abs) +\n    facet_wrap(~ parallel_along_C_L_cut, nrow = 2) +\n    coord_equal() +\n    scale_color_discrete(\"\") +\n    theme(legend.position = \"bottom\")\n}\ngraph_perpendicular_L_by_parallel_comparison(compare)\n\ngraph_parallel_L_by_perpendicular_comparison <- function(color_points) {\n  ggplot(data = color_points, \n         aes(x = parallel_along_C_L,\n             y = L,\n             color = setting)) +\n    geom_point(alpha = .5) +\n    labs(x = \"Distance Parallel to C-L Plane\") +\n    facet_wrap(~ perpendicular_from_C_L_cut, nrow = 2,\n               labeller = as_labeller(label_perpendicular_from_C_L_cut)) +\n    coord_equal() +\n    scale_color_discrete(\"\") +\n    theme(legend.position = \"bottom\")\n}\ngraph_parallel_L_by_perpendicular_comparison(compare)\n\ncompare <- compare %>%\n  group_by(setting) %>%\n  arrange((180 - abs(abs(H - H_point) - 180)) * \n            sign(180 - abs(H - H_point)) * sign(H - H_point)) %>%\n  mutate(col_value = ceiling(row_number() / sqrt(n_color))) %>%\n  arrange(col_value, L) %>%\n  group_by(setting, col_value) %>%\n  mutate(row_value = row_number())\n\nggplot(data = compare,\n       aes(x = col_value,\n           y = row_value,\n           fill = color_value)) +\n  geom_tile() +\n  coord_equal() +\n  scale_fill_identity() +\n  theme_void() +\n  facet_grid(~setting)\n\ncompare <- compare %>%\n  group_by(setting) %>%\n  arrange((180 - abs(abs(H - H_point) - 180)) * \n            sign(180 - abs(H - H_point)) * sign(H - H_point)) %>%\n  mutate(col_value = ceiling(row_number() / sqrt(n_color))) %>%\n  arrange(col_value, C) %>%\n  group_by(setting, col_value) %>%\n  mutate(row_value = row_number())\n\nggplot(data = compare,\n       aes(x = col_value,\n           y = row_value,\n           fill = color_value)) +\n  geom_tile() +\n  coord_equal() +\n  scale_fill_identity() +\n  theme_void() +\n  facet_grid(~setting)\n\n\nThis section will have graphs with points from the xyL function in blue and HCL in orange. In the left image, we see the lop-sidedness of the xyL points with the circles of the HCL ones. Also, we can easily see that the XYL points stretch out to farther Hue values. Then we have the ovals for xyL and circles for HCL.\n\n\n\n\n\n\nComparison: C-L by L cuts\n\n\n\n\n\n\n\nComparison: H-L by C cuts\n\n\n\n\n\nThen we can see the circles for xyL and teardrops for HCL.\n\n\n\n\n\n\nComparison: H-C by L cuts\n\n\n\n\n\n\n\nComparison: Parallel-Perpendicular by L cuts\n\n\n\n\n\nFinally, we get a comparison that confirms the HCL function results in squeezing the perimeter, not stretching it vertically.\n\n\n\n\n\n\nComparison: Perpendicular-L by Parallel cuts\n\n\n\n\n\n\n\nComparison: Parallel-L by Perpendicular cuts\n\n\n\n\n\nGraphing the perimeters is nice to see what’s going on mathematically in the space, but the main result is in the end colors. So the last images of this main section show the direct colors of the samples.\nThe first comparison has the samples sorted by Hue for the columns, then within each column, sorted by Luminance. I can’t tell a huge difference, but the Hue values for xyL seem to stretch slightly more than HCL, and HCL has a little more gray.\n\n\n\nComparison: Samples H by L\n\n\nThe second comparison has the samples sorted by Hue (like the first comparison), then sorted by Chroma. To me, this looks very different for having the same colors in the same columns. The HCL square is a little grayer at the bottom.\n\n\n\nComparison: Samples H by C\n\n\nFor the final two main sections, we’ll change only the Chroma value. First, we’ll move everything closer to the center.\n\n\n\nLower Chroma Value\n\n\n\nWe’ll start by changing the base Chroma value from 26 to 16. Everything else is the same.\n\n\n[Lower Chroma Value Code]\n## Try lower value of C ----\nC_point <- 16\n\nxyl <- get_xyl_data(H_point, C_point, L_point, width, n_color)\n\nhcl <- get_hcl_data(H_point, C_point, L_point, width, n_color)\n\ncompare <- rbind(xyl[, c('H', 'C', 'L', 'x', 'y', \n                         'H_cut', 'C_cut', 'L_cut', \n                         'perpendicular_from_C_L', 'parallel_along_C_L', \n                         'perpendicular_from_C_L_cut', 'parallel_along_C_L_cut',\n                         'row_value', 'col_value', 'color_value')],\n                 hcl[, c('H', 'C', 'L', 'x', 'y', \n                         'H_cut', 'C_cut', 'L_cut', \n                         'perpendicular_from_C_L', 'parallel_along_C_L', \n                         'perpendicular_from_C_L_cut', 'parallel_along_C_L_cut',\n                         'row_value', 'col_value', 'color_value')]) %>%\n  mutate(setting = c(rep('XYL', n_color), rep('HCL', n_color))) \n\n# C L plane\ngraph_C_L_by_H_comparison(compare)\n\n# H L curve\ngraph_H_L_by_C_comparison(compare, H_point)\n\n# H C plane\ngraph_x_y_by_L_comparison(compare, H_point)\n\n# C tangent plane\ngraph_parallel_perpendicular_by_L_comparison(compare)\n\ngraph_perpendicular_L_by_parallel_comparison(compare)\n\ngraph_parallel_L_by_perpendicular_comparison(compare)\n\ncompare <- compare %>%\n  group_by(setting) %>%\n  arrange((180 - abs(abs(H - H_point) - 180)) * \n            sign(180 - abs(H - H_point)) * sign(H - H_point)) %>%\n  mutate(col_value = ceiling(row_number() / sqrt(n_color))) %>%\n  arrange(col_value, L) %>%\n  group_by(setting, col_value) %>%\n  mutate(row_value = row_number())\n\nggplot(data = compare,\n       aes(x = col_value,\n           y = row_value,\n           fill = color_value)) +\n  geom_tile() +\n  coord_equal() +\n  scale_fill_identity() +\n  theme_void() +\n  facet_grid(~setting)\n\ncompare <- compare %>%\n  group_by(setting) %>%\n  arrange((180 - abs(abs(H - H_point) - 180)) * \n            sign(180 - abs(H - H_point)) * sign(H - H_point)) %>%\n  mutate(col_value = ceiling(row_number() / sqrt(n_color))) %>%\n  arrange(col_value, C) %>%\n  group_by(setting, col_value) %>%\n  mutate(row_value = row_number())\n\nggplot(data = compare,\n       aes(x = col_value,\n           y = row_value,\n           fill = color_value)) +\n  geom_tile() +\n  coord_equal() +\n  scale_fill_identity() +\n  theme_void() +\n  facet_grid(~setting)\n\n\nFrom these graphs, we can see everything is exaggerated. For example, the first two images have similar circles for HCL, but xyL is breaking apart and getting stretched.\n\n\n\n\n\n\nLower C: C-L by L cuts\n\n\n\n\n\n\n\nLower C: H-L by C cuts\n\n\n\n\n\nThen we see the circles for xyL, but HCL has a very tight teardrop shape.\n\n\n\n\n\n\nLower C: H-C by L cuts\n\n\n\n\n\n\n\nLower C: Parallel-Perpendicular by L cuts\n\n\n\n\n\nThe same trend continues with the exaggeration for HCL.\n\n\n\n\n\n\nLower C: Perpendicular-L by Parallel cuts\n\n\n\n\n\n\n\nLower C: Parallel-L by Perpendicular cuts\n\n\n\n\n\nFinally, we can compare the sampled colors directly.\n\n\n\n\n\n\nLower C: Samples H by L\n\n\n\n\n\n\n\nLower C: Samples H by C\n\n\n\n\n\n\n\n\nHigher Chroma Value\n\n\n\nFor the last main section, we’ll set Chroma to 75.\n\n\n[Higher Chroma Value Code]\n## Try higher value of C ----\nC_point <- 75\n \nxyl <- get_xyl_data(H_point, C_point, L_point, width, n_color)\n\nhcl <- get_hcl_data(H_point, C_point, L_point, width, n_color)\n\ncompare <- rbind(xyl[, c('H', 'C', 'L', 'x', 'y', \n                         'H_cut', 'C_cut', 'L_cut', \n                         'perpendicular_from_C_L', 'parallel_along_C_L', \n                         'perpendicular_from_C_L_cut', 'parallel_along_C_L_cut',\n                         'row_value', 'col_value', 'color_value')],\n                 hcl[, c('H', 'C', 'L', 'x', 'y', \n                         'H_cut', 'C_cut', 'L_cut', \n                         'perpendicular_from_C_L', 'parallel_along_C_L', \n                         'perpendicular_from_C_L_cut', 'parallel_along_C_L_cut',\n                         'row_value', 'col_value', 'color_value')]) %>%\n  mutate(setting = c(rep('XYL', n_color), rep('HCL', n_color))) \n\n# C L plane\ngraph_C_L_by_H_comparison(compare)\n\n# H L curve\ngraph_H_L_by_C_comparison(compare, H_point)\n\n# H C plane\ngraph_x_y_by_L_comparison(compare, H_point)\n\n# C tangent plane\ngraph_parallel_perpendicular_by_L_comparison(compare)\n\ngraph_perpendicular_L_by_parallel_comparison(compare)\n\ngraph_parallel_L_by_perpendicular_comparison(compare)\n\ncompare <- compare %>%\n  group_by(setting) %>%\n  arrange((180 - abs(abs(H - H_point) - 180)) * \n            sign(180 - abs(H - H_point)) * sign(H - H_point)) %>%\n  mutate(col_value = ceiling(row_number() / sqrt(n_color))) %>%\n  arrange(col_value, L) %>%\n  group_by(setting, col_value) %>%\n  mutate(row_value = row_number())\n\nggplot(data = compare,\n       aes(x = col_value,\n           y = row_value,\n           fill = color_value)) +\n  geom_tile() +\n  coord_equal() +\n  scale_fill_identity() +\n  theme_void() +\n  facet_grid(~setting)\n\ncompare <- compare %>%\n  group_by(setting) %>%\n  arrange((180 - abs(abs(H - H_point) - 180)) * \n            sign(180 - abs(H - H_point)) * sign(H - H_point)) %>%\n  mutate(col_value = ceiling(row_number() / sqrt(n_color))) %>%\n  arrange(col_value, C) %>%\n  group_by(setting, col_value) %>%\n  mutate(row_value = row_number())\n\nggplot(data = compare,\n       aes(x = col_value,\n           y = row_value,\n           fill = color_value)) +\n  geom_tile() +\n  coord_equal() +\n  scale_fill_identity() +\n  theme_void() +\n  facet_grid(~setting)\n\n\nThis time, the results have a different shape. Now, HCL stretches out farther than xyL (but still in a circle).\n\n\n\n\n\n\nHigher C: C-L by L cuts\n\n\n\n\n\n\n\nHigher C: H-L by C cuts\n\n\n\n\n\nThe graphs from above show the stretching well. Instead of the teardrop shape, we now have ovals. At some point in increasing the Chroma value, the HCL function surpasses xyL.\n\n\n\n\n\n\nHigher C: H-C by L cuts\n\n\n\n\n\n\n\nHigher C: Parallel-Perpendicular by L cuts\n\n\n\n\n\nWe see the same trend continue here.\n\n\n\n\n\n\nHigher C: Perpendicular-L by Parallel cuts\n\n\n\n\n\n\n\nHigher C: Parallel-L by Perpendicular cuts\n\n\n\n\n\nAnd finally, we can compare the examples again.\n\n\n\n\n\n\nHigher C: Samples H by L\n\n\n\n\n\n\n\nHigher C: Samples H by C\n\n\n\n\n\nI think at the end of the post, I’m supposed to say one is better than the other, but I think they’re just different. It really depends on what you’re looking for in selecting colors. Looking at the colors is the best way to do that, but the other graphs help explain what is happening and determine the next steps.\n\n\n\n\nFootnotes\n\n\nIf you understand the difference between polar and Cartesian coordinates pretty well, this blog post will be obvious to you. But I’m bad at math and needed to see a lot of graphs to understand what was happening. So I figured I’d throw this in a blog post.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "William Tyler Bradley",
    "section": "",
    "text": "Generative art project based on a looped path\n\n\n\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nPresentation on cross-validation using toys\n\n\n\n\n\n\nMay 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nGenerative art based on grocery store floor signs\n\n\n\n\n\n\nMay 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nConnecting graphs throughout a document\n\n\n\n\n\n\nMar 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nGenerative art project based on constellations\n\n\n\n\n\n\nFeb 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nR code to create a flashcard presentation\n\n\n\n\n\n\nDec 20, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nGenerative art based on chess games\n\n\n\n\n\n\nDec 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nR package for Penrose tiling patterns\n\n\n\n\n\n\nOct 19, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nPresentation write-up on Power BI and Missing Values\n\n\n\n\n\n\nOct 19, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nGenerative art based on the Bertrand Paradox\n\n\n\n\n\n\nAug 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFollow-up from previous post using the HCL coordinates directly\n\n\n\n\n\n\nAug 10, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAttempting to select colors close to a chosen one by sampling colors in a ellipse in the HCL color space\n\n\n\n\n\n\nAug 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nPresentation write-up on SQL Server’s Machine Learning Services\n\n\n\n\n\n\nJun 28, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nGENUARY 2021: Art and Experience\n\n\n\n\n\n\nJan 31, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn elegant theme for growth and flow.\n\n\n\n\n\n\nMay 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis post continues the work from the previous one. This code takes the saved data and creates the images.\n\n\n\n\n\n\nApr 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog post and the next one cover the code for making the site images. The first post pulls the data, and the second one creates the images. The setup first pulls the contents from the Data Science page of Wikipedia to get the letter frequencies. The code then pulls the overall letter frequencies for the English language. Finally, it saves them together to graph later.\n\n\n\n\n\n\nApr 16, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "Data scientist location in Asheville, NC.\nCurrently working in healthcare.\nHaving fun with generative art."
  }
]